last-xp-id: 25
xps:
  xp-2-1: BaseModel BaseDataset 6-AttentionHeads 6-Blocks
  xp-2-2: 8-AttentionHeads 8-Blocks
  xp-2-3: Normalization RMSNorm
  xp-2-4: ActivationFunctions SiLU
  xp-2-5: ActivationFunctions SwiGLU
  xp-2-6: XP-5IsHere
  xp-2-7: 12-AttentionHeads 12-Block
  xp-2-8: BaseModel 6-AttentionHeads 6-AttentionBlocks
  xp-2-9: 12-AttentionHeads 12-Blocks RMSNorm SiLU
  xp-2-10: 16-AttentionHeads 24-Blocks
  xp-2-11: Normalization RMSNorm
  xp-2-12: 12-AttentionHeads 12-AttentionBlock RMSNorm SiLU 60MModel 30MData
  xp-2-13: 16-AttentionHeads 24-AttentionBlock RMSNorm SiLU 60MModel 30MData
  xp-2-14: 12-AttentionHeads 12-AttentionBlock RMSNorm SiLU 60MModel 30MData WeightsTying
  xp-2-15: 12-AttentionHeads 12-AttentionBlock RMSNorm SiLU 40MModel 30MData
  xp-2-16: RoPE 12-AttentionHeads 12-AttentionBlock RMSNorm SiLU 60MModel 30MData
  xp-2-17: 12-AttentionHeads 12-AttentionBlock RMSNorm SiLU 50MModel 30MData
  xp-2-18: 12-AttentionHeads 12-AttentionBlock RMSNorm SiLU 60MModel 60MData
  xp-2-19: 16-AttentionHeads 24-AttentionBlock RMSNorm SiLU 50MModel 30MData
  xp-2-20: 16-AttentionHeads 24-AttentionBlock RMSNorm SiLU 71MModel 30MData
  xp-2-21: 32-AttentionHeads 24-AttentionBlock RMSNorm SiLU 58MModel 30MData
  xp-2-22: 16-AttentionHeads 24-AttentionBlock RMSNorm SiLU 60MModel 90MData 1.7Epochs
  xp-2-23: 16-AttentionHeads 24-AttentionBlock RMSNorm SiLU 1MModel 60MData 1.7Epochs
    Dp-11-1
  xp-2-24: 16-AttentionHeads 24-AttentionBlock RMSNorm SiLU 5MModel 60MData 1.7Epochs
    Dp-11-1
  xp-2-25: 16-AttentionHeads 24-AttentionBlock RMSNorm SiLU 11MModel 60MData 1.7Epochs
    Dp-11-1
