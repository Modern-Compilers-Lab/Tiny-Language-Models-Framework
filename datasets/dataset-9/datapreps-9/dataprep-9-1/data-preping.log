|ITERS: 11 / 11 | COMP: 100.00% | RATE: 0.01 it./s | SPD: 103.3095 s/it.| ERT: (0, 0, 0, 0)                                                                                                             
|ITERS: 2 / 2 | COMP: 100.00% | RATE: 0.01 it./s | SPD: 72.4227 s/it.| ERT: (0, 0, 0, 0)                                                                                                                

Saving the data-preping-numpy-random state:
	--> saving it
	--> freeing its memory

Loading the dataset:

Get all the unique characters that occur in this text:
	--> all the unique characters: '\t\n !#%()*+,-./0123456789:<=>abcdefghijklmnopqrstuvwxyz'
	--> vocab size: 54

Create a mapping from characters to integers:

Save the meta information as well, to help us encode/decode later:
	--> freeing its memory

Split by examples using \n\n:
	--> splitting
	--> freeing data memory
	--> total number of examples: 90,000,000


Creating the train.txt, val.txt and test.txt:
	--> shuffling examples
	--> creating the train_examples
	--> train_examples has 72000000 examples
	--> creating the train_data
	--> train_data has 10940656101 tokens
	--> writing the train_data to train.txt
	--> creating the val_examples
	--> val_examples has 9000000 examples
	--> creating the val_data
	--> val_data has 1367656535 tokens
	--> writing the val_data to val.txt
	--> creating the test_examples
	--> test_examples has 9000000 examples
	--> creating the test_data
	--> test_data has 1367605875 tokens
	--> writing the test_data to test.txt
	--> freeing examples memory

We define the encoding function:

Reading and encoding train.txt directly to binary:

Reading and encoding val.txt directly to binary:
