                                                                                                                                                                                                        
                                                                                                                                                                                                        

Saving the data-preping-numpy-random state:
	--> saving it
	--> freeing its memory

Loading the dataset:

Split by examples using \n\n:
	--> splitting
	--> freeing data memory
	--> total number of examples: 29,999,999


Creating the train.txt, val.txt and test.txt:
	--> shuffling examples
	--> creating the train_examples
	--> train_examples has 23999999 examples
	--> creating the train_data
	--> writing the train_data to train.txt
	--> creating the val_examples
	--> val_examples has 3000000 examples
	--> creating the val_data
	--> writing the val_data to val.txt
	--> creating the test_examples
	--> test_examples has 3000000 examples
	--> creating the test_data
	--> test_data has 415616450 tokens
	--> writing the test_data to test.txt
	--> freeing examples memory

We generate the tokenized file of train.txt:

We generate the tokenized file of val.txt:
