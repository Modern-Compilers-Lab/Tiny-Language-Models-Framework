{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-21T17:35:22.591942Z","iopub.status.busy":"2024-09-21T17:35:22.591390Z","iopub.status.idle":"2024-09-21T17:35:26.107377Z","shell.execute_reply":"2024-09-21T17:35:26.106460Z","shell.execute_reply.started":"2024-09-21T17:35:22.591896Z"},"trusted":true},"outputs":[],"source":["import random\n","import os\n","import pickle\n","import time\n","import datetime\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim.lr_scheduler import StepLR\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import re"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T17:35:26.109879Z","iopub.status.busy":"2024-09-21T17:35:26.109335Z","iopub.status.idle":"2024-09-21T17:35:26.154157Z","shell.execute_reply":"2024-09-21T17:35:26.153051Z","shell.execute_reply.started":"2024-09-21T17:35:26.109834Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Device set to cuda.\n"]}],"source":["# Set the random seed for reproducibility\n","seed = 42\n","torch.manual_seed(seed)\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","# Set the device to GPU if available, otherwise CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Device set to {device}.\")"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preparation"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T17:35:26.156265Z","iopub.status.busy":"2024-09-21T17:35:26.155584Z","iopub.status.idle":"2024-09-21T17:35:26.161705Z","shell.execute_reply":"2024-09-21T17:35:26.160755Z","shell.execute_reply.started":"2024-09-21T17:35:26.156220Z"},"trusted":true},"outputs":[],"source":["# Helper functions to load and save data\n","def save_data(data, file_path):\n","    with open(file_path, 'w') as f:\n","        f.write(data)\n","\n","def load_data(file_path):\n","    with open(file_path, 'r') as f:\n","        return f.read()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T17:35:26.164357Z","iopub.status.busy":"2024-09-21T17:35:26.164004Z","iopub.status.idle":"2024-09-21T17:35:26.171508Z","shell.execute_reply":"2024-09-21T17:35:26.170644Z","shell.execute_reply.started":"2024-09-21T17:35:26.164314Z"},"trusted":true},"outputs":[],"source":["# Directory where the data is stored\n","DATA_DIR = \"/kaggle/input/all-level-dataset-1-46m\""]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T17:35:26.173180Z","iopub.status.busy":"2024-09-21T17:35:26.172736Z","iopub.status.idle":"2024-09-21T17:35:26.184870Z","shell.execute_reply":"2024-09-21T17:35:26.183993Z","shell.execute_reply.started":"2024-09-21T17:35:26.173138Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["found vocab_size = 52 (inside /kaggle/input/all-level-dataset-1-46m/meta.pkl)\n"]}],"source":["# Attempt to derive vocab_size from the dataset\n","\n","meta_path = os.path.join(os.path.join(DATA_DIR, 'meta.pkl'))\n","vocab_size = None\n","\n","if os.path.exists(meta_path):\n","    with open(meta_path, 'rb') as f:\n","        meta = pickle.load(f)\n","    vocab_size = meta['vocab_size']\n","    print(f\"found vocab_size = {vocab_size} (inside {meta_path})\")\n","else:\n","    print(\"Meta file not found. Please ensure the meta.pkl file is present in the data directory.\")\n","\n","# Encode and decode functions for character-level Tokenzation \n","def encode(s):\n","    return [meta['stoi'][c] for c in s]\n","\n","def decode(l):\n","    return ''.join([meta['itos'][i] for i in l])"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T17:35:26.186192Z","iopub.status.busy":"2024-09-21T17:35:26.185920Z","iopub.status.idle":"2024-09-21T17:35:26.193860Z","shell.execute_reply":"2024-09-21T17:35:26.192864Z","shell.execute_reply.started":"2024-09-21T17:35:26.186162Z"},"trusted":true},"outputs":[],"source":["# Load encoded data\n","train_data = np.memmap(os.path.join(DATA_DIR, 'train.bin'), dtype=np.uint16, mode='r')\n","val_data = np.memmap(os.path.join(DATA_DIR, 'val.bin'), dtype=np.uint16, mode='r')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:31:14.261052Z","iopub.status.busy":"2024-09-15T13:31:14.260152Z","iopub.status.idle":"2024-09-15T13:31:14.267116Z","shell.execute_reply":"2024-09-15T13:31:14.266144Z","shell.execute_reply.started":"2024-09-15T13:31:14.261009Z"},"trusted":true},"outputs":[],"source":["# Hyperparameters\n","block_size = 256\n","vocab_size = 53\n","n_embd = 372\n","n_head = 6\n","n_layer = 6\n","dropout = 0\n","batch_size = 64\n","max_iters = 150000\n","learning_rate = 1e-3\n","miles = [int(max_iters * m) for m in [0.7, 0.8, 0.9]]\n","eval_interval = 20000\n","eval_iters = 1000\n","lora_r = 24\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","compile = False"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:31:15.914961Z","iopub.status.busy":"2024-09-15T13:31:15.914192Z","iopub.status.idle":"2024-09-15T13:31:18.889208Z","shell.execute_reply":"2024-09-15T13:31:18.888287Z","shell.execute_reply.started":"2024-09-15T13:31:15.914923Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Data in tokens: 231163106\n","Number of iters for one pseudo-epoch : 14109\n","Number of pseudo-epochs : 10.63\n"]}],"source":["batch_size = 64  \n","block_size = 256 \n","num_iters = 150000\n","with open('/kaggle/input/all-level-dataset-1-46m/train.txt', 'r') as f:\n","    data = f.read()\n","    print(f\"Data in tokens: {len(data)}\")\n","    iters4epoch = len(data)//(batch_size * block_size)\n","    print(f\"Number of iters for one pseudo-epoch : {iters4epoch}\")\n","    print(f\"Number of pseudo-epochs : {num_iters / iters4epoch:.2f}\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:31:21.883001Z","iopub.status.busy":"2024-09-15T13:31:21.882645Z","iopub.status.idle":"2024-09-15T13:31:21.887668Z","shell.execute_reply":"2024-09-15T13:31:21.886398Z","shell.execute_reply.started":"2024-09-15T13:31:21.882969Z"},"trusted":true},"outputs":[],"source":["model_path = \"/kaggle/working/1777944_2024-09-14_20-51.pth\""]},{"cell_type":"markdown","metadata":{},"source":["# MODEL"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:31:24.849762Z","iopub.status.busy":"2024-09-15T13:31:24.848987Z","iopub.status.idle":"2024-09-15T13:31:24.976316Z","shell.execute_reply":"2024-09-15T13:31:24.975230Z","shell.execute_reply.started":"2024-09-15T13:31:24.849721Z"},"trusted":true},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    \"\"\" LayerNorm with an optional bias. PyTorch's LayerNorm doesn't support simply bias=False \"\"\"\n","\n","    def __init__(self, ndim, bias):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(ndim))\n","        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n","\n","    def forward(self, input):\n","        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n","\n","class Head(nn.Module):\n","    \"\"\"One head of self-attention.\"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B, T, C = x.shape\n","        k = self.key(x)   # (B, T, head_size)\n","        q = self.query(x) # (B, T, head_size)\n","        v = self.value(x) # (B, T, head_size)\n","\n","        # Apply scaled dot-product attention\n","        out = torch.nn.functional.scaled_dot_product_attention(\n","            q, k, v, attn_mask=None, dropout_p=dropout if self.training else 0, is_causal=True\n","        )\n","        \n","        return out\n","    \n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embd, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, x):\n","        # Concatenate the outputs from each head\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","    \n","class FeedForward(nn.Module):\n","    \"\"\"A simple linear layer followed by a non-linearity.\"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd, bias=False),\n","            nn.GELU(),\n","            nn.Linear(4 * n_embd, n_embd, bias=False),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class LinearLoRA(nn.Module):\n","    def __init__(self, original_layer, rank=8):\n","        super().__init__()\n","        self.original_layer = original_layer\n","        self.original_layer.weight.requires_grad = False\n","        self.rank = rank\n","        \n","        self.lora_a = nn.Parameter(torch.randn((original_layer.in_features, rank)))\n","        self.lora_b = nn.Parameter(torch.randn((rank, original_layer.out_features)))\n","        \n","        self.reset_parameters()\n","        \n","    def reset_parameters(self):\n","        nn.init.kaiming_uniform_(self.lora_a, a=np.sqrt(5))\n","        nn.init.zeros_(self.lora_b)\n","        \n","    def forward(self, x):\n","        lora_output = x @ self.lora_a @ self.lora_b\n","        return self.original_layer(x) + lora_output\n","    \n","class Block(nn.Module):\n","    \"\"\"Transformer block: communication followed by feedforward.\"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedForward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd, bias=False)\n","        self.ln2 = nn.LayerNorm(n_embd, bias=False)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","class GPT(nn.Module):\n","    \"\"\"GPT language model.\"\"\"\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd, bias=False) \n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # Token and position embeddings\n","        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n","        x = tok_emb + pos_emb # (B, T, n_embd)\n","        x = self.blocks(x) # (B, T, n_embd)\n","        x = self.ln_f(x) # (B, T, n_embd)\n","        logits = self.lm_head(x) # (B, T, vocab_size)\n","\n","        # Compute loss if targets are provided\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B * T, C)\n","            targets = targets.view(B * T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","    \n","    def generate(self, idx, max_new_tokens):\n","        \"\"\"Generate new tokens given an initial context `idx`.\"\"\"\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -block_size:] # Crop to the last block_size tokens\n","            logits, _ = self(idx_cond)\n","            logits = logits[:, -1, :] # Focus on the last time step\n","            probs = F.softmax(logits, dim=-1) # Convert to probabilities\n","            idx_next = torch.multinomial(probs, num_samples=1) # Sample from the distribution\n","            idx = torch.cat((idx, idx_next), dim=1) # Append sampled index to the sequence\n","        return idx\n","    \n","    def activate_lora(self, r=8, heads_only=False, freeze_others=True):\n","        self.lora_rank = r\n","        self.replace_multihead_attention_recursion(heads_only)\n","        if freeze_others:\n","            self.freeze_parameters_except_lora_and_bias()\n","    \n","    def replace_multihead_attention_recursion(self, heads_only=False, model=None):\n","        children = self.named_children() if model is None else model.named_children()\n","        for name, module in children:\n","            if heads_only and name in {\"query\", \"key\", \"value\"}:\n","                # Replace with Lora SelfAttention\n","                new_layer = LinearLoRA(module, rank=self.lora_rank)\n","\n","                if model == None:\n","                    self.__setattr__(name, new_layer)\n","                else:\n","                    setattr(model, name, new_layer)\n","            \n","            elif isinstance(module, nn.Linear) and not heads_only:\n","                new_layer = LinearLoRA(module, rank=self.lora_rank)\n","                \n","                if model == None:\n","                    self.__setattr__(name, new_layer)\n","                else:\n","                    setattr(model, name, new_layer)\n","            \n","            else:\n","                # Recursive call for child modules\n","                self.replace_multihead_attention_recursion(heads_only, model=module)\n","                \n","                \n","    def freeze_parameters_except_lora_and_bias(self):\n","        for name, param in self.named_parameters():\n","            is_trainable = (\n","                \"lora_\" in name\n","                #(self.train_layer_norms and \"LayerNorm\" in name)\n","            )\n","\n","            param.requires_grad = is_trainable"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:31:24.978256Z","iopub.status.busy":"2024-09-15T13:31:24.977865Z","iopub.status.idle":"2024-09-15T13:31:24.992945Z","shell.execute_reply":"2024-09-15T13:31:24.992052Z","shell.execute_reply.started":"2024-09-15T13:31:24.978218Z"},"trusted":true},"outputs":[],"source":["# Get random batch of data\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n","    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","# Estimate loss on train and val splits\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters) \n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:31:24.994762Z","iopub.status.busy":"2024-09-15T13:31:24.994455Z","iopub.status.idle":"2024-09-15T13:31:25.008917Z","shell.execute_reply":"2024-09-15T13:31:25.008019Z","shell.execute_reply.started":"2024-09-15T13:31:24.994731Z"},"trusted":true},"outputs":[],"source":["def load_model():\n","        \"\"\"\n","        Load pre-trained model based on the provided model name.\n","        \"\"\"\n","        if not os.path.exists(model_path):\n","            raise FileNotFoundError(f\"Model file '{model_path}' not found.\")\n","        \n","        model = GPT()\n","        print(\"Compiling the model...\\n\")\n","        r = -1\n","        if compile:\n","            try:\n","                model = torch.compile(model)  # requires PyTorch 2.0\n","            except Exception as e:\n","                pass\n","\n","            checkpoint = torch.load(model_path, map_location=device)\n","            if 'lora_rank' in checkpoint.keys():\n","                r = checkpoint['lora_rank']\n","                state = checkpoint['state_dict']\n","\n","                if r > 0:\n","                    model.activate_lora(r)\n","                model.load_state_dict(state)\n","            else:\n","                model.load_state_dict(checkpoint)\n","        else:\n","            checkpoint = torch.load(model_path, map_location=device)\n","            if 'lora_rank' in checkpoint.keys():\n","                r = checkpoint['lora_rank']\n","                state_dict = checkpoint['state_dict']\n","\n","                if r > 0:\n","                    model.activate_lora(r)\n","            else:\n","                state_dict = checkpoint\n","            \n","            state_dict_keys = map(lambda x: x.replace(\"_orig_mod.\", \"\"), state_dict.keys())\n","            state_dict = dict(zip(state_dict_keys, state_dict.values()))\n","            model.load_state_dict(state_dict)\n","\n","        m = model.to(device)\n","        return m, r"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:31:25.351243Z","iopub.status.busy":"2024-09-15T13:31:25.350410Z","iopub.status.idle":"2024-09-15T13:31:25.871257Z","shell.execute_reply":"2024-09-15T13:31:25.870319Z","shell.execute_reply.started":"2024-09-15T13:31:25.351204Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading model...\n","\n","Compiling the model...\n","\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_36/1873016250.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(model_path, map_location=device)\n"]},{"name":"stdout","output_type":"stream","text":["The model has 1777944 trainable parameters\n"]}],"source":["# Initialize model and move it to the device (GPU)\n","if len(model_path) > 0:\n","    print(\"Loading model...\\n\")\n","    model, r = load_model()\n","    \n","    if r > 0:\n","        lora_rank = r\n","\n","else:\n","    model = GPT()\n","    m = model.to(device)\n","    r = -1\n","\n","    # compile the model\n","    if compile:\n","        print(\"compiling the model... (takes a ~minute)\")\n","        model = torch.compile(model)\n","\n","if lora_r > 0 and r < 0:\n","    print(\"Activating LoRA...\")\n","    model.activate_lora(lora_r)\n","    model = model.to(device)\n","\n","num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","# num_parameters_hr = human_readable(num_parameters)\n","print(f'The model has {num_parameters} trainable parameters')"]},{"cell_type":"markdown","metadata":{},"source":["# TRAINING "]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T20:51:58.474703Z","iopub.status.busy":"2024-09-14T20:51:58.474330Z","iopub.status.idle":"2024-09-14T20:51:58.483383Z","shell.execute_reply":"2024-09-14T20:51:58.482564Z","shell.execute_reply.started":"2024-09-14T20:51:58.474666Z"},"trusted":true},"outputs":[],"source":["# Initialize optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","# Initialize learning rate scheduler\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=miles, gamma=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T20:51:59.169305Z","iopub.status.busy":"2024-09-14T20:51:59.168949Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["iter     0 | train loss 11.9293 | val loss 11.9305\n","iter 20000 | train loss 0.4261 | val loss 0.4256\n","iter 40000 | train loss 0.4201 | val loss 0.4198\n","iter 60000 | train loss 0.4178 | val loss 0.4176\n","iter 80000 | train loss 0.4168 | val loss 0.4169\n","iter 100000 | train loss 0.4158 | val loss 0.4168\n","iter 120000 | train loss 0.4122 | val loss 0.4128\n","iter 140000 | train loss 0.4125 | val loss 0.4126\n"]}],"source":["# Get current date and hour to get track of experiments\n","now = datetime.datetime.now()\n","date_hour = now.strftime(\"%Y-%m-%d_%H-%M\")\n","\n","# Train\n","# Start training timer\n","start_time = time.time()\n","\n","# Training loop\n","for iter in range(max_iters):\n","\n","    # evaluate the model on the train and val splits and log the losses\n","    if iter % eval_interval == 0:\n","        losses = estimate_loss()\n","        print(f'iter {iter:5d} | train loss {losses[\"train\"]:.4f} | val loss {losses[\"val\"]:.4f}')\n","        \n","    # train the model for one iteration\n","    xb, yb = get_batch('train')\n","\n","    # forward pass\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    #loss.requires_grad = True\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Step the scheduler\n","    scheduler.step()\n","\n","# End training timer\n","end_time = time.time()\n","print(f'Training time: {(end_time - start_time) / 60}  min')\n","\n","# Save the trained model\n","model_path = f\"{num_parameters}_{date_hour}.pth\"\n","checkpoint = {\n","    'lora_rank': model.lora_rank if(hasattr(model, \"lora_rank\")) else -1,\n","    'state_dict': model.state_dict()\n","}\n","\n","torch.save(checkpoint, f\"{num_parameters}_{date_hour}.pth\")\n","print(f\"Model saved to {num_parameters}_{date_hour}.pth\\n\")\n","\n","losses = estimate_loss()\n","print(f'iter {iter:5d} | train loss {losses[\"train\"]:.4f} | val loss {losses[\"val\"]:.4f}')"]},{"cell_type":"markdown","metadata":{},"source":["# EVALUATION"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:31:32.911980Z","iopub.status.busy":"2024-09-15T13:31:32.911136Z","iopub.status.idle":"2024-09-15T13:31:32.919244Z","shell.execute_reply":"2024-09-15T13:31:32.918532Z","shell.execute_reply.started":"2024-09-15T13:31:32.911939Z"},"trusted":true},"outputs":[],"source":["test_data = np.memmap(os.path.join(os.path.join(DATA_DIR, 'test.bin')), dtype=np.uint16, mode='r')"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T10:42:28.256543Z","iopub.status.busy":"2024-09-15T10:42:28.255715Z","iopub.status.idle":"2024-09-15T10:44:12.790061Z","shell.execute_reply":"2024-09-15T10:44:12.789192Z","shell.execute_reply.started":"2024-09-15T10:42:28.256506Z"},"trusted":true},"outputs":[],"source":["examples = decode(test_data).split(\"\\n\\n\")\n","examples = [example for example in examples if example]"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:31:35.508103Z","iopub.status.busy":"2024-09-15T13:31:35.507234Z","iopub.status.idle":"2024-09-15T13:31:35.514655Z","shell.execute_reply":"2024-09-15T13:31:35.513722Z","shell.execute_reply.started":"2024-09-15T13:31:35.508063Z"},"trusted":true},"outputs":[],"source":["def evaluate_example(example, model, max_new_tokens=4):\n","\n","    # Split example and determine maximum new tokens allowed\n","    splited_example = example.split(\"# is clone\")\n","    encoded_example = torch.tensor(encode(splited_example[0] + \"# is clone\"), dtype=torch.long).unsqueeze(0).to(device)\n","    prompt_text = splited_example[0] + \"# is clone\"\n","\n","    result_example = splited_example[-1]\n","    model.eval()\n","    # Generate response from model and extract generated results\n","    response = decode(model.generate(encoded_example, max_new_tokens=max_new_tokens)[0].tolist())\n","    splited_response = response.split(\"# is clone\")\n","    result_response = splited_response[-1]\n","    \n","    return prompt_text, result_example.strip(), result_response.strip()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:31:35.854443Z","iopub.status.busy":"2024-09-15T13:31:35.853589Z","iopub.status.idle":"2024-09-15T13:31:35.859665Z","shell.execute_reply":"2024-09-15T13:31:35.858659Z","shell.execute_reply.started":"2024-09-15T13:31:35.854397Z"},"trusted":true},"outputs":[],"source":["# Write results to file\n","def write_results_to_file(output_file, prompt, real_results, generated_results):\n","    df = pd.DataFrame({\n","        'Prompt': prompt,\n","        'Real_Results': real_results,\n","        'Generated_Results': generated_results\n","    })\n","    df.to_csv(output_file, index=False)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:31:36.520339Z","iopub.status.busy":"2024-09-15T13:31:36.519967Z","iopub.status.idle":"2024-09-15T13:31:36.525751Z","shell.execute_reply":"2024-09-15T13:31:36.524864Z","shell.execute_reply.started":"2024-09-15T13:31:36.520301Z"},"trusted":true},"outputs":[],"source":["def evaluate_pair(real, generated_result):\n","    # Determine the length of the shorter and longer strings\n","    min_len = min(len(real), len(generated_result))\n","    max_len = max(len(real), len(generated_result))\n","    match_count = 0\n","    if real == generated_result:\n","        match_count = 1\n","    ratio = match_count\n","    return ratio"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T10:47:10.395606Z","iopub.status.busy":"2024-09-15T10:47:10.394863Z","iopub.status.idle":"2024-09-15T10:47:10.400642Z","shell.execute_reply":"2024-09-15T10:47:10.399653Z","shell.execute_reply.started":"2024-09-15T10:47:10.395568Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["# snippet 1\n","o = 0\n","d = 2\n","o = 8\n","c = (o + 7)\n","for d in range(0, 10) :\n","\tprint(c)\n","# snippet 2\n","k = 2\n","f = 6\n","l = k + f\n","for f in range(5, 17, 1) :\n","\tprint(l)\n","# is clone\n","0\n"]}],"source":["print(examples[182782])"]},{"cell_type":"code","execution_count":214,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:57:37.972425Z","iopub.status.busy":"2024-09-15T13:57:37.971656Z","iopub.status.idle":"2024-09-15T13:57:37.976509Z","shell.execute_reply":"2024-09-15T13:57:37.975608Z","shell.execute_reply.started":"2024-09-15T13:57:37.972387Z"},"trusted":true},"outputs":[],"source":["# Start evaluation process\n","prompt = []\n","real_results = []\n","generated_results = []"]},{"cell_type":"code","execution_count":215,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:57:38.192269Z","iopub.status.busy":"2024-09-15T13:57:38.191368Z","iopub.status.idle":"2024-09-15T13:57:38.303982Z","shell.execute_reply":"2024-09-15T13:57:38.303107Z","shell.execute_reply.started":"2024-09-15T13:57:38.192231Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00,  9.79it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 100.00%\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["for i,example in enumerate(tqdm(examples)):\n","    try:\n","        prompt_text, real_result, result = evaluate_example(example, model)\n","        prompt.append(prompt_text)\n","        real_results.append(real_result)\n","        generated_results.append(result)\n","    except:\n","        prompt.append(prompt_text)\n","        real_results.append(real_result)\n","        generated_results.append(result)"]},{"cell_type":"code","execution_count":216,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:57:38.497830Z","iopub.status.busy":"2024-09-15T13:57:38.497135Z","iopub.status.idle":"2024-09-15T13:57:38.502452Z","shell.execute_reply":"2024-09-15T13:57:38.501543Z","shell.execute_reply.started":"2024-09-15T13:57:38.497788Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["# snippet 1\n","t = 2\n","i = 9\n","g = 7\n","p = (t / i)\n","for i in range(5, 6) :\n","\tprint(i - g)\n","# snippet 2\n","y = 0\n","f = 6\n","z = (f / f)-(y / 1)\n","for y in range(5, 6) :\n","\tprint(y - f)\n","# is clone\n"]}],"source":["print(prompt[0])"]},{"cell_type":"code","execution_count":217,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:57:38.847169Z","iopub.status.busy":"2024-09-15T13:57:38.846426Z","iopub.status.idle":"2024-09-15T13:57:38.851890Z","shell.execute_reply":"2024-09-15T13:57:38.850989Z","shell.execute_reply.started":"2024-09-15T13:57:38.847129Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["print(real_results[0])"]},{"cell_type":"code","execution_count":218,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T13:57:39.201549Z","iopub.status.busy":"2024-09-15T13:57:39.200830Z","iopub.status.idle":"2024-09-15T13:57:39.206369Z","shell.execute_reply":"2024-09-15T13:57:39.205438Z","shell.execute_reply.started":"2024-09-15T13:57:39.201504Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["print(generated_results[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["score=0\n","\n","for real,generated in zip(real_results, generated_results):\n","    score+=evaluate_pair(real.strip(),generated.strip())\n","accuracy = score / len(generated_results)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")"]},{"cell_type":"code","execution_count":109,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T03:31:53.895990Z","iopub.status.busy":"2024-09-10T03:31:53.895472Z","iopub.status.idle":"2024-09-10T03:31:54.200170Z","shell.execute_reply":"2024-09-10T03:31:54.199356Z","shell.execute_reply.started":"2024-09-10T03:31:53.895942Z"},"trusted":true},"outputs":[],"source":["# Store accuracy in a file\n","with open(\"accuracy.txt\", 'w') as f:\n","    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n","# Store predictions in a CSV file\n","write_results_to_file(f\"aboud_predictions.csv\", prompt, real_results, generated_results)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5597979,"sourceId":9252801,"sourceType":"datasetVersion"},{"datasetId":5701278,"sourceId":9394191,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
