{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa556b17-8ea0-4788-aea8-8d6259526157","_uuid":"a14f6813-426a-4666-9280-7ed88ebdb85e","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:47:30.282920Z","iopub.status.busy":"2024-09-23T12:47:30.282569Z","iopub.status.idle":"2024-09-23T12:47:30.288207Z","shell.execute_reply":"2024-09-23T12:47:30.287329Z","shell.execute_reply.started":"2024-09-23T12:47:30.282894Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import random\n","import os\n","import pickle\n","import time\n","import datetime\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim.lr_scheduler import StepLR\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"063623b6-121d-4c18-a660-93d2f1be3305","_uuid":"f10e66ef-f466-4cfc-8ddb-594df92adb45","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:47:31.741666Z","iopub.status.busy":"2024-09-23T12:47:31.741274Z","iopub.status.idle":"2024-09-23T12:47:31.747701Z","shell.execute_reply":"2024-09-23T12:47:31.746726Z","shell.execute_reply.started":"2024-09-23T12:47:31.741638Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Set the random seed for reproducibility\n","seed = 42\n","torch.manual_seed(seed) \n","random.seed(seed)\n","np.random.seed(seed)\n","\n","# Set the device to GPU if available, otherwise CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Device set to {device}.\")"]},{"cell_type":"markdown","metadata":{"_cell_guid":"4776333c-08cd-4127-bea7-d7ec8898df7b","_uuid":"f61836e4-3f71-432d-8c50-9de1ff2e05e0","trusted":true},"source":["# Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fcc4b173-f5e5-4110-b14f-46a8fa6da9ae","_uuid":"0aa1c1b8-a945-4baa-8d46-3a08056a9004","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:47:33.450870Z","iopub.status.busy":"2024-09-23T12:47:33.450042Z","iopub.status.idle":"2024-09-23T12:47:33.455749Z","shell.execute_reply":"2024-09-23T12:47:33.454855Z","shell.execute_reply.started":"2024-09-23T12:47:33.450837Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Helper functions to load and save data\n","def save_data(data, file_path):\n","    with open(file_path, 'w') as f:\n","        f.write(data)\n","\n","def load_data(file_path):\n","    with open(file_path, 'r') as f:\n","        return f.read()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9731ee3f-b4d1-4b6e-afb2-859c56bef6c6","_uuid":"3da5ca68-e0d7-4aed-b89f-5f2a4ab910d9","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:47:44.813132Z","iopub.status.busy":"2024-09-23T12:47:44.812785Z","iopub.status.idle":"2024-09-23T12:47:44.817401Z","shell.execute_reply":"2024-09-23T12:47:44.816456Z","shell.execute_reply.started":"2024-09-23T12:47:44.813103Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Directory where the data is stored \"must contain 4 files : train.txt, val.txt, test.txt and a meta.pkl file\"\n","DATA_DIR = \"/yourDataDirectoryHere\"\n","# Directory where the model is stored\n","MODEL_DIR = \"/yourModelDirectoryHere\""]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ddae0037-0f42-425d-a2e9-4238f4c608f2","_uuid":"6d064118-585d-46a9-8f40-f9472fe879b4","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:47:46.392475Z","iopub.status.busy":"2024-09-23T12:47:46.391663Z","iopub.status.idle":"2024-09-23T12:47:46.403456Z","shell.execute_reply":"2024-09-23T12:47:46.402524Z","shell.execute_reply.started":"2024-09-23T12:47:46.392441Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Attempt to derive vocab_size from the dataset\n","\n","meta_path = os.path.join(DATA_DIR, 'meta.pkl')\n","vocab_size = None\n","\n","if os.path.exists(meta_path):\n","    with open(meta_path, 'rb') as f:\n","        meta = pickle.load(f)\n","    vocab_size = meta['vocab_size']\n","    print(f\"found vocab_size = {vocab_size} (inside {meta_path})\")\n","else:\n","    print(\"Meta file not found. Please ensure the meta.pkl file is present in the data directory.\")\n","\n","# Encode and decode functions for character-level Tokenzation \n","def encode(s):\n","    return [meta['stoi'][c] for c in s]\n","\n","def decode(l):\n","    return ''.join([meta['itos'][i] for i in l])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff53a3e0-09ab-4396-90d9-cef86df0605b","_uuid":"1b2892b5-a904-4550-a8d6-ae8f51f1841f","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:47:48.476650Z","iopub.status.busy":"2024-09-23T12:47:48.476261Z","iopub.status.idle":"2024-09-23T12:47:49.391422Z","shell.execute_reply":"2024-09-23T12:47:49.390496Z","shell.execute_reply.started":"2024-09-23T12:47:48.476620Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Load data\n","train_data = load_data(os.path.join(DATA_DIR, 'train.txt'))\n","val_data = load_data(os.path.join(DATA_DIR, 'val.txt'))\n","test_data = load_data(os.path.join(DATA_DIR, 'test.txt'))\n","\n","# Encode data\n","train_ids = encode(train_data)\n","val_ids = encode(val_data)\n","test_ids = encode(test_data)\n","\n","# Save encoded data to bin files, make sure to choose \"Files only\" on the persistence option of the session so that you don't encode data each time\n","train_ids = np.array(train_ids, dtype=np.uint16)\n","val_ids = np.array(val_ids, dtype=np.uint16)\n","test_ids = np.array(test_ids, dtype=np.uint16)\n","\n","train_ids.tofile( 'train.bin')\n","val_ids.tofile( 'val.bin')\n","test_ids.tofile('test.bin')\n","\n","print(\"Encoded data saved as binary files.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"125ce42e-8df9-4094-b0c5-242fcd99a597","_uuid":"6a2d1ac2-5ef7-441c-9837-050c59120ab9","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:47:51.225679Z","iopub.status.busy":"2024-09-23T12:47:51.225322Z","iopub.status.idle":"2024-09-23T12:47:51.230098Z","shell.execute_reply":"2024-09-23T12:47:51.229117Z","shell.execute_reply.started":"2024-09-23T12:47:51.225651Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["del(train_ids)\n","del(val_ids)\n","del(test_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c53f3930-8d16-443d-a5ec-a6926f3f6cf4","_uuid":"9cd8ff5a-2170-4c53-be17-02ac7d0cffd9","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:47:52.915803Z","iopub.status.busy":"2024-09-23T12:47:52.915072Z","iopub.status.idle":"2024-09-23T12:47:52.920735Z","shell.execute_reply":"2024-09-23T12:47:52.919741Z","shell.execute_reply.started":"2024-09-23T12:47:52.915770Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Load encoded data\n","train_data = np.memmap(\"/kaggle/working/train.bin\", dtype=np.uint16, mode='r')\n","val_data = np.memmap(\"/kaggle/working/val.bin\", dtype=np.uint16, mode='r')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"8574d987-cef6-47d1-b889-e8242a0bcd23","_uuid":"f4fc1523-1d72-49db-a3bc-8d521f236993","trusted":true},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d4305c5-c1c6-48b0-a048-953a98954854","_uuid":"1fd63d8c-f842-444c-9dc8-cab3263ae6e4","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:47:54.647417Z","iopub.status.busy":"2024-09-23T12:47:54.647052Z","iopub.status.idle":"2024-09-23T12:47:54.653828Z","shell.execute_reply":"2024-09-23T12:47:54.652930Z","shell.execute_reply.started":"2024-09-23T12:47:54.647386Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Hyperparameters for the GPT model\n","block_size = 256  # Maximum context length\n","n_embd = 372      # Embedding dimension\n","n_head = 6        # Number of attention heads\n","n_layer = 6       # Number of transformer blocks\n","dropout = 0       # Dropout rate\n","batch_size = 64   # Batch size for training\n","max_iters = 100_000  # Maximum number of iterations\n","learning_rate = 1e-3 # Initial Learning rate value\n","miles = [int(max_iters * m) for m in [0.7, 0.8, 0.9]]  # Milestones for learning rate decay as fractions of max_iters\n","eval_interval = 10_000 # Evaluation interval\n","eval_iters = 1000 # Number of iterations for evaluation\n","vocab_size = 53 # Vocabulary size\n","\n","# Model to be fine-tuned \"set the model name without .pth\" (Keep it empty for training from scratch)\n","model_name = 'yourModelNameWithoutExtensionHere'\n","\n","# LoRA Rank - Set it to 0 if you want to train from scratch or perform full fine-tuning\n","lora_r = 12\n","\n","compile = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T12:47:57.166947Z","iopub.status.busy":"2024-09-23T12:47:57.166102Z","iopub.status.idle":"2024-09-23T12:47:57.171883Z","shell.execute_reply":"2024-09-23T12:47:57.170912Z","shell.execute_reply.started":"2024-09-23T12:47:57.166910Z"},"trusted":true},"outputs":[],"source":["print(f\"Data in tokens: {len(train_data)}\")\n","iters4epoch = len(train_data)//(batch_size * block_size)\n","print(f\"Number of iters for one pseudo-epoch : {iters4epoch}\")\n","print(f\"Number of pseudo-epochs : {max_iters / iters4epoch:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17ff6e02-86d2-4f49-a384-be8c035377a7","_uuid":"9c3a2af2-99a7-4657-bb8d-168a3e8dfcfb","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:47:59.282904Z","iopub.status.busy":"2024-09-23T12:47:59.282430Z","iopub.status.idle":"2024-09-23T12:47:59.430364Z","shell.execute_reply":"2024-09-23T12:47:59.429483Z","shell.execute_reply.started":"2024-09-23T12:47:59.282864Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# defining the entire structure of the model, and in parallel implementing lora\n","class LayerNorm(nn.Module):\n","    \"\"\" LayerNorm with an optional bias. PyTorch's LayerNorm doesn't support simply bias=False \"\"\"\n","\n","    def __init__(self, ndim, bias):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(ndim))\n","        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n","\n","    def forward(self, input):\n","        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n","\n","class Head(nn.Module):\n","    \"\"\"One head of self-attention.\"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B, T, C = x.shape\n","        k = self.key(x)   # (B, T, head_size)\n","        q = self.query(x) # (B, T, head_size)\n","        v = self.value(x) # (B, T, head_size)\n","\n","        # Apply scaled dot-product attention\n","        out = torch.nn.functional.scaled_dot_product_attention(\n","            q, k, v, attn_mask=None, dropout_p=dropout if self.training else 0, is_causal=True\n","        )\n","        \n","        return out\n","    \n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embd, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, x):\n","        # Concatenate the outputs from each head\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","    \n","class FeedForward(nn.Module):\n","    \"\"\"A simple linear layer followed by a non-linearity.\"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd, bias=False),\n","            nn.GELU(),\n","            nn.Linear(4 * n_embd, n_embd, bias=False),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class LinearLoRA(nn.Module):\n","    def __init__(self, original_layer, rank=8):\n","        super().__init__()\n","        self.original_layer = original_layer\n","        self.original_layer.weight.requires_grad = False\n","        self.rank = rank\n","        \n","        self.lora_a = nn.Parameter(torch.randn((original_layer.in_features, rank)))\n","        self.lora_b = nn.Parameter(torch.randn((rank, original_layer.out_features)))\n","        \n","        self.reset_parameters()\n","        \n","    def reset_parameters(self):\n","        nn.init.kaiming_uniform_(self.lora_a, a=np.sqrt(5))\n","        nn.init.zeros_(self.lora_b)\n","        \n","    def forward(self, x):\n","        lora_output = x @ self.lora_a @ self.lora_b\n","        return self.original_layer(x) + lora_output\n","    \n","class Block(nn.Module):\n","    \"\"\"Transformer block: communication followed by feedforward.\"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedForward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd, bias=False)\n","        self.ln2 = nn.LayerNorm(n_embd, bias=False)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","class GPT(nn.Module):\n","    \"\"\"GPT language model.\"\"\"\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd, bias=False) \n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # Token and position embeddings\n","        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n","        x = tok_emb + pos_emb # (B, T, n_embd)\n","        x = self.blocks(x) # (B, T, n_embd)\n","        x = self.ln_f(x) # (B, T, n_embd)\n","        logits = self.lm_head(x) # (B, T, vocab_size)\n","\n","        # Compute loss if targets are provided\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B * T, C)\n","            targets = targets.view(B * T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","    \n","    def generate(self, idx, max_new_tokens):\n","        \"\"\"Generate new tokens given an initial context `idx`.\"\"\"\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -block_size:] # Crop to the last block_size tokens\n","            logits, _ = self(idx_cond)\n","            logits = logits[:, -1, :] # Focus on the last time step\n","            probs = F.softmax(logits, dim=-1) # Convert to probabilities\n","            idx_next = torch.multinomial(probs, num_samples=1) # Sample from the distribution\n","            idx = torch.cat((idx, idx_next), dim=1) # Append sampled index to the sequence\n","        return idx\n","    \n","    def activate_lora(self, r=8, heads_only=False, freeze_others=True):\n","        self.lora_rank = r\n","        self.replace_multihead_attention_recursion(heads_only)\n","        if freeze_others:\n","            self.freeze_parameters_except_lora_and_bias()\n","    \n","    def replace_multihead_attention_recursion(self, heads_only=False, model=None):\n","        children = self.named_children() if model is None else model.named_children()\n","        for name, module in children:\n","            if heads_only and name in {\"query\", \"key\", \"value\"}:\n","                # Replace with Lora SelfAttention\n","                new_layer = LinearLoRA(module, rank=self.lora_rank)\n","\n","                if model == None:\n","                    self.__setattr__(name, new_layer)\n","                else:\n","                    setattr(model, name, new_layer)\n","            \n","            elif isinstance(module, nn.Linear) and not heads_only:\n","                new_layer = LinearLoRA(module, rank=self.lora_rank)\n","                \n","                if model == None:\n","                    self.__setattr__(name, new_layer)\n","                else:\n","                    setattr(model, name, new_layer)\n","            \n","            else:\n","                # Recursive call for child modules\n","                self.replace_multihead_attention_recursion(heads_only, model=module)\n","                \n","                \n","    def freeze_parameters_except_lora_and_bias(self):\n","        for name, param in self.named_parameters():\n","            is_trainable = (\n","                \"lora_\" in name\n","                #(self.train_layer_norms and \"LayerNorm\" in name)\n","            )\n","\n","            param.requires_grad = is_trainable"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a716f789-f605-42d0-9494-d8927ed09a6f","_uuid":"be441d8d-c18b-4694-b2ff-607aac4b11e6","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:48:03.987746Z","iopub.status.busy":"2024-09-23T12:48:03.987386Z","iopub.status.idle":"2024-09-23T12:48:03.998567Z","shell.execute_reply":"2024-09-23T12:48:03.997639Z","shell.execute_reply.started":"2024-09-23T12:48:03.987716Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Get random batch of data\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n","    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","# Estimate loss on train and val splits\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters) \n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","\n","# Helper function to make large numbers of parameters human-readable\n","def human_readable(num):\n","    magnitude = 0\n","    while abs(num) >= 1000:\n","        magnitude += 1\n","        num /= 1000.0\n","    return '%.0f%s' % (num, ['', 'K', 'M', 'G', 'T', 'P'][magnitude])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T12:48:08.446054Z","iopub.status.busy":"2024-09-23T12:48:08.445693Z","iopub.status.idle":"2024-09-23T12:48:08.456231Z","shell.execute_reply":"2024-09-23T12:48:08.455320Z","shell.execute_reply.started":"2024-09-23T12:48:08.446025Z"},"trusted":true},"outputs":[],"source":["# load the language model\n","def load_model():\n","        \"\"\"\n","        Load pre-trained model based on the provided model name.\n","        \"\"\"\n","        model_path = os.path.join(MODEL_DIR, f\"{model_name}.pth\")\n","        if not os.path.exists(model_path):\n","            raise FileNotFoundError(f\"Model file '{model_path}' not found.\")\n","        \n","        model = GPT()\n","        print(\"Compiling the model...\\n\")\n","        r = -1\n","        if compile:\n","            try:\n","                model = torch.compile(model)  # requires PyTorch 2.0\n","            except Exception as e:\n","                pass\n","\n","            checkpoint = torch.load(model_path, map_location=device)\n","            if 'lora_rank' in checkpoint.keys():\n","                r = checkpoint['lora_rank']\n","                state = checkpoint['state_dict']\n","\n","                if r > 0:\n","                    model.activate_lora(r)\n","                model.load_state_dict(state)\n","            else:\n","                model.load_state_dict(checkpoint)\n","        else:\n","            checkpoint = torch.load(model_path, map_location=device)\n","            if 'lora_rank' in checkpoint.keys():\n","                r = checkpoint['lora_rank']\n","                state_dict = checkpoint['state_dict']\n","\n","                if r > 0:\n","                    model.activate_lora(r)\n","            else:\n","                state_dict = checkpoint\n","            \n","            state_dict_keys = map(lambda x: x.replace(\"_orig_mod.\", \"\"), state_dict.keys())\n","            state_dict = dict(zip(state_dict_keys, state_dict.values()))\n","            model.load_state_dict(state_dict)\n","\n","        m = model.to(device)\n","        return m, (r > 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21de39d0-d298-45ce-a590-c6be400f31e8","_uuid":"db1edcb0-7dae-40b8-99f0-3a524bd1311e","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:48:10.715656Z","iopub.status.busy":"2024-09-23T12:48:10.714845Z","iopub.status.idle":"2024-09-23T12:48:11.061542Z","shell.execute_reply":"2024-09-23T12:48:11.060652Z","shell.execute_reply.started":"2024-09-23T12:48:10.715624Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Initialize model and move it to the device (GPU)\n","if len(model_name) > 0:\n","    print(\"Loading model...\\n\")\n","    model, r_exists = load_model()\n","\n","else:\n","    model = GPT()\n","    m = model.to(device)\n","    r_exists = False\n","\n","    # compile the model\n","    if compile:\n","        print(\"compiling the model... (takes a ~minute)\")\n","        model = torch.compile(model)\n","\n","if lora_r > 0 and not r_exists:\n","    print(\"Activating LoRA...\")\n","    model.activate_lora(lora_r)\n","    model = model.to(device)\n","\n","num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","num_parameters_hr = human_readable(num_parameters)\n","print(f'The model has {num_parameters_hr} trainable parameters')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"ac1fe251-e0c8-4079-9da4-68aff59262f4","_uuid":"8cdf45cc-0d3a-43a9-b10d-5381799a21f2","trusted":true},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e725706a-19a1-4e82-91b1-514dd0488f33","_uuid":"45093d41-9498-45e4-b93b-95b0b239c0af","collapsed":false,"execution":{"iopub.execute_input":"2024-09-11T16:44:05.970233Z","iopub.status.busy":"2024-09-11T16:44:05.969481Z","iopub.status.idle":"2024-09-11T16:44:07.752808Z","shell.execute_reply":"2024-09-11T16:44:07.751536Z","shell.execute_reply.started":"2024-09-11T16:44:05.970172Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Initialize optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","# Initialize learning rate scheduler\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=miles, gamma=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76b8e469-893d-4151-a175-99b54dbabe60","_uuid":"534a6c6a-e6b8-4632-8078-86aab93500de","collapsed":false,"execution":{"iopub.execute_input":"2024-09-11T10:57:07.371504Z","iopub.status.busy":"2024-09-11T10:57:07.371046Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Get current date and hour to get track of experiments\n","now = datetime.datetime.now()\n","date_hour = now.strftime(\"%Y-%m-%d_%H-%M\")\n","\n","# Train\n","# Start training timer\n","start_time = time.time()\n","\n","# Training loop\n","for iter in range(max_iters):\n","\n","    # evaluate the model on the train and val splits and log the losses\n","    if iter % eval_interval == 0:\n","        losses = estimate_loss()\n","        print(f'iter {iter:5d} | train loss {losses[\"train\"]:.4f} | val loss {losses[\"val\"]:.4f}')\n","        \n","    # train the model for one iteration\n","    xb, yb = get_batch('train')\n","\n","    # forward passd\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    #loss.requires_grad = True\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Step the scheduler\n","    scheduler.step()\n","\n","# End training timer\n","end_time = time.time()\n","print(f'Training time: {(end_time - start_time) / 60}  min')\n","\n","# Save the trained model\n","model_path = f\"{num_parameters_hr}_{date_hour}.pth\"\n","checkpoint = {\n","    'lora_rank': model.lora_rank if(hasattr(model, \"lora_rank\")) else -1,\n","    'state_dict': model.state_dict()\n","}\n","\n","torch.save(checkpoint, model_path)\n","print(f\"Model saved to {model_path}\\n\")"]},{"cell_type":"markdown","metadata":{"_cell_guid":"e831564c-6b76-489b-98b0-69cad098fdd6","_uuid":"facd8250-1fd4-4486-a9a6-f099df266caf","trusted":true},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8071f1a-961b-4410-ae36-ba54b5b525d0","_uuid":"f4e10d4c-a4c8-4e6b-891e-f3d14947adfb","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:48:15.483123Z","iopub.status.busy":"2024-09-23T12:48:15.482531Z","iopub.status.idle":"2024-09-23T12:48:15.490084Z","shell.execute_reply":"2024-09-23T12:48:15.489192Z","shell.execute_reply.started":"2024-09-23T12:48:15.483092Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["test_data = np.memmap('test.bin', dtype=np.uint16, mode='r')"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3d6ae4b-e069-43bd-be3f-9e46f19146d3","_uuid":"2e9f95ba-ca83-48bc-bb18-8910efc37422","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:48:20.950761Z","iopub.status.busy":"2024-09-23T12:48:20.950432Z","iopub.status.idle":"2024-09-23T12:48:20.961347Z","shell.execute_reply":"2024-09-23T12:48:20.960565Z","shell.execute_reply.started":"2024-09-23T12:48:20.950737Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Evaluate example \"line execution counting\"\n","def evaluate_example(model, example, max_new_tokens=30):\n","    \n","    # Split example and determine maximum new tokens allowed\n","    splited_example = example.split(\"# count\")\n","    if not (\"for\" in splited_example[0]):\n","        max_new_tokens = 22\n","    # Encode prompt and prepare for evaluation    \n","    encoded_example = torch.tensor(encode(splited_example[0] + \"# count\"), dtype=torch.long).unsqueeze(0).to(device)\n","    prompt_text = splited_example[0] + \"# count\"\n","    result_example = splited_example[-1]\n","    \n","    # Extract real results from example\n","    real_results = [float(match.group()) for match in re.finditer(r\"(?<=# )-?\\d+(\\.\\d+)?\", result_example.split('\\n\\n')[0].replace(\"\\n\", \"\"))]\n","    \n","    # Generate response from model and extract generated results\n","    try:\n","        response = decode(model.generate(encoded_example, max_new_tokens=max_new_tokens)[0].tolist())\n","        splited_response = response.split(\"# count\")\n","        result_response = splited_response[-1]\n","        generated_results = [float(match.group()) for match in re.finditer(r\"(?<=# )-?\\d+(\\.\\d+)?\", result_response.split('\\n\\n')[0].replace(\"\\n\", \"\"))]\n","    except:\n","        generated_results = \"error\"\n","    return prompt_text, real_results, generated_results\n","\n","\n","\n","# Write results to file\n","def write_results_to_file(output_file, prompt, real_results, generated_results):\n","    df = pd.DataFrame({\n","        'Prompt': prompt,\n","        'Real_Results': real_results,\n","        'Generated_Results': generated_results\n","    })\n","    df.to_csv(output_file, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2536ece9-1d3c-4373-b308-fd1049f3297f","_uuid":"7b21f8fd-2e4c-443b-8120-e0af732bf558","collapsed":false,"execution":{"iopub.execute_input":"2024-09-23T12:48:31.214124Z","iopub.status.busy":"2024-09-23T12:48:31.213222Z","iopub.status.idle":"2024-09-23T13:32:13.381039Z","shell.execute_reply":"2024-09-23T13:32:13.380177Z","shell.execute_reply.started":"2024-09-23T12:48:31.214089Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Evaluation Loop\n","\n","# Split examples and initialize lists for results\n","examples = decode(test_data).split(\"\\n\\n\")\n","examples = [example for example in examples if example]\n","# Taking a subset of the examples for short \"aimed for verification purposes\" evaluations\n","example_subset = examples[:5000]\n","# Start evaluation process\n","prompt = []\n","real_results = []\n","generated_results = []\n","\n","# Iterate through examples and evaluate the model on each one\n","for example in tqdm(example_subset):\n","    prompt_text, real_result, result = evaluate_example(model, example)\n","    prompt.append(prompt_text)\n","    real_results.append(real_result)\n","    generated_results.append(result)\n","\n","# Calculate and print accuracy\n","correct_count = sum(1 for real, generated in zip(real_results, generated_results) if real == generated)\n","accuracy = correct_count / len(generated_results)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Store accuracy in a file\n","with open(\"accuracy.txt\", 'w') as f:\n","    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n","\n","# Store predictions in a CSV file\n","    write_results_to_file(\"predictions.csv\", prompt, real_results, generated_results)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5419152,"sourceId":9104825,"sourceType":"datasetVersion"},{"datasetId":5544565,"sourceId":9174472,"sourceType":"datasetVersion"},{"datasetId":5546822,"sourceId":9177797,"sourceType":"datasetVersion"},{"datasetId":5527817,"sourceId":9194009,"sourceType":"datasetVersion"},{"datasetId":5559645,"sourceId":9196288,"sourceType":"datasetVersion"},{"datasetId":5560892,"sourceId":9198028,"sourceType":"datasetVersion"},{"datasetId":5560896,"sourceId":9198035,"sourceType":"datasetVersion"},{"datasetId":5560904,"sourceId":9198045,"sourceType":"datasetVersion"},{"datasetId":5566438,"sourceId":9206254,"sourceType":"datasetVersion"},{"datasetId":5592996,"sourceId":9245526,"sourceType":"datasetVersion"},{"datasetId":5596284,"sourceId":9250376,"sourceType":"datasetVersion"},{"datasetId":5603809,"sourceId":9261202,"sourceType":"datasetVersion"},{"datasetId":5603815,"sourceId":9261210,"sourceType":"datasetVersion"},{"datasetId":5628994,"sourceId":9297219,"sourceType":"datasetVersion"},{"datasetId":5628996,"sourceId":9297222,"sourceType":"datasetVersion"},{"datasetId":5628998,"sourceId":9297227,"sourceType":"datasetVersion"},{"datasetId":5628999,"sourceId":9297228,"sourceType":"datasetVersion"},{"datasetId":5629001,"sourceId":9297232,"sourceType":"datasetVersion"},{"datasetId":5629005,"sourceId":9297237,"sourceType":"datasetVersion"},{"datasetId":5670920,"sourceId":9354642,"sourceType":"datasetVersion"},{"datasetId":5673838,"sourceId":9358533,"sourceType":"datasetVersion"},{"datasetId":5673878,"sourceId":9358581,"sourceType":"datasetVersion"},{"datasetId":5676378,"sourceId":9361789,"sourceType":"datasetVersion"},{"datasetId":5676476,"sourceId":9361942,"sourceType":"datasetVersion"},{"datasetId":5680088,"sourceId":9366638,"sourceType":"datasetVersion"},{"datasetId":5681041,"sourceId":9367903,"sourceType":"datasetVersion"},{"datasetId":5707886,"sourceId":9402486,"sourceType":"datasetVersion"},{"datasetId":5708526,"sourceId":9403279,"sourceType":"datasetVersion"},{"datasetId":5708753,"sourceId":9403553,"sourceType":"datasetVersion"},{"datasetId":5720522,"sourceId":9418762,"sourceType":"datasetVersion"},{"datasetId":5749118,"sourceId":9457179,"sourceType":"datasetVersion"},{"datasetId":5749126,"sourceId":9457191,"sourceType":"datasetVersion"},{"datasetId":5752981,"sourceId":9462317,"sourceType":"datasetVersion"},{"datasetId":5753388,"sourceId":9462832,"sourceType":"datasetVersion"},{"modelId":103985,"modelInstanceId":79512,"sourceId":94818,"sourceType":"modelInstanceVersion"},{"modelId":104098,"modelInstanceId":79617,"sourceId":94938,"sourceType":"modelInstanceVersion"},{"modelId":106026,"modelInstanceId":81700,"sourceId":97385,"sourceType":"modelInstanceVersion"},{"modelId":106655,"modelInstanceId":82335,"sourceId":98147,"sourceType":"modelInstanceVersion"},{"modelId":107006,"modelInstanceId":82700,"sourceId":98573,"sourceType":"modelInstanceVersion"},{"modelId":107017,"modelInstanceId":82711,"sourceId":98585,"sourceType":"modelInstanceVersion"},{"modelId":108993,"modelInstanceId":84758,"sourceId":101069,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":109445,"modelInstanceId":85225,"sourceId":101650,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":117231,"modelInstanceId":93025,"sourceId":111042,"sourceType":"modelInstanceVersion"},{"modelId":121705,"modelInstanceId":97518,"sourceId":116074,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":124007,"modelInstanceId":99834,"sourceId":118695,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":124376,"modelInstanceId":100207,"sourceId":119159,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
