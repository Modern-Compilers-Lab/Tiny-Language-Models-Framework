{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8996886,"sourceType":"datasetVersion","datasetId":5419152}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport os\nimport pickle\nimport time\nimport datetime\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import StepLR\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport re","metadata":{"_uuid":"a14f6813-426a-4666-9280-7ed88ebdb85e","_cell_guid":"aa556b17-8ea0-4788-aea8-8d6259526157","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-20T17:22:00.966528Z","iopub.execute_input":"2024-07-20T17:22:00.966880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the random seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\n\n# Set the device to GPU if available, otherwise CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device set to {device}.\")","metadata":{"_uuid":"f10e66ef-f466-4cfc-8ddb-594df92adb45","_cell_guid":"063623b6-121d-4c18-a660-93d2f1be3305","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T14:56:35.876424Z","iopub.execute_input":"2024-07-20T14:56:35.876901Z","iopub.status.idle":"2024-07-20T14:56:35.918570Z","shell.execute_reply.started":"2024-07-20T14:56:35.876871Z","shell.execute_reply":"2024-07-20T14:56:35.917691Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{"_uuid":"f61836e4-3f71-432d-8c50-9de1ff2e05e0","_cell_guid":"4776333c-08cd-4127-bea7-d7ec8898df7b","trusted":true}},{"cell_type":"code","source":"# Helper functions to load and save data\ndef save_data(data, file_path):\n    with open(file_path, 'w') as f:\n        f.write(data)\n\ndef load_data(file_path):\n    with open(file_path, 'r') as f:\n        return f.read()","metadata":{"_uuid":"0aa1c1b8-a945-4baa-8d46-3a08056a9004","_cell_guid":"fcc4b173-f5e5-4110-b14f-46a8fa6da9ae","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T14:56:35.919649Z","iopub.execute_input":"2024-07-20T14:56:35.919920Z","iopub.status.idle":"2024-07-20T14:56:35.925793Z","shell.execute_reply.started":"2024-07-20T14:56:35.919896Z","shell.execute_reply":"2024-07-20T14:56:35.924714Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Directory where the data is stored\nDATA_DIR = \"/kaggle/input/tinylm-data\"","metadata":{"_uuid":"3da5ca68-e0d7-4aed-b89f-5f2a4ab910d9","_cell_guid":"9731ee3f-b4d1-4b6e-afb2-859c56bef6c6","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T14:56:35.927835Z","iopub.execute_input":"2024-07-20T14:56:35.928144Z","iopub.status.idle":"2024-07-20T14:56:35.934561Z","shell.execute_reply.started":"2024-07-20T14:56:35.928121Z","shell.execute_reply":"2024-07-20T14:56:35.933680Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Attempt to derive vocab_size from the dataset\n\nmeta_path = os.path.join(DATA_DIR, 'meta.pkl')\nvocab_size = None\n\nif os.path.exists(meta_path):\n    with open(meta_path, 'rb') as f:\n        meta = pickle.load(f)\n    vocab_size = meta['vocab_size']\n    print(f\"found vocab_size = {vocab_size} (inside {meta_path})\")\nelse:\n    print(\"Meta file not found. Please ensure the meta.pkl file is present in the data directory.\")\n\n# Encode and decode functions for character-level Tokenzation \ndef encode(s):\n    return [meta['stoi'][c] for c in s]\n\ndef decode(l):\n    return ''.join([meta['itos'][i] for i in l])","metadata":{"_uuid":"6d064118-585d-46a9-8f40-f9472fe879b4","_cell_guid":"ddae0037-0f42-425d-a2e9-4238f4c608f2","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T14:56:35.935881Z","iopub.execute_input":"2024-07-20T14:56:35.936315Z","iopub.status.idle":"2024-07-20T14:56:35.954533Z","shell.execute_reply.started":"2024-07-20T14:56:35.936285Z","shell.execute_reply":"2024-07-20T14:56:35.953576Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data\ntrain_data = load_data(os.path.join(DATA_DIR, 'train.txt'))\nval_data = load_data(os.path.join(DATA_DIR, 'val.txt'))\ntest_data = load_data(os.path.join(DATA_DIR, 'test.txt'))\n\n# Encode data\ntrain_ids = encode(train_data)\nval_ids = encode(val_data)\ntest_ids = encode(test_data)\n\n# Save encoded data to bin files, make sure to choose \"Files only\" on the persistence option of the session so that you don't encode data each time\ntrain_ids = np.array(train_ids, dtype=np.uint16)\nval_ids = np.array(val_ids, dtype=np.uint16)\ntest_ids = np.array(test_ids, dtype=np.uint16)\n\ntrain_ids.tofile( 'train.bin')\nval_ids.tofile( 'val.bin')\ntest_ids.tofile('test.bin')\n\nprint(\"Encoded data saved as binary files.\")","metadata":{"_uuid":"1b2892b5-a904-4550-a8d6-ae8f51f1841f","_cell_guid":"ff53a3e0-09ab-4396-90d9-cef86df0605b","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T14:56:35.955777Z","iopub.execute_input":"2024-07-20T14:56:35.956201Z","iopub.status.idle":"2024-07-20T14:58:30.560356Z","shell.execute_reply.started":"2024-07-20T14:56:35.956153Z","shell.execute_reply":"2024-07-20T14:58:30.559345Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del(train_ids)\ndel(val_ids)\ndel(test_ids)","metadata":{"_uuid":"6a2d1ac2-5ef7-441c-9837-050c59120ab9","_cell_guid":"125ce42e-8df9-4094-b0c5-242fcd99a597","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T14:58:30.561546Z","iopub.execute_input":"2024-07-20T14:58:30.561837Z","iopub.status.idle":"2024-07-20T14:58:30.569102Z","shell.execute_reply.started":"2024-07-20T14:58:30.561811Z","shell.execute_reply":"2024-07-20T14:58:30.568232Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load encoded data\ntrain_data = np.memmap(\"/kaggle/working/train.bin\", dtype=np.uint16, mode='r')\nval_data = np.memmap(\"/kaggle/working/val.bin\", dtype=np.uint16, mode='r')","metadata":{"_uuid":"9cd8ff5a-2170-4c53-be17-02ac7d0cffd9","_cell_guid":"c53f3930-8d16-443d-a5ec-a6926f3f6cf4","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T14:58:30.570357Z","iopub.execute_input":"2024-07-20T14:58:30.570777Z","iopub.status.idle":"2024-07-20T14:58:30.611964Z","shell.execute_reply.started":"2024-07-20T14:58:30.570746Z","shell.execute_reply":"2024-07-20T14:58:30.611224Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"_uuid":"f4fc1523-1d72-49db-a3bc-8d521f236993","_cell_guid":"8574d987-cef6-47d1-b889-e8242a0bcd23","trusted":true}},{"cell_type":"code","source":"# Hyperparameters for the GPT model\nblock_size = 256  # Maximum context length\nn_embd = 120      # Embedding dimension\nn_head = 6        # Number of attention heads\nn_layer = 6       # Number of transformer blocks\ndropout = 0       # Dropout rate\nbatch_size = 64   # Batch size for training\nmax_iters = 60000  # Maximum number of iterations\nlearning_rate = 1e-3 # Initial Learning rate value\nmiles = [int(max_iters * m) for m in [0.7, 0.8, 0.9]]  # Milestones for learning rate decay as fractions of max_iters\neval_interval = 10000 # Evaluation interval\neval_iters = 500  # Number of iterations for evaluation\n\ncompile = False # requires PyTorch 2.0","metadata":{"_uuid":"1fd63d8c-f842-444c-9dc8-cab3263ae6e4","_cell_guid":"2d4305c5-c1c6-48b0-a048-953a98954854","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T14:58:30.613008Z","iopub.execute_input":"2024-07-20T14:58:30.613362Z","iopub.status.idle":"2024-07-20T14:58:30.626685Z","shell.execute_reply.started":"2024-07-20T14:58:30.613331Z","shell.execute_reply":"2024-07-20T14:58:30.625843Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\nclass Head(nn.Module):\n    \"\"\"One head of self-attention.\"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B, T, 16)\n        q = self.query(x) # (B, T, 16)\n        v = self.value(x)\n        \n        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=dropout if self.training else 0, is_causal=True)\n            \n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"multiple heads of self-attention in parallel.\"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n    \nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity.\"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd, bias=False),\n            nn.GELU(),\n            nn.Linear( 4 * n_embd, n_embd, bias=False),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n    \nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by feedforward.\"\"\"\n\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd, bias=False)\n        self.ln2 = nn.LayerNorm(n_embd, bias=False)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPT(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd, bias=False) \n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:] # (B, T)\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx","metadata":{"_uuid":"9c3a2af2-99a7-4657-bb8d-168a3e8dfcfb","_cell_guid":"17ff6e02-86d2-4f49-a384-be8c035377a7","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T14:58:30.630074Z","iopub.execute_input":"2024-07-20T14:58:30.630394Z","iopub.status.idle":"2024-07-20T14:58:30.656067Z","shell.execute_reply.started":"2024-07-20T14:58:30.630370Z","shell.execute_reply":"2024-07-20T14:58:30.655234Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get random batch of data\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n# Estimate loss on train and val splits\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters) \n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n\n# Helper function to make large numbers of parameters human-readable\ndef human_readable(num):\n    magnitude = 0\n    while abs(num) >= 1000:\n        magnitude += 1\n        num /= 1000.0\n    return '%.0f%s' % (num, ['', 'K', 'M', 'G', 'T', 'P'][magnitude])","metadata":{"_uuid":"be441d8d-c18b-4694-b2ff-607aac4b11e6","_cell_guid":"a716f789-f605-42d0-9494-d8927ed09a6f","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T14:58:30.657121Z","iopub.execute_input":"2024-07-20T14:58:30.657474Z","iopub.status.idle":"2024-07-20T14:58:30.670624Z","shell.execute_reply.started":"2024-07-20T14:58:30.657443Z","shell.execute_reply":"2024-07-20T14:58:30.669729Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize model and move it to the device (GPU)\nmodel = GPT()\nm = model.to(device)\nnum_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# compile the model\nif compile:\n    print(\"compiling the model... (takes a ~minute)\")\n    model = torch.compile(model) \n    \nnum_parameters_hr = human_readable(num_parameters)\nprint(f'The model has {num_parameters_hr} trainable parameters')","metadata":{"_uuid":"db1edcb0-7dae-40b8-99f0-3a524bd1311e","_cell_guid":"21de39d0-d298-45ce-a590-c6be400f31e8","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T14:58:30.671622Z","iopub.execute_input":"2024-07-20T14:58:30.671881Z","iopub.status.idle":"2024-07-20T14:58:30.893571Z","shell.execute_reply.started":"2024-07-20T14:58:30.671859Z","shell.execute_reply":"2024-07-20T14:58:30.892594Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"_uuid":"8cdf45cc-0d3a-43a9-b10d-5381799a21f2","_cell_guid":"ac1fe251-e0c8-4079-9da4-68aff59262f4","trusted":true}},{"cell_type":"code","source":"# Initialize optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# Initialize learning rate scheduler\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=miles, gamma=0.1)","metadata":{"_uuid":"45093d41-9498-45e4-b93b-95b0b239c0af","_cell_guid":"e725706a-19a1-4e82-91b1-514dd0488f33","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T14:58:30.894863Z","iopub.execute_input":"2024-07-20T14:58:30.895301Z","iopub.status.idle":"2024-07-20T14:58:32.215917Z","shell.execute_reply.started":"2024-07-20T14:58:30.895264Z","shell.execute_reply":"2024-07-20T14:58:32.215087Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get current date and hour to get track of experiments\nnow = datetime.datetime.now()\ndate_hour = now.strftime(\"%Y-%m-%d_%H-%M\")\n\n# Train\n# Start training timer\nstart_time = time.time()\n\n# Training loop\nfor iter in range(max_iters):\n\n    # evaluate the model on the train and val splits and log the losses\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f'iter {iter:5d} | train loss {losses[\"train\"]:.4f} | val loss {losses[\"val\"]:.4f}')\n        \n    # train the model for one iteration\n    xb, yb = get_batch('train')\n\n    # forward pass\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    # Step the scheduler\n    \n    \n    \n    \n    scheduler.step()\n\n# End training timer\nend_time = time.time()\nprint(f'Training time: {(end_time - start_time) / 60}  min')\n\n# Save the trained model\ntorch.save(model.state_dict(), f\"{num_parameters_hr}_{date_hour}.pth\")","metadata":{"_uuid":"534a6c6a-e6b8-4632-8078-86aab93500de","_cell_guid":"76b8e469-893d-4151-a175-99b54dbabe60","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T14:58:32.216993Z","iopub.execute_input":"2024-07-20T14:58:32.217407Z","iopub.status.idle":"2024-07-20T15:28:44.728335Z","shell.execute_reply.started":"2024-07-20T14:58:32.217380Z","shell.execute_reply":"2024-07-20T15:28:44.726717Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{"_uuid":"facd8250-1fd4-4486-a9a6-f099df266caf","_cell_guid":"e831564c-6b76-489b-98b0-69cad098fdd6","trusted":true}},{"cell_type":"code","source":"test_data = np.memmap('test.bin', dtype=np.uint16, mode='r')","metadata":{"_uuid":"f4e10d4c-a4c8-4e6b-891e-f3d14947adfb","_cell_guid":"d8071f1a-961b-4410-ae36-ba54b5b525d0","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T15:28:53.687568Z","iopub.execute_input":"2024-07-20T15:28:53.688312Z","iopub.status.idle":"2024-07-20T15:28:53.719102Z","shell.execute_reply.started":"2024-07-20T15:28:53.688275Z","shell.execute_reply":"2024-07-20T15:28:53.718217Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate example\ndef evaluate_example(example, model, max_new_tokens=30):\n    \n    # Split example and determine maximum new tokens allowed\n    splited_example = example.split(\"# output\")\n    if not (\"for\" in splited_example[0]):\n        max_new_tokens = 22\n    # Encode prompt and prepare for evaluation    \n    encoded_example = torch.tensor(encode(splited_example[0] + \"# output\"), dtype=torch.long).unsqueeze(0).to(device)\n    prompt_text = splited_example[0] + \"# output\"\n    result_example = splited_example[-1]\n    \n    # Extract real results from example\n    real_results = [float(match.group()) for match in re.finditer(r\"(?<=# )-?\\d+(\\.\\d+)?\", result_example.split('\\n\\n')[0].replace(\"\\n\", \"\"))]\n    \n    # Generate response from model and extract generated results\n    response = decode(model.generate(encoded_example, max_new_tokens=max_new_tokens)[0].tolist())\n    splited_response = response.split(\"# output\")\n    result_response = splited_response[-1]\n    generated_results = [float(match.group()) for match in re.finditer(r\"(?<=# )-?\\d+(\\.\\d+)?\", result_response.split('\\n\\n')[0].replace(\"\\n\", \"\"))]\n\n    return prompt_text, real_results, generated_results\n\n\n\n# Write results to file\ndef write_results_to_file(output_file, prompt, real_results, generated_results):\n    df = pd.DataFrame({\n        'Prompt': prompt,\n        'Real_Results': real_results,\n        'Generated_Results': generated_results\n    })\n    df.to_csv(output_file, index=False)","metadata":{"_uuid":"2e9f95ba-ca83-48bc-bb18-8910efc37422","_cell_guid":"f3d6ae4b-e069-43bd-be3f-9e46f19146d3","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T15:31:17.995588Z","iopub.execute_input":"2024-07-20T15:31:17.996005Z","iopub.status.idle":"2024-07-20T15:31:18.006157Z","shell.execute_reply.started":"2024-07-20T15:31:17.995974Z","shell.execute_reply":"2024-07-20T15:31:18.005244Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation Loop\n\n# Split examples and initialize lists for results\nexamples = decode(test_data).split(\"\\n\\n\")\nexamples = [example for example in examples if example]\n\n# Start evaluation process\nprompt = []\nreal_results = []\ngenerated_results = []\n\n# Iterate through examples and evaluate the model on each one\nfor example in tqdm(examples):\n    prompt_text, real_result, result = evaluate_example(example, model)\n    prompt.append(prompt_text)\n    real_results.append(real_result)\n    generated_results.append(result)\n\n# Calculate and print accuracy\ncorrect_count = sum(1 for real, generated in zip(real_results, generated_results) if real == generated)\naccuracy = correct_count / len(generated_results)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n\n# Store accuracy in a file\nwith open(\"accuracy.txt\", 'w') as f:\n    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n\n# Store predictions in a CSV file\n    write_results_to_file(\"predictions.csv\", prompt, real_results, generated_results)","metadata":{"_uuid":"7b21f8fd-2e4c-443b-8120-e0af732bf558","_cell_guid":"2536ece9-1d3c-4373-b308-fd1049f3297f","collapsed":false,"execution":{"iopub.status.busy":"2024-07-20T15:32:25.564254Z","iopub.execute_input":"2024-07-20T15:32:25.564645Z","iopub.status.idle":"2024-07-20T15:32:30.853268Z","shell.execute_reply.started":"2024-07-20T15:32:25.564616Z","shell.execute_reply":"2024-07-20T15:32:30.852339Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}