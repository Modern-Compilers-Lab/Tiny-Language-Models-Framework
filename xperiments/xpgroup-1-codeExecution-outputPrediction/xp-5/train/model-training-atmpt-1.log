|ITERS: 400000 / 6792189 | COMP: 5.89% | RATE: 2.79 it./s | SPD: 0.3585 s/it.| ERT: (26, 12, 33, 24)                                                                                                    
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 8.38 it./s | SPD: 0.1194 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 4)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 2, 53)
	--> val.bin
	--> took (0, 0, 1, 31)

Setting train-hyperparams and util variables:
	--> max_iters = 6792189

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 30M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-02_03-21 : iter     0 <=> epoch 0 | train loss 4.1529 | val loss 4.1522:
	--> training ...
	--> checkpointing ...

2024-10-02_03-52 : iter       5000 <=> epoch 0.002208418986214694 | train loss 0.6571505666 | val loss 0.6579629183:
	--> training ...
	--> checkpointing ...

2024-10-02_04-25 : iter      10000 <=> epoch 0.004416837972429388 | train loss 0.6380867362 | val loss 0.6404500604:
	--> training ...
	--> checkpointing ...

2024-10-02_04-56 : iter      15000 <=> epoch 0.006625256958644082 | train loss 0.6334573627 | val loss 0.6319733858:
	--> training ...
	--> checkpointing ...

2024-10-02_05-28 : iter      20000 <=> epoch 0.008833675944858775 | train loss 0.6256970167 | val loss 0.6267494559:
	--> training ...
	--> checkpointing ...

2024-10-02_06-00 : iter      25000 <=> epoch 0.01104209493107347 | train loss 0.6232506037 | val loss 0.6239672303:
	--> training ...
	--> checkpointing ...

2024-10-02_06-31 : iter      30000 <=> epoch 0.013250513917288164 | train loss 0.6182540655 | val loss 0.6192226410:
	--> training ...
	--> checkpointing ...

2024-10-02_07-03 : iter      35000 <=> epoch 0.01545893290350286 | train loss 0.6181470156 | val loss 0.6167954206:
	--> training ...
	--> checkpointing ...

2024-10-02_07-35 : iter      40000 <=> epoch 0.01766735188971755 | train loss 0.6143714786 | val loss 0.6150822043:
	--> training ...
	--> checkpointing ...

2024-10-02_08-06 : iter      45000 <=> epoch 0.019875770875932248 | train loss 0.6149205565 | val loss 0.6152332425:
	--> training ...
	--> checkpointing ...

2024-10-02_08-38 : iter      50000 <=> epoch 0.02208418986214694 | train loss 0.6136812568 | val loss 0.6140612364:
	--> training ...
	--> checkpointing ...

2024-10-02_09-10 : iter      55000 <=> epoch 0.024292608848361635 | train loss 0.6113308072 | val loss 0.6127268076:
	--> training ...
	--> checkpointing ...

2024-10-02_09-41 : iter      60000 <=> epoch 0.026501027834576328 | train loss 0.6131508350 | val loss 0.6124062538:
	--> training ...
	--> checkpointing ...

2024-10-02_10-13 : iter      65000 <=> epoch 0.02870944682079102 | train loss 0.6119241118 | val loss 0.6104813814:
	--> training ...
	--> checkpointing ...

2024-10-02_10-44 : iter      70000 <=> epoch 0.03091786580700572 | train loss 0.6094288826 | val loss 0.6097817421:
	--> training ...
	--> checkpointing ...

2024-10-02_11-16 : iter      75000 <=> epoch 0.03312628479322041 | train loss 0.6084360480 | val loss 0.6100493073:
	--> training ...
	--> checkpointing ...

2024-10-02_11-47 : iter      80000 <=> epoch 0.0353347037794351 | train loss 0.6091048717 | val loss 0.6084110737:
	--> training ...
	--> checkpointing ...

2024-10-02_12-19 : iter      85000 <=> epoch 0.0375431227656498 | train loss 0.6080473661 | val loss 0.6088900566:
	--> training ...
	--> checkpointing ...

2024-10-02_12-51 : iter      90000 <=> epoch 0.039751541751864496 | train loss 0.6065958142 | val loss 0.6091149449:
	--> training ...
	--> checkpointing ...

2024-10-02_13-22 : iter      95000 <=> epoch 0.041959960738079186 | train loss 0.6067644358 | val loss 0.6073774695:
	--> training ...
	--> checkpointing ...

2024-10-02_13-54 : iter     100000 <=> epoch 0.04416837972429388 | train loss 0.6087557077 | val loss 0.6070531011:
	--> training ...
	--> checkpointing ...

2024-10-02_14-26 : iter     105000 <=> epoch 0.04637679871050857 | train loss 0.6064656973 | val loss 0.6076139808:
	--> training ...
	--> checkpointing ...

2024-10-02_14-57 : iter     110000 <=> epoch 0.04858521769672327 | train loss 0.6067269444 | val loss 0.6059607267:
	--> training ...
	--> checkpointing ...

2024-10-02_15-29 : iter     115000 <=> epoch 0.050793636682937966 | train loss 0.6051300168 | val loss 0.6057843566:
	--> training ...
	--> checkpointing ...

2024-10-02_16-01 : iter     120000 <=> epoch 0.053002055669152656 | train loss 0.6054717898 | val loss 0.6063292623:
	--> training ...
	--> checkpointing ...

2024-10-02_16-33 : iter     125000 <=> epoch 0.05521047465536735 | train loss 0.6061344743 | val loss 0.6057439446:
	--> training ...
	--> checkpointing ...

2024-10-02_17-04 : iter     130000 <=> epoch 0.05741889364158204 | train loss 0.6056591272 | val loss 0.6078667045:
	--> training ...
	--> checkpointing ...

2024-10-02_17-36 : iter     135000 <=> epoch 0.05962731262779674 | train loss 0.6042066813 | val loss 0.6042064428:
	--> training ...
	--> checkpointing ...

2024-10-02_18-08 : iter     140000 <=> epoch 0.06183573161401144 | train loss 0.6045489907 | val loss 0.6054960489:
	--> training ...
	--> checkpointing ...

2024-10-02_18-39 : iter     145000 <=> epoch 0.06404415060022613 | train loss 0.6060239077 | val loss 0.6045843959:
	--> training ...
	--> checkpointing ...

2024-10-02_19-11 : iter     150000 <=> epoch 0.06625256958644082 | train loss 0.6052364111 | val loss 0.6045990586:
	--> training ...
	--> checkpointing ...

2024-10-02_19-43 : iter     155000 <=> epoch 0.06846098857265552 | train loss 0.6037033796 | val loss 0.6042101979:
	--> training ...
	--> checkpointing ...

2024-10-02_20-14 : iter     160000 <=> epoch 0.0706694075588702 | train loss 0.6028453708 | val loss 0.6030650139:
	--> training ...
	--> checkpointing ...

2024-10-02_20-46 : iter     165000 <=> epoch 0.0728778265450849 | train loss 0.6033636928 | val loss 0.6030762196:
	--> training ...
	--> checkpointing ...

2024-10-02_21-18 : iter     170000 <=> epoch 0.0750862455312996 | train loss 0.6033759713 | val loss 0.6036798358:
	--> training ...
	--> checkpointing ...

2024-10-02_21-49 : iter     175000 <=> epoch 0.0772946645175143 | train loss 0.6030404568 | val loss 0.6033511758:
	--> training ...
	--> checkpointing ...

2024-10-02_22-21 : iter     180000 <=> epoch 0.07950308350372899 | train loss 0.6044967771 | val loss 0.6030460596:
	--> training ...
	--> checkpointing ...

2024-10-02_22-53 : iter     185000 <=> epoch 0.08171150248994367 | train loss 0.6035633087 | val loss 0.6050740480:
	--> training ...
	--> checkpointing ...

2024-10-02_23-24 : iter     190000 <=> epoch 0.08391992147615837 | train loss 0.6038053632 | val loss 0.6032382846:
	--> training ...
	--> checkpointing ...

2024-10-02_23-56 : iter     195000 <=> epoch 0.08612834046237307 | train loss 0.6042562723 | val loss 0.6038897038:
	--> training ...
	--> checkpointing ...

2024-10-03_00-28 : iter     200000 <=> epoch 0.08833675944858776 | train loss 0.6017351747 | val loss 0.6056787372:
	--> training ...
	--> checkpointing ...

2024-10-03_00-59 : iter     205000 <=> epoch 0.09054517843480246 | train loss 0.6015030742 | val loss 0.6030330658:
	--> training ...
	--> checkpointing ...

2024-10-03_01-31 : iter     210000 <=> epoch 0.09275359742101714 | train loss 0.6015931368 | val loss 0.6033543944:
	--> training ...
	--> checkpointing ...

2024-10-03_02-03 : iter     215000 <=> epoch 0.09496201640723184 | train loss 0.6019971967 | val loss 0.6033992171:
	--> training ...
	--> checkpointing ...

2024-10-03_02-34 : iter     220000 <=> epoch 0.09717043539344654 | train loss 0.6018151641 | val loss 0.6013380289:
	--> training ...
	--> checkpointing ...

2024-10-03_03-06 : iter     225000 <=> epoch 0.09937885437966124 | train loss 0.6036648154 | val loss 0.6036788225:
	--> training ...
	--> checkpointing ...

2024-10-03_03-38 : iter     230000 <=> epoch 0.10158727336587593 | train loss 0.6032592654 | val loss 0.6019942760:
	--> training ...
	--> checkpointing ...

2024-10-03_04-10 : iter     235000 <=> epoch 0.10379569235209062 | train loss 0.6024428010 | val loss 0.6012475491:
	--> training ...
	--> checkpointing ...

2024-10-03_04-41 : iter     240000 <=> epoch 0.10600411133830531 | train loss 0.6012635827 | val loss 0.6030955911:
	--> training ...
	--> checkpointing ...

2024-10-03_05-13 : iter     245000 <=> epoch 0.10821253032452001 | train loss 0.6025944948 | val loss 0.6043007374:
	--> training ...
	--> checkpointing ...

2024-10-03_05-45 : iter     250000 <=> epoch 0.1104209493107347 | train loss 0.6028639674 | val loss 0.6026862860:
	--> training ...
	--> checkpointing ...

2024-10-03_06-16 : iter     255000 <=> epoch 0.1126293682969494 | train loss 0.6010313034 | val loss 0.6015847325:
	--> training ...
	--> checkpointing ...

2024-10-03_06-48 : iter     260000 <=> epoch 0.11483778728316409 | train loss 0.6020066738 | val loss 0.6025670767:
	--> training ...
	--> checkpointing ...

2024-10-03_07-20 : iter     265000 <=> epoch 0.11704620626937878 | train loss 0.6015477777 | val loss 0.6017661095:
	--> training ...
	--> checkpointing ...

2024-10-03_07-51 : iter     270000 <=> epoch 0.11925462525559348 | train loss 0.6013019085 | val loss 0.6016370058:
	--> training ...
	--> checkpointing ...

2024-10-03_08-23 : iter     275000 <=> epoch 0.12146304424180818 | train loss 0.6016970277 | val loss 0.6008374095:
	--> training ...
	--> checkpointing ...

2024-10-03_08-55 : iter     280000 <=> epoch 0.12367146322802287 | train loss 0.6014593244 | val loss 0.6020722985:
	--> training ...
	--> checkpointing ...

2024-10-03_09-26 : iter     285000 <=> epoch 0.12587988221423757 | train loss 0.6010342836 | val loss 0.6017381549:
	--> training ...
	--> checkpointing ...

2024-10-03_09-58 : iter     290000 <=> epoch 0.12808830120045225 | train loss 0.6010420322 | val loss 0.6019151211:
	--> training ...
	--> checkpointing ...

2024-10-03_10-30 : iter     295000 <=> epoch 0.13029672018666694 | train loss 0.6023580432 | val loss 0.6013076305:
	--> training ...
	--> checkpointing ...

2024-10-03_11-01 : iter     300000 <=> epoch 0.13250513917288165 | train loss 0.5998196006 | val loss 0.6012403369:
	--> training ...
	--> checkpointing ...

2024-10-03_11-33 : iter     305000 <=> epoch 0.13471355815909633 | train loss 0.6001894474 | val loss 0.6017575264:
	--> training ...
	--> checkpointing ...

2024-10-03_12-05 : iter     310000 <=> epoch 0.13692197714531104 | train loss 0.6002645493 | val loss 0.6019032598:
	--> training ...
	--> checkpointing ...

2024-10-03_12-36 : iter     315000 <=> epoch 0.13913039613152572 | train loss 0.6006474495 | val loss 0.6008545756:
	--> training ...
	--> checkpointing ...

2024-10-03_13-08 : iter     320000 <=> epoch 0.1413388151177404 | train loss 0.6029533744 | val loss 0.6005489230:
	--> training ...
	--> checkpointing ...

2024-10-03_13-40 : iter     325000 <=> epoch 0.14354723410395512 | train loss 0.6018894911 | val loss 0.6025612354:
	--> training ...
	--> checkpointing ...

2024-10-03_14-11 : iter     330000 <=> epoch 0.1457556530901698 | train loss 0.6008696556 | val loss 0.6000924110:
	--> training ...
	--> checkpointing ...

2024-10-03_14-43 : iter     335000 <=> epoch 0.1479640720763845 | train loss 0.6011112928 | val loss 0.6019214392:
	--> training ...
	--> checkpointing ...

2024-10-03_15-15 : iter     340000 <=> epoch 0.1501724910625992 | train loss 0.6003690958 | val loss 0.6016303897:
	--> training ...
	--> checkpointing ...

2024-10-03_15-46 : iter     345000 <=> epoch 0.15238091004881388 | train loss 0.5997262597 | val loss 0.6024017930:
	--> training ...
	--> checkpointing ...

2024-10-03_16-18 : iter     350000 <=> epoch 0.1545893290350286 | train loss 0.6012148857 | val loss 0.6013091803:
	--> training ...
	--> checkpointing ...

2024-10-03_16-50 : iter     355000 <=> epoch 0.15679774802124327 | train loss 0.6021685600 | val loss 0.6004719734:
	--> training ...
	--> checkpointing ...

2024-10-03_17-21 : iter     360000 <=> epoch 0.15900616700745798 | train loss 0.6010605097 | val loss 0.6013142467:
	--> training ...
	--> checkpointing ...

2024-10-03_17-53 : iter     365000 <=> epoch 0.16121458599367267 | train loss 0.6015254855 | val loss 0.6015218496:
	--> training ...
	--> checkpointing ...

2024-10-03_18-25 : iter     370000 <=> epoch 0.16342300497988735 | train loss 0.6009197235 | val loss 0.6010703444:
	--> training ...
	--> checkpointing ...

2024-10-03_18-56 : iter     375000 <=> epoch 0.16563142396610206 | train loss 0.6014112830 | val loss 0.6005527377:
	--> training ...
	--> checkpointing ...

2024-10-03_19-29 : iter     380000 <=> epoch 0.16783984295231674 | train loss 0.5985925198 | val loss 0.6005563736:
	--> training ...
	--> checkpointing ...

2024-10-03_20-00 : iter     385000 <=> epoch 0.17004826193853145 | train loss 0.6014177799 | val loss 0.5993712544:
	--> training ...
	--> checkpointing ...

2024-10-03_20-32 : iter     390000 <=> epoch 0.17225668092474614 | train loss 0.6006208062 | val loss 0.6011231542:
	--> training ...
	--> checkpointing ...

2024-10-03_21-04 : iter     395000 <=> epoch 0.17446509991096082 | train loss 0.5998015404 | val loss 0.6012008786:
	--> training ...
	--> checkpointing ...

2024-10-03_21-36 : iter     400000 <=> epoch 0.17667351889717553 | train loss 0.5984551311 | val loss 0.6013513803:

EARLY STOPPING at iter 400000 == epoch 0.17667351889717553:
