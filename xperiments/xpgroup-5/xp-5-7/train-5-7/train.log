|ITERS: 198499 / 198499 | EPOCHS: 1.70 | COMP: 100.00% | RATE: 2.49 it./s | SPD: 0.4019 s/it.| ERT: (0, 0, 0, 0) | ET: (1, 0, 24, 42)                                                                   
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 5.83 it./s | SPD: 0.1715 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Finished setup:

Initilizaing neptune:
	--> neptune init
	--> saving the neptune runid

Set the random seed for reproducibility:

Setting the device to GPU if available, otherwise CPU:
	--> device set to cuda:4.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> loading train.bin
	--> took (0, 0, 0, 0)
	--> loading val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:

Defining the model and utilities:

The model:
	--> def human readable

Creating the model:

The model has 30M trainable parameters:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

==========================================================================================:

2025-01-06_10-46 : iter     0 <=> epoch 0 | train loss 4.5516 | val loss 4.5510:
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_11-42 : iter       5000 <=> epoch 0.04282119414952905 | train loss 0.6826151013 | val loss 0.6910493970
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_12-19 : iter      10000 <=> epoch 0.0856423882990581 | train loss 0.6244997382 | val loss 0.6638470292
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_12-55 : iter      15000 <=> epoch 0.12846358244858716 | train loss 0.5807864666 | val loss 0.6822074652
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_13-32 : iter      20000 <=> epoch 0.1712847765981162 | train loss 0.5461681485 | val loss 0.6836411357
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_14-08 : iter      25000 <=> epoch 0.21410597074764526 | train loss 0.5279647708 | val loss 0.6850953102
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_14-45 : iter      30000 <=> epoch 0.2569271648971743 | train loss 0.5138344169 | val loss 0.6891344190
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_15-21 : iter      35000 <=> epoch 0.2997483590467033 | train loss 0.5020445585 | val loss 0.6928574443
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_15-58 : iter      40000 <=> epoch 0.3425695531962324 | train loss 0.4922226369 | val loss 0.6908944249
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_16-34 : iter      45000 <=> epoch 0.38539074734576145 | train loss 0.4827213585 | val loss 0.6897424459
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_17-11 : iter      50000 <=> epoch 0.4282119414952905 | train loss 0.4743593633 | val loss 0.7092136741
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_17-47 : iter      55000 <=> epoch 0.4710331356448195 | train loss 0.4707641900 | val loss 0.6928042173
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_18-23 : iter      60000 <=> epoch 0.5138543297943486 | train loss 0.4629235864 | val loss 0.6887003183
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_19-00 : iter      65000 <=> epoch 0.5566755239438776 | train loss 0.4569964111 | val loss 0.6866936088
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_19-36 : iter      70000 <=> epoch 0.5994967180934067 | train loss 0.4521945119 | val loss 0.6869808435
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_20-13 : iter      75000 <=> epoch 0.6423179122429358 | train loss 0.4470514059 | val loss 0.6840841770
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_20-49 : iter      80000 <=> epoch 0.6851391063924648 | train loss 0.4474208951 | val loss 0.6860154867
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_21-26 : iter      85000 <=> epoch 0.7279603005419939 | train loss 0.4421241283 | val loss 0.6914137602
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_22-02 : iter      90000 <=> epoch 0.7707814946915229 | train loss 0.4380545020 | val loss 0.6909435391
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_22-39 : iter      95000 <=> epoch 0.8136026888410519 | train loss 0.4361071885 | val loss 0.6821281314
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_23-15 : iter     100000 <=> epoch 0.856423882990581 | train loss 0.4302044511 | val loss 0.6916689873
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-06_23-51 : iter     105000 <=> epoch 0.89924507714011 | train loss 0.4299013019 | val loss 0.6996010542
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_00-28 : iter     110000 <=> epoch 0.942066271289639 | train loss 0.4274669886 | val loss 0.6937310100
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_01-04 : iter     115000 <=> epoch 0.9848874654391682 | train loss 0.4238563180 | val loss 0.6984232068
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_01-41 : iter     120000 <=> epoch 1.0277086595886973 | train loss 0.4227206111 | val loss 0.6891571283
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_02-17 : iter     125000 <=> epoch 1.0705298537382262 | train loss 0.4206952751 | val loss 0.6939511299
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_02-54 : iter     130000 <=> epoch 1.1133510478877553 | train loss 0.4159356356 | val loss 0.6991806626
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_03-30 : iter     135000 <=> epoch 1.1561722420372844 | train loss 0.4175975919 | val loss 0.6932086349
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_04-06 : iter     140000 <=> epoch 1.1989934361868133 | train loss 0.4001840949 | val loss 0.6949269176
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_04-43 : iter     145000 <=> epoch 1.2418146303363424 | train loss 0.3870876431 | val loss 0.6931148171
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_05-19 : iter     150000 <=> epoch 1.2846358244858715 | train loss 0.3834077716 | val loss 0.7032591701
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_05-56 : iter     155000 <=> epoch 1.3274570186354004 | train loss 0.3801760077 | val loss 0.7100485563
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_06-32 : iter     160000 <=> epoch 1.3702782127849296 | train loss 0.3748630285 | val loss 0.7163372040
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_07-09 : iter     165000 <=> epoch 1.4130994069344587 | train loss 0.3724225461 | val loss 0.7147067785
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_07-45 : iter     170000 <=> epoch 1.4559206010839878 | train loss 0.3752619922 | val loss 0.7135769129
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_08-21 : iter     175000 <=> epoch 1.4987417952335167 | train loss 0.3726472259 | val loss 0.7155493498
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_08-58 : iter     180000 <=> epoch 1.5415629893830458 | train loss 0.3742060065 | val loss 0.7150237560
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_09-34 : iter     185000 <=> epoch 1.584384183532575 | train loss 0.3737430274 | val loss 0.7165377140
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_10-11 : iter     190000 <=> epoch 1.6272053776821038 | train loss 0.3736623526 | val loss 0.7175621986
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_10-47 : iter     195000 <=> epoch 1.670026571831633 | train loss 0.3752039671 | val loss 0.7182449102
	--> training ...
	--> checkpointing
	--> checkpoint_2025-01-07_11-13 : iter     198499 <=> epoch 1.6999928434974734 | train loss 0.3748915493 | val loss 0.7187349796
	--> training ...
