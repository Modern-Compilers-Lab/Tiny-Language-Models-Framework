{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tinypy_code_tracer_m2_tokenizer import TinypyTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpt = TinypyTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/yb2618/Tiny-Language-Models-Framework/datasets/dataset-20/datapreps-20/dataprep-20-1/data-dp-20-1/train.txt\", 'r') as f:\n",
    "\tdata = f.read()\n",
    "sft_examples = data.split('\\n\\n')[:1000]\n",
    "with open('sft_examples.txt', 'w') as f:\n",
    "\t\tf.write('\\n\\n'.join(sft_examples)+'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator class over the steps pairs\n",
    "class StepsPairsIterator:\n",
    "\n",
    "\tdef __init__(self, data_path, shuffle):\n",
    "\t\tself.index = 0\n",
    "\n",
    "\t\twith open(data_path, 'r') as f:\n",
    "\t\t\tdata = f.read()\n",
    "\t\t\n",
    "\t\t# Get the code trace examples\n",
    "\t\tcode_trace_examples = data.split('\\n\\n')[-1] # We ignore the last element of the list because it is an empty string\n",
    "\t\t\n",
    "\t\t# Iterate over the code trace examples and extract the steps pairs\n",
    "\t\tsteps_pairs = []\n",
    "\t\tfor code_trace_example in code_trace_examples:\n",
    "\t\t\tsteps = code_trace_example.split('\\n#STEP\\n')\n",
    "\t\t\tfor i in range(len(steps)-1):\n",
    "\t\t\t\tinput = steps[i]\n",
    "\t\t\t\toutput = '\\n#STEP\\n' + steps[i+1]\n",
    "\t\t\t\tif i != len(steps)-2:\n",
    "\t\t\t\t\toutput += '\\n#STEP\\n'\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\toutput += '\\n\\n'\n",
    "\t\t\t\tsteps_pairs.append((input, output))\n",
    "\t\t\n",
    "\t\tself.steps_pairs = steps_pairs\n",
    "\t\tif shuffle:\n",
    "\t\t\trandom.shuffle(self.steps_pairs)\n",
    "\n",
    "\t\tself.tpt = TinypyTokenizer()\n",
    "\n",
    "\tdef __iter__(self):\n",
    "\t\treturn self\n",
    "\n",
    "\tdef __next__(self):\n",
    "\n",
    "\t\tif self.index < len(self.steps_pairs):\n",
    "\t\t\tx = self.tpt.encode(self.steps_pairs[self.index][0])\n",
    "\t\t\ty = self.tpt.encode(self.steps_pairs[self.index][1])\n",
    "\t\t\tx = x + y[:-1]\n",
    "\t\t\tx = torch.tensor(x, dtype=torch.int64).view(1,-1)\n",
    "\t\t\ty = torch.tensor(y, dtype=torch.int64).view(1,-1)\n",
    "\t\t\tself.index += 1\n",
    "\t\t\treturn x, y\n",
    "\t\telse:\n",
    "\t\t\traise StopIteration\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# code\\n', 'y', '=', '2', '5', '5', '\\n', 'd', '=', '1', '4', '4', '\\n', 's', '=', 'd', '/', '2', '3', '8', '\\n', 'if', '1', '1', '7', '>=', '6', '6', ':', '\\n', '\\t', 'm', '=', '1', '1', '9', '%', 'd', '\\n', '\\t', 'l', '=', 'd', '//', 's', '\\n', '\\t', 'v', '=', '1', '1', '9', '\\n', '\\t', 'h', '=', '1', '5', '5', '\\n', 'z', '=', '1', '3', '\\n', 't', '=', '2', '1', '5', '*', '3', '9', '\\n', 'k', '=', '2', '0', '9', '\\n', 'm', '=', 'z', '//', '1', '3', '1', '\\n', 'o', '=', '1', '5', '+', 'd', '\\n', 'while', 'k', '<', '2', '6', '0', ':', '\\n', '\\t', 'k', '=', 'k', '+', 'z', '\\n', 'print(', 'd', ')', '\\n#STEP\\n', '# code\\n', '@', 'y', '=', '2', '5', '5', '$', '|', '\\n', 'd', '=', '1', '4', '4', '\\n', 's', '=', 'd', '/', '2', '3', '8', '\\n', 'if', '1', '1', '7', '>=', '6', '6', ':', '\\n', '\\t', 'm', '=', '1', '1', '9', '%', 'd', '\\n', '\\t', 'l', '=', 'd', '//', 's', '\\n', '\\t', 'v', '=', '1', '1', '9', '\\n', '\\t', 'h', '=', '1', '5', '5', '\\n', 'z', '=', '1', '3', '\\n', 't', '=', '2', '1', '5', '*', '3', '9', '\\n', 'k', '=', '2', '0', '9', '\\n', 'm', '=', 'z', '//', '1', '3', '1', '\\n', 'o', '=', '1', '5', '+', 'd', '\\n', 'while', 'k', '<', '2', '6', '0', ':', '\\n', '\\t', 'k', '=', 'k', '+', 'z', '\\n', 'print(', 'd', ')']\n",
      "['\\n#STEP\\n', '# code\\n', '@', 'y', '=', '2', '5', '5', '$', '|', '\\n', 'd', '=', '1', '4', '4', '\\n', 's', '=', 'd', '/', '2', '3', '8', '\\n', 'if', '1', '1', '7', '>=', '6', '6', ':', '\\n', '\\t', 'm', '=', '1', '1', '9', '%', 'd', '\\n', '\\t', 'l', '=', 'd', '//', 's', '\\n', '\\t', 'v', '=', '1', '1', '9', '\\n', '\\t', 'h', '=', '1', '5', '5', '\\n', 'z', '=', '1', '3', '\\n', 't', '=', '2', '1', '5', '*', '3', '9', '\\n', 'k', '=', '2', '0', '9', '\\n', 'm', '=', 'z', '//', '1', '3', '1', '\\n', 'o', '=', '1', '5', '+', 'd', '\\n', 'while', 'k', '<', '2', '6', '0', ':', '\\n', '\\t', 'k', '=', 'k', '+', 'z', '\\n', 'print(', 'd', ')', '\\n#STEP\\n']\n"
     ]
    }
   ],
   "source": [
    "for x, y in StepsPairsIterator('sft_examples.txt', False):\n",
    "\tx = tpt.decode(x[0].tolist())\n",
    "\ty = tpt.decode(y[0].tolist())\n",
    "\tprint(x)\n",
    "\tprint(y)\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'r') as f:\n",
    "\tdata = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeTracesIterator:\n",
    "\tdef __init__(self, data_path, block_size, shuffle):\n",
    "\t\tprint('Initializing CodeTracesIterator ...')\n",
    "\t\tself.data_path = data_path\n",
    "\t\tself.block_size = block_size\n",
    "\t\tself.index = 0\n",
    "\t\tprint('Reading data ...')\n",
    "\t\twith open(data_path, 'r') as f:\n",
    "\t\t\tdata = f.read()\n",
    "\t\t\n",
    "\t\t# Tokenize the data\n",
    "\t\tprint('Tokenizing data ...')\n",
    "\t\tself.data_tokens = tpt.tokenize(data)\n",
    "\t\t\n",
    "\t\t# Get the indices of all '# code\\n' tokens in the data\n",
    "\t\tprint('Getting boundary tokens indices ...')\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tif shuffle:\n",
    "\t\t\tprint('Shuffling ...')\n",
    "\t\t\trandom.shuffle(self.examples_indices)\n",
    "\n",
    "\tdef __iter__(self):\n",
    "\t\treturn self\n",
    "\t\n",
    "\tdef __next__(self):\n",
    "\t\tif self.index < len(self.examples_indices):\n",
    "\t\t\tchunk = self.data_tokens[self.examples_indices[self.index] : self.examples_indices[self.index] + self.block_size + 1]\n",
    "\t\t\t\n",
    "\t\t\toutput_idx = None\n",
    "\t\t\tfor i, token in enumerate(chunk):\n",
    "\t\t\t\tif token == '\\n#STEP\\n' or token == '\\n\\n':\n",
    "\t\t\t\t\toutput_idx = i\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\tif '\\n\\n' in chunk:\n",
    "\t\t\t\tnn_idx = chunk.index('\\n\\n')\n",
    "\t\t\t\tx = chunk[:nn_idx]\n",
    "\t\t\t\ty = chunk[output_idx:nn_idx+1]\n",
    "\t\t\telse:\n",
    "\t\t\t\tx = chunk[:-1]\n",
    "\t\t\t\ty = chunk[output_idx:]\n",
    "\n",
    "\t\t\tx = torch.tensor(tpt.encode_tokens_list(x), dtype=torch.int64).view(1,-1)\n",
    "\t\t\ty = torch.tensor(tpt.encode_tokens_list(y), dtype=torch.int64).view(1,-1)\n",
    "\t\t\t\n",
    "\t\t\tself.index += 1\n",
    "\t\t\t\n",
    "\t\t\treturn x, y\n",
    "\t\telse:\n",
    "\t\t\traise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data ...\n"
     ]
    }
   ],
   "source": [
    "with open('sft_examples.txt', 'r') as f:\n",
    "\t\t\tdata = f.read()\n",
    "\t\t\n",
    "# Tokenize the data\n",
    "print('Tokenizing data ...')\n",
    "data_tokens = tpt.tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CodeTracesIterator ...\n",
      "Reading data ...\n",
      "Tokenizing data ...\n",
      "Getting boundary tokens indices ...\n",
      "['# code\\n', 'y', '=', '2', '5', '5', '\\n', 'd', '=', '1', '4', '4', '\\n', 's', '=', 'd', '/', '2', '3', '8', '\\n', 'if', '1', '1', '7', '>=', '6', '6', ':', '\\n', '\\t', 'm', '=', '1', '1', '9', '%', 'd', '\\n', '\\t', 'l', '=', 'd', '//', 's', '\\n', '\\t', 'v', '=', '1', '1', '9', '\\n', '\\t', 'h', '=', '1', '5', '5', '\\n', 'z', '=', '1', '3', '\\n', 't', '=', '2', '1', '5', '*', '3', '9', '\\n', 'k', '=', '2', '0', '9', '\\n', 'm', '=', 'z', '//', '1', '3', '1', '\\n', 'o', '=', '1', '5', '+', 'd', '\\n', 'while', 'k', '<', '2', '6', '0', ':', '\\n', '\\t', 'k', '=', 'k', '+', 'z', '\\n', 'print(', 'd', ')', '\\n', '@', '^', '$', 'y', '?', '2', '5', '5', ';', 'd', '?', '1', '4', '4', ';', 's', '?', '0', '.', '6', '0', '5', '0', '4', '2', '0', '1', '6', '8', '0', '6', '7', '2', '2', '6', ';', 'm', '?', '0', ';', 'l', '?', '2', '3', '8', '.', '0', ';', 'v', '?', '1', '1', '9', ';', 'h', '?', '1', '5', '5', ';', 'z', '?', '1', '3', ';', 't', '?', '8', '3', '8', '5', ';', 'k', '?', '2', '6', '1', ';', 'o', '?', '1', '5', '9', '|', '# ', '1', '4', '4']\n",
      "['\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(cti:=CodeTracesIterator('sft_examples.txt', 512, shuffle=False)):\n",
    "\tx = tpt.decode(x[0].tolist())\n",
    "\ty = tpt.decode(y[0].tolist())\n",
    "\tif cti.index == 25:\n",
    "\t\tbreak\n",
    "print(x)\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinylm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
