|ITERS: 188570 / 188570 | COMP: 100.00% | RATE: 2.67 it./s | SPD: 0.3752 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 8.34 it./s | SPD: 0.1199 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:1.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 188570

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the vocab_size:
	--> loading

Creating the model:

The model has 30M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-12-04_14-27 : iter     0 <=> epoch 0 | train loss 4.2922 | val loss 4.2930:
	--> training ...
	--> checkpointing ...

2024-12-04_15-00 : iter       5000 <=> epoch 0.02651528359330939 | train loss 1.0711438656 | val loss 1.0699831247:
	--> training ...
	--> checkpointing ...

2024-12-04_15-33 : iter      10000 <=> epoch 0.05303056718661878 | train loss 1.0547335148 | val loss 1.0541336536:
	--> training ...
	--> checkpointing ...

2024-12-04_16-07 : iter      15000 <=> epoch 0.07954585077992816 | train loss 1.0263251066 | val loss 1.0261250734:
	--> training ...
	--> checkpointing ...

2024-12-04_16-40 : iter      20000 <=> epoch 0.10606113437323755 | train loss 1.0158380270 | val loss 1.0147925615:
	--> training ...
	--> checkpointing ...

2024-12-04_17-13 : iter      25000 <=> epoch 0.13257641796654693 | train loss 1.0097292662 | val loss 1.0091897249:
	--> training ...
	--> checkpointing ...

2024-12-04_17-47 : iter      30000 <=> epoch 0.15909170155985633 | train loss 1.0038899183 | val loss 1.0056693554:
	--> training ...
	--> checkpointing ...

2024-12-04_18-20 : iter      35000 <=> epoch 0.1856069851531657 | train loss 1.0009158850 | val loss 1.0013759136:
	--> training ...
	--> checkpointing ...

2024-12-04_18-53 : iter      40000 <=> epoch 0.2121222687464751 | train loss 0.9985774159 | val loss 0.9976596832:
	--> training ...
	--> checkpointing ...

2024-12-04_19-27 : iter      45000 <=> epoch 0.23863755233978448 | train loss 0.9936999679 | val loss 0.9935790896:
	--> training ...
	--> checkpointing ...

2024-12-04_20-00 : iter      50000 <=> epoch 0.26515283593309386 | train loss 0.9932302237 | val loss 0.9937574267:
	--> training ...
	--> checkpointing ...

2024-12-04_20-33 : iter      55000 <=> epoch 0.29166811952640326 | train loss 0.9931606650 | val loss 0.9916133881:
	--> training ...
	--> checkpointing ...

2024-12-04_21-06 : iter      60000 <=> epoch 0.31818340311971266 | train loss 0.9915499687 | val loss 0.9909816980:
	--> training ...
	--> checkpointing ...

2024-12-04_21-40 : iter      65000 <=> epoch 0.34469868671302206 | train loss 0.9913340807 | val loss 0.9912971258:
	--> training ...
	--> checkpointing ...

2024-12-04_22-13 : iter      70000 <=> epoch 0.3712139703063314 | train loss 0.9898744226 | val loss 0.9896915555:
	--> training ...
	--> checkpointing ...

2024-12-04_22-46 : iter      75000 <=> epoch 0.3977292538996408 | train loss 0.9907582402 | val loss 0.9896784425:
	--> training ...
	--> checkpointing ...

2024-12-04_23-19 : iter      80000 <=> epoch 0.4242445374929502 | train loss 0.9889566898 | val loss 0.9883534908:
	--> training ...
	--> checkpointing ...

2024-12-04_23-55 : iter      85000 <=> epoch 0.4507598210862596 | train loss 0.9881873131 | val loss 0.9883629084:
	--> training ...
	--> checkpointing ...

2024-12-05_00-32 : iter      90000 <=> epoch 0.47727510467956896 | train loss 0.9866116047 | val loss 0.9878935218:
	--> training ...
	--> checkpointing ...

2024-12-05_01-11 : iter      95000 <=> epoch 0.5037903882728784 | train loss 0.9868537188 | val loss 0.9868855476:
	--> training ...
	--> checkpointing ...

2024-12-05_01-45 : iter     100000 <=> epoch 0.5303056718661877 | train loss 0.9881154895 | val loss 0.9892672896:
	--> training ...
	--> checkpointing ...

2024-12-05_02-19 : iter     105000 <=> epoch 0.5568209554594972 | train loss 0.9878520370 | val loss 0.9870976210:
	--> training ...
	--> checkpointing ...

2024-12-05_02-52 : iter     110000 <=> epoch 0.5833362390528065 | train loss 0.9856981635 | val loss 0.9859294295:
	--> training ...
	--> checkpointing ...

2024-12-05_03-25 : iter     115000 <=> epoch 0.609851522646116 | train loss 0.9859562516 | val loss 0.9848802090:
	--> training ...
	--> checkpointing ...

2024-12-05_04-02 : iter     120000 <=> epoch 0.6363668062394253 | train loss 0.9851568937 | val loss 0.9850612283:
	--> training ...
	--> checkpointing ...

2024-12-05_04-40 : iter     125000 <=> epoch 0.6628820898327347 | train loss 0.9848827720 | val loss 0.9853804111:
	--> training ...
	--> checkpointing ...

2024-12-05_05-18 : iter     130000 <=> epoch 0.6893973734260441 | train loss 0.9853506088 | val loss 0.9840115905:
	--> training ...
	--> checkpointing ...

2024-12-05_05-56 : iter     135000 <=> epoch 0.7159126570193535 | train loss 0.9809914827 | val loss 0.9823491573:
	--> training ...
	--> checkpointing ...

2024-12-05_06-33 : iter     140000 <=> epoch 0.7424279406126628 | train loss 0.9792439938 | val loss 0.9800421000:
	--> training ...
	--> checkpointing ...

2024-12-05_07-06 : iter     145000 <=> epoch 0.7689432242059723 | train loss 0.9794772863 | val loss 0.9801535606:
	--> training ...
	--> checkpointing ...

2024-12-05_07-39 : iter     150000 <=> epoch 0.7954585077992816 | train loss 0.9803117514 | val loss 0.9796488285:
	--> training ...
	--> checkpointing ...

2024-12-05_08-12 : iter     155000 <=> epoch 0.8219737913925911 | train loss 0.9793628454 | val loss 0.9797777534:
	--> training ...
	--> checkpointing ...

2024-12-05_08-45 : iter     160000 <=> epoch 0.8484890749859004 | train loss 0.9797744751 | val loss 0.9803109169:
	--> training ...
	--> checkpointing ...

2024-12-05_09-19 : iter     165000 <=> epoch 0.8750043585792098 | train loss 0.9792456031 | val loss 0.9787815213:
	--> training ...
	--> checkpointing ...

2024-12-05_09-52 : iter     170000 <=> epoch 0.9015196421725192 | train loss 0.9797896743 | val loss 0.9809449315:
	--> training ...
	--> checkpointing ...

2024-12-05_10-25 : iter     175000 <=> epoch 0.9280349257658286 | train loss 0.9801996350 | val loss 0.9799596071:
	--> training ...
	--> checkpointing ...

2024-12-05_10-59 : iter     180000 <=> epoch 0.9545502093591379 | train loss 0.9783917069 | val loss 0.9802095294:
	--> training ...
	--> checkpointing ...

2024-12-05_11-32 : iter     185000 <=> epoch 0.9810654929524474 | train loss 0.9802471399 | val loss 0.9803891778:
	--> training ...
