|ITERS: 184128 / 184128 | COMP: 100.00% | RATE: 2.67 it./s | SPD: 0.3742 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 8.39 it./s | SPD: 0.1191 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:0.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 184128

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the vocab_size:
	--> loading

Creating the model:

The model has 30M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-06_10-46 : iter     0 <=> epoch 0 | train loss 4.5029 | val loss 4.5029:
	--> training ...
	--> checkpointing ...

2024-11-06_11-20 : iter       5000 <=> epoch 0.02715494125650981 | train loss 0.4532005191 | val loss 0.4529145956:
	--> training ...
	--> checkpointing ...

2024-11-06_11-53 : iter      10000 <=> epoch 0.05430988251301962 | train loss 0.4353347123 | val loss 0.4349721968:
	--> training ...
	--> checkpointing ...

2024-11-06_12-26 : iter      15000 <=> epoch 0.08146482376952943 | train loss 0.4247696996 | val loss 0.4245010912:
	--> training ...
	--> checkpointing ...

2024-11-06_12-59 : iter      20000 <=> epoch 0.10861976502603923 | train loss 0.4210409522 | val loss 0.4222677946:
	--> training ...
	--> checkpointing ...

2024-11-06_13-32 : iter      25000 <=> epoch 0.13577470628254903 | train loss 0.4174900949 | val loss 0.4172587395:
	--> training ...
	--> checkpointing ...

2024-11-06_14-05 : iter      30000 <=> epoch 0.16292964753905886 | train loss 0.4140323102 | val loss 0.4155321717:
	--> training ...
	--> checkpointing ...

2024-11-06_14-38 : iter      35000 <=> epoch 0.19008458879556867 | train loss 0.4135879576 | val loss 0.4111002684:
	--> training ...
	--> checkpointing ...

2024-11-06_15-12 : iter      40000 <=> epoch 0.21723953005207847 | train loss 0.4080672562 | val loss 0.4093708694:
	--> training ...
	--> checkpointing ...

2024-11-06_15-45 : iter      45000 <=> epoch 0.24439447130858827 | train loss 0.4087478220 | val loss 0.4078906178:
	--> training ...
	--> checkpointing ...

2024-11-06_16-18 : iter      50000 <=> epoch 0.27154941256509807 | train loss 0.4075185061 | val loss 0.4063951075:
	--> training ...
	--> checkpointing ...

2024-11-06_16-51 : iter      55000 <=> epoch 0.29870435382160787 | train loss 0.4060791135 | val loss 0.4058502913:
	--> training ...
	--> checkpointing ...

2024-11-06_17-24 : iter      60000 <=> epoch 0.32585929507811773 | train loss 0.4050249755 | val loss 0.4060279429:
	--> training ...
	--> checkpointing ...

2024-11-06_17-57 : iter      65000 <=> epoch 0.35301423633462753 | train loss 0.4040894806 | val loss 0.4063700140:
	--> training ...
	--> checkpointing ...

2024-11-06_18-30 : iter      70000 <=> epoch 0.38016917759113733 | train loss 0.4029975235 | val loss 0.4021400511:
	--> training ...
	--> checkpointing ...

2024-11-06_19-03 : iter      75000 <=> epoch 0.40732411884764713 | train loss 0.4039273560 | val loss 0.4032254517:
	--> training ...
	--> checkpointing ...

2024-11-06_19-36 : iter      80000 <=> epoch 0.43447906010415693 | train loss 0.4035783112 | val loss 0.4023863971:
	--> training ...
	--> checkpointing ...

2024-11-06_20-10 : iter      85000 <=> epoch 0.46163400136066673 | train loss 0.4025164843 | val loss 0.4024556875:
	--> training ...
	--> checkpointing ...

2024-11-06_20-43 : iter      90000 <=> epoch 0.48878894261717654 | train loss 0.4015960097 | val loss 0.4022963345:
	--> training ...
	--> checkpointing ...

2024-11-06_21-16 : iter      95000 <=> epoch 0.5159438838736864 | train loss 0.4026146829 | val loss 0.4014147818:
	--> training ...
	--> checkpointing ...

2024-11-06_21-49 : iter     100000 <=> epoch 0.5430988251301961 | train loss 0.4010282755 | val loss 0.4028639495:
	--> training ...
	--> checkpointing ...

2024-11-06_22-22 : iter     105000 <=> epoch 0.570253766386706 | train loss 0.4005280733 | val loss 0.4021413326:
	--> training ...
	--> checkpointing ...

2024-11-06_22-55 : iter     110000 <=> epoch 0.5974087076432157 | train loss 0.4006153047 | val loss 0.3997617364:
	--> training ...
	--> checkpointing ...

2024-11-06_23-28 : iter     115000 <=> epoch 0.6245636488997256 | train loss 0.4009092152 | val loss 0.4003961682:
	--> training ...
	--> checkpointing ...

2024-11-07_00-02 : iter     120000 <=> epoch 0.6517185901562355 | train loss 0.3999346793 | val loss 0.4003011882:
	--> training ...
	--> checkpointing ...

2024-11-07_00-35 : iter     125000 <=> epoch 0.6788735314127452 | train loss 0.3990435302 | val loss 0.4004472792:
	--> training ...
	--> checkpointing ...

2024-11-07_01-08 : iter     130000 <=> epoch 0.7060284726692551 | train loss 0.3972451687 | val loss 0.3958385289:
	--> training ...
	--> checkpointing ...

2024-11-07_01-41 : iter     135000 <=> epoch 0.7331834139257648 | train loss 0.3959604502 | val loss 0.3947659135:
	--> training ...
	--> checkpointing ...

2024-11-07_02-14 : iter     140000 <=> epoch 0.7603383551822747 | train loss 0.3949543834 | val loss 0.3956297040:
	--> training ...
	--> checkpointing ...

2024-11-07_02-47 : iter     145000 <=> epoch 0.7874932964387844 | train loss 0.3943491578 | val loss 0.3956266344:
	--> training ...
	--> checkpointing ...

2024-11-07_03-20 : iter     150000 <=> epoch 0.8146482376952943 | train loss 0.3948429823 | val loss 0.3955984414:
	--> training ...
	--> checkpointing ...

2024-11-07_03-54 : iter     155000 <=> epoch 0.8418031789518041 | train loss 0.3938927352 | val loss 0.3940798938:
	--> training ...
	--> checkpointing ...

2024-11-07_04-27 : iter     160000 <=> epoch 0.8689581202083139 | train loss 0.3958658278 | val loss 0.3958814740:
	--> training ...
	--> checkpointing ...

2024-11-07_05-00 : iter     165000 <=> epoch 0.8961130614648237 | train loss 0.3959594071 | val loss 0.3947498500:
	--> training ...
	--> checkpointing ...

2024-11-07_05-33 : iter     170000 <=> epoch 0.9232680027213335 | train loss 0.3942236006 | val loss 0.3955512047:
	--> training ...
	--> checkpointing ...

2024-11-07_06-06 : iter     175000 <=> epoch 0.9504229439778433 | train loss 0.3941447735 | val loss 0.3945682347:
	--> training ...
	--> checkpointing ...

2024-11-07_06-39 : iter     180000 <=> epoch 0.9775778852343531 | train loss 0.3947184980 | val loss 0.3935878873:
	--> training ...
