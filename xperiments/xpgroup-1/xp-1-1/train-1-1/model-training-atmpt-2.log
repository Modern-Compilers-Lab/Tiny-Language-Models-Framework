|ITERS: 2310109 / 5548507 | COMP: 41.63% | RATE: 2.19 it./s | SPD: 0.4565 s/it.| ERT: (17, 2, 36, 57)                                                                                                   
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 6.67 it./s | SPD: 0.1499 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:1.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 15)
	--> val.bin
	--> took (0, 0, 0, 1)

Setting train-hyperparams and util variables:
	--> max_iters = 5548507

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 61M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping

Training loop:

==========================================================================================:

2024-10-18_18-32 : iter 1725000 <=> epoch 0.93268 | train loss 0.5907 | val loss 0.5913:
	--> training ...
	--> checkpointing ...

2024-10-18_19-13 : iter    1730000 <=> epoch 0.9353867239640343 | train loss 0.5911970735 | val loss 0.5904285908:
	--> training ...
	--> checkpointing ...

2024-10-18_19-53 : iter    1735000 <=> epoch 0.9380901538020806 | train loss 0.5903158188 | val loss 0.5917947888:
	--> training ...
	--> checkpointing ...

2024-10-18_20-34 : iter    1740000 <=> epoch 0.940793583640127 | train loss 0.5897400379 | val loss 0.5902389288:
	--> training ...
	--> checkpointing ...

2024-10-18_21-14 : iter    1745000 <=> epoch 0.9434970134781733 | train loss 0.5902523994 | val loss 0.5916864276:
	--> training ...
	--> checkpointing ...

2024-10-18_21-55 : iter    1750000 <=> epoch 0.9462004433162197 | train loss 0.5898944139 | val loss 0.5914537311:
	--> training ...
	--> checkpointing ...

2024-10-18_22-35 : iter    1755000 <=> epoch 0.9489038731542659 | train loss 0.5908982158 | val loss 0.5901831388:
	--> training ...
	--> checkpointing ...

2024-10-18_23-16 : iter    1760000 <=> epoch 0.9516073029923123 | train loss 0.5890566111 | val loss 0.5904325843:
	--> training ...
	--> checkpointing ...

2024-10-18_23-57 : iter    1765000 <=> epoch 0.9543107328303587 | train loss 0.5910288095 | val loss 0.5919875503:
	--> training ...
	--> checkpointing ...

2024-10-19_00-37 : iter    1770000 <=> epoch 0.957014162668405 | train loss 0.5901574492 | val loss 0.5900353193:
	--> training ...
	--> checkpointing ...

2024-10-19_01-18 : iter    1775000 <=> epoch 0.9597175925064514 | train loss 0.5899499059 | val loss 0.5890127420:
	--> training ...
	--> checkpointing ...

2024-10-19_01-59 : iter    1780000 <=> epoch 0.9624210223444977 | train loss 0.5894636512 | val loss 0.5919413567:
	--> training ...
	--> checkpointing ...

2024-10-19_02-39 : iter    1785000 <=> epoch 0.9651244521825441 | train loss 0.5901278257 | val loss 0.5907128453:
	--> training ...
	--> checkpointing ...

2024-10-19_03-20 : iter    1790000 <=> epoch 0.9678278820205903 | train loss 0.5926014185 | val loss 0.5910331607:
	--> training ...
	--> checkpointing ...

2024-10-19_04-00 : iter    1795000 <=> epoch 0.9705313118586367 | train loss 0.5905229449 | val loss 0.5907841921:
	--> training ...
	--> checkpointing ...

2024-10-19_04-41 : iter    1800000 <=> epoch 0.9732347416966831 | train loss 0.5923421383 | val loss 0.5900987387:
	--> training ...
	--> checkpointing ...

2024-10-19_05-21 : iter    1805000 <=> epoch 0.9759381715347294 | train loss 0.5904622674 | val loss 0.5906764865:
	--> training ...
	--> checkpointing ...

2024-10-19_06-02 : iter    1810000 <=> epoch 0.9786416013727758 | train loss 0.5898108482 | val loss 0.5895677209:
	--> training ...
	--> checkpointing ...

2024-10-19_06-43 : iter    1815000 <=> epoch 0.9813450312108221 | train loss 0.5891367197 | val loss 0.5904898047:
	--> training ...
	--> checkpointing ...

2024-10-19_07-23 : iter    1820000 <=> epoch 0.9840484610488685 | train loss 0.5905343890 | val loss 0.5899883509:
	--> training ...
	--> checkpointing ...

2024-10-19_08-04 : iter    1825000 <=> epoch 0.9867518908869147 | train loss 0.5891173482 | val loss 0.5912088156:
	--> training ...
	--> checkpointing ...

2024-10-19_08-44 : iter    1830000 <=> epoch 0.9894553207249611 | train loss 0.5902510285 | val loss 0.5905171633:
	--> training ...
	--> checkpointing ...

2024-10-19_09-25 : iter    1835000 <=> epoch 0.9921587505630075 | train loss 0.5906100273 | val loss 0.5898553133:
	--> training ...
	--> checkpointing ...

2024-10-19_10-06 : iter    1840000 <=> epoch 0.9948621804010538 | train loss 0.5917204618 | val loss 0.5917969942:
	--> training ...
	--> checkpointing ...

2024-10-19_10-46 : iter    1845000 <=> epoch 0.9975656102391002 | train loss 0.5913818479 | val loss 0.5905608535:
	--> training ...
	--> checkpointing ...

2024-10-19_11-27 : iter    1850000 <=> epoch 1.0002690400771466 | train loss 0.5901865363 | val loss 0.5907719731:
	--> training ...
	--> checkpointing ...

2024-10-19_12-07 : iter    1855000 <=> epoch 1.0029724699151927 | train loss 0.5885245204 | val loss 0.5909575224:
	--> training ...
	--> checkpointing ...

2024-10-19_12-48 : iter    1860000 <=> epoch 1.0056758997532391 | train loss 0.5908187032 | val loss 0.5899679661:
	--> training ...
	--> checkpointing ...

2024-10-19_13-29 : iter    1865000 <=> epoch 1.0083793295912855 | train loss 0.5893556476 | val loss 0.5918624401:
	--> training ...
	--> checkpointing ...

2024-10-19_14-09 : iter    1870000 <=> epoch 1.011082759429332 | train loss 0.5903753638 | val loss 0.5891635418:
	--> training ...
	--> checkpointing ...

2024-10-19_14-50 : iter    1875000 <=> epoch 1.0137861892673783 | train loss 0.5889383554 | val loss 0.5904729962:
	--> training ...
	--> checkpointing ...

2024-10-19_15-31 : iter    1880000 <=> epoch 1.0164896191054245 | train loss 0.5905331969 | val loss 0.5906395316:
	--> training ...
	--> checkpointing ...

2024-10-19_16-11 : iter    1885000 <=> epoch 1.0191930489434708 | train loss 0.5911699533 | val loss 0.5922369361:
	--> training ...
	--> checkpointing ...

2024-10-19_16-52 : iter    1890000 <=> epoch 1.0218964787815172 | train loss 0.5909287930 | val loss 0.5901995897:
	--> training ...
	--> checkpointing ...

2024-10-19_17-32 : iter    1895000 <=> epoch 1.0245999086195636 | train loss 0.5915052295 | val loss 0.5908859968:
	--> training ...
	--> checkpointing ...

2024-10-19_18-13 : iter    1900000 <=> epoch 1.0273033384576098 | train loss 0.5908197761 | val loss 0.5902040005:
	--> training ...
	--> checkpointing ...

2024-10-19_18-54 : iter    1905000 <=> epoch 1.0300067682956562 | train loss 0.5905849338 | val loss 0.5894718766:
	--> training ...
	--> checkpointing ...

2024-10-19_19-34 : iter    1910000 <=> epoch 1.0327101981337026 | train loss 0.5904663801 | val loss 0.5905888677:
	--> training ...
	--> checkpointing ...

2024-10-19_20-15 : iter    1915000 <=> epoch 1.035413627971749 | train loss 0.5896481872 | val loss 0.5899192095:
	--> training ...
	--> checkpointing ...

2024-10-19_20-55 : iter    1920000 <=> epoch 1.0381170578097954 | train loss 0.5919261575 | val loss 0.5917448997:
	--> training ...
	--> checkpointing ...

2024-10-19_21-36 : iter    1925000 <=> epoch 1.0408204876478415 | train loss 0.5909448266 | val loss 0.5912186503:
	--> training ...
	--> checkpointing ...

2024-10-19_22-17 : iter    1930000 <=> epoch 1.043523917485888 | train loss 0.5919483900 | val loss 0.5915153027:
	--> training ...
	--> checkpointing ...

2024-10-19_22-57 : iter    1935000 <=> epoch 1.0462273473239343 | train loss 0.5911020637 | val loss 0.5916030407:
	--> training ...
	--> checkpointing ...

2024-10-19_23-38 : iter    1940000 <=> epoch 1.0489307771619807 | train loss 0.5913985968 | val loss 0.5909500718:
	--> training ...
	--> checkpointing ...

2024-10-20_00-19 : iter    1945000 <=> epoch 1.051634207000027 | train loss 0.5914247036 | val loss 0.5903630853:
	--> training ...
	--> checkpointing ...

2024-10-20_00-59 : iter    1950000 <=> epoch 1.0543376368380732 | train loss 0.5907331705 | val loss 0.5889332294:
	--> training ...
	--> checkpointing ...

2024-10-20_01-40 : iter    1955000 <=> epoch 1.0570410666761196 | train loss 0.5903716683 | val loss 0.5899094939:
	--> training ...
	--> checkpointing ...

2024-10-20_02-20 : iter    1960000 <=> epoch 1.059744496514166 | train loss 0.5913701057 | val loss 0.5903449059:
	--> training ...
	--> checkpointing ...

2024-10-20_03-01 : iter    1965000 <=> epoch 1.0624479263522124 | train loss 0.5904084444 | val loss 0.5905965567:
	--> training ...
	--> checkpointing ...

2024-10-20_03-41 : iter    1970000 <=> epoch 1.0651513561902586 | train loss 0.5909211040 | val loss 0.5900267959:
	--> training ...
	--> checkpointing ...

2024-10-20_04-22 : iter    1975000 <=> epoch 1.067854786028305 | train loss 0.5895898938 | val loss 0.5909114480:
	--> training ...
	--> checkpointing ...

2024-10-20_05-03 : iter    1980000 <=> epoch 1.0705582158663514 | train loss 0.5911068320 | val loss 0.5885041952:
	--> training ...
	--> checkpointing ...

2024-10-20_05-43 : iter    1985000 <=> epoch 1.0732616457043977 | train loss 0.5910871029 | val loss 0.5894213319:
	--> training ...
	--> checkpointing ...

2024-10-20_06-24 : iter    1990000 <=> epoch 1.0759650755424441 | train loss 0.5897734165 | val loss 0.5903512239:
	--> training ...
	--> checkpointing ...

2024-10-20_07-04 : iter    1995000 <=> epoch 1.0786685053804903 | train loss 0.5902306437 | val loss 0.5896545053:
	--> training ...
	--> checkpointing ...

2024-10-20_07-45 : iter    2000000 <=> epoch 1.0813719352185367 | train loss 0.5890252590 | val loss 0.5927456021:
	--> training ...
	--> checkpointing ...

2024-10-20_08-25 : iter    2005000 <=> epoch 1.084075365056583 | train loss 0.5907948613 | val loss 0.5908287764:
	--> training ...
	--> checkpointing ...

2024-10-20_09-06 : iter    2010000 <=> epoch 1.0867787948946295 | train loss 0.5902791619 | val loss 0.5910720229:
	--> training ...
	--> checkpointing ...

2024-10-20_09-47 : iter    2015000 <=> epoch 1.0894822247326759 | train loss 0.5900416374 | val loss 0.5895590186:
	--> training ...
	--> checkpointing ...

2024-10-20_10-27 : iter    2020000 <=> epoch 1.092185654570722 | train loss 0.5885883570 | val loss 0.5912005901:
	--> training ...
	--> checkpointing ...

2024-10-20_11-08 : iter    2025000 <=> epoch 1.0948890844087684 | train loss 0.5907306671 | val loss 0.5902519822:
	--> training ...
	--> checkpointing ...

2024-10-20_11-49 : iter    2030000 <=> epoch 1.0975925142468148 | train loss 0.5905243158 | val loss 0.5914604068:
	--> training ...
	--> checkpointing ...

2024-10-20_12-29 : iter    2035000 <=> epoch 1.1002959440848612 | train loss 0.5915062428 | val loss 0.5908387899:
	--> training ...
	--> checkpointing ...

2024-10-20_13-10 : iter    2040000 <=> epoch 1.1029993739229074 | train loss 0.5909292698 | val loss 0.5890650153:
	--> training ...
	--> checkpointing ...

2024-10-20_13-51 : iter    2045000 <=> epoch 1.1057028037609538 | train loss 0.5898404121 | val loss 0.5912134051:
	--> training ...
	--> checkpointing ...

2024-10-20_14-31 : iter    2050000 <=> epoch 1.1084062335990001 | train loss 0.5907710791 | val loss 0.5907096267:
	--> training ...
	--> checkpointing ...

2024-10-20_15-12 : iter    2055000 <=> epoch 1.1111096634370465 | train loss 0.5903865695 | val loss 0.5889419317:
	--> training ...
	--> checkpointing ...

2024-10-20_15-53 : iter    2060000 <=> epoch 1.113813093275093 | train loss 0.5903477073 | val loss 0.5896507502:
	--> training ...
	--> checkpointing ...

2024-10-20_16-33 : iter    2065000 <=> epoch 1.116516523113139 | train loss 0.5909543633 | val loss 0.5903417468:
	--> training ...
	--> checkpointing ...

2024-10-20_17-14 : iter    2070000 <=> epoch 1.1192199529511855 | train loss 0.5896530747 | val loss 0.5904120207:
	--> training ...
	--> checkpointing ...

2024-10-20_17-55 : iter    2075000 <=> epoch 1.1219233827892319 | train loss 0.5913990736 | val loss 0.5896160007:
	--> training ...
	--> checkpointing ...

2024-10-20_18-35 : iter    2080000 <=> epoch 1.1246268126272783 | train loss 0.5906223059 | val loss 0.5909492970:
	--> training ...
	--> checkpointing ...

2024-10-20_19-16 : iter    2085000 <=> epoch 1.1273302424653247 | train loss 0.5904836655 | val loss 0.5908131599:
	--> training ...
	--> checkpointing ...

2024-10-20_19-57 : iter    2090000 <=> epoch 1.1300336723033708 | train loss 0.5909451842 | val loss 0.5921125412:
	--> training ...
	--> checkpointing ...

2024-10-20_20-37 : iter    2095000 <=> epoch 1.1327371021414172 | train loss 0.5907896757 | val loss 0.5903854966:
	--> training ...
	--> checkpointing ...

2024-10-20_21-18 : iter    2100000 <=> epoch 1.1354405319794636 | train loss 0.5907745361 | val loss 0.5905005336:
	--> training ...
	--> checkpointing ...

2024-10-20_21-58 : iter    2105000 <=> epoch 1.13814396181751 | train loss 0.5899285078 | val loss 0.5922752619:
	--> training ...
	--> checkpointing ...

2024-10-20_22-39 : iter    2110000 <=> epoch 1.1408473916555562 | train loss 0.5887498260 | val loss 0.5888294578:
	--> training ...
	--> checkpointing ...

2024-10-20_23-20 : iter    2115000 <=> epoch 1.1435508214936025 | train loss 0.5905207992 | val loss 0.5898708701:
	--> training ...
	--> checkpointing ...

2024-10-21_00-00 : iter    2120000 <=> epoch 1.146254251331649 | train loss 0.5908803344 | val loss 0.5889544487:
	--> training ...
	--> checkpointing ...

2024-10-21_00-41 : iter    2125000 <=> epoch 1.1489576811696953 | train loss 0.5914046764 | val loss 0.5921004415:
	--> training ...
	--> checkpointing ...

2024-10-21_01-21 : iter    2130000 <=> epoch 1.1516611110077417 | train loss 0.5902111530 | val loss 0.5891919732:
	--> training ...
	--> checkpointing ...

2024-10-21_02-02 : iter    2135000 <=> epoch 1.1543645408457879 | train loss 0.5900884867 | val loss 0.5898061395:
	--> training ...
	--> checkpointing ...

2024-10-21_02-43 : iter    2140000 <=> epoch 1.1570679706838343 | train loss 0.5910666585 | val loss 0.5893009305:
	--> training ...
	--> checkpointing ...

2024-10-21_03-23 : iter    2145000 <=> epoch 1.1597714005218807 | train loss 0.5902583003 | val loss 0.5901883841:
	--> training ...
	--> checkpointing ...

2024-10-21_04-04 : iter    2150000 <=> epoch 1.162474830359927 | train loss 0.5907691121 | val loss 0.5898641348:
	--> training ...
	--> checkpointing ...

2024-10-21_04-44 : iter    2155000 <=> epoch 1.1651782601979734 | train loss 0.5907711387 | val loss 0.5892772079:
	--> training ...
	--> checkpointing ...

2024-10-21_05-25 : iter    2160000 <=> epoch 1.1678816900360196 | train loss 0.5906161070 | val loss 0.5882205963:
	--> training ...
	--> checkpointing ...

2024-10-21_06-06 : iter    2165000 <=> epoch 1.170585119874066 | train loss 0.5901526809 | val loss 0.5911635756:
	--> training ...
	--> checkpointing ...

2024-10-21_06-46 : iter    2170000 <=> epoch 1.1732885497121124 | train loss 0.5915160775 | val loss 0.5909139514:
	--> training ...
	--> checkpointing ...

2024-10-21_07-27 : iter    2175000 <=> epoch 1.1759919795501588 | train loss 0.5904390812 | val loss 0.5900924206:
	--> training ...
	--> checkpointing ...

2024-10-21_08-07 : iter    2180000 <=> epoch 1.178695409388205 | train loss 0.5901921391 | val loss 0.5895767212:
	--> training ...
	--> checkpointing ...

2024-10-21_08-48 : iter    2185000 <=> epoch 1.1813988392262513 | train loss 0.5910754800 | val loss 0.5896883607:
	--> training ...
	--> checkpointing ...

2024-10-21_09-29 : iter    2190000 <=> epoch 1.1841022690642977 | train loss 0.5877312422 | val loss 0.5895535350:
	--> training ...
	--> checkpointing ...

2024-10-21_10-09 : iter    2195000 <=> epoch 1.186805698902344 | train loss 0.5901114345 | val loss 0.5906722546:
	--> training ...
	--> checkpointing ...

2024-10-21_10-50 : iter    2200000 <=> epoch 1.1895091287403905 | train loss 0.5912701488 | val loss 0.5898173451:
	--> training ...
	--> checkpointing ...

2024-10-21_11-31 : iter    2205000 <=> epoch 1.1922125585784367 | train loss 0.5906887650 | val loss 0.5914576650:
	--> training ...
	--> checkpointing ...

2024-10-21_12-11 : iter    2210000 <=> epoch 1.194915988416483 | train loss 0.5906679034 | val loss 0.5906169415:
	--> training ...
	--> checkpointing ...

2024-10-21_12-52 : iter    2215000 <=> epoch 1.1976194182545294 | train loss 0.5888280272 | val loss 0.5897324085:
	--> training ...
	--> checkpointing ...

2024-10-21_13-33 : iter    2220000 <=> epoch 1.2003228480925758 | train loss 0.5908704996 | val loss 0.5898250341:
	--> training ...
	--> checkpointing ...

2024-10-21_14-13 : iter    2225000 <=> epoch 1.2030262779306222 | train loss 0.5902881026 | val loss 0.5902875662:
	--> training ...
	--> checkpointing ...

2024-10-21_14-54 : iter    2230000 <=> epoch 1.2057297077686684 | train loss 0.5895446539 | val loss 0.5898658633:
	--> training ...
	--> checkpointing ...

2024-10-21_15-35 : iter    2235000 <=> epoch 1.2084331376067148 | train loss 0.5910315514 | val loss 0.5901486874:
	--> training ...
	--> checkpointing ...

2024-10-21_16-15 : iter    2240000 <=> epoch 1.2111365674447612 | train loss 0.5892401338 | val loss 0.5889463425:
	--> training ...
	--> checkpointing ...

2024-10-21_16-56 : iter    2245000 <=> epoch 1.2138399972828076 | train loss 0.5906042457 | val loss 0.5915910602:
	--> training ...
	--> checkpointing ...

2024-10-21_17-36 : iter    2250000 <=> epoch 1.2165434271208537 | train loss 0.5890480876 | val loss 0.5904673934:
	--> training ...
	--> checkpointing ...

2024-10-21_18-17 : iter    2255000 <=> epoch 1.2192468569589001 | train loss 0.5910200477 | val loss 0.5894488692:
	--> training ...
	--> checkpointing ...

2024-10-21_18-58 : iter    2260000 <=> epoch 1.2219502867969465 | train loss 0.5906577110 | val loss 0.5896399617:
	--> training ...
	--> checkpointing ...

2024-10-21_19-38 : iter    2265000 <=> epoch 1.224653716634993 | train loss 0.5912002921 | val loss 0.5894453526:
	--> training ...
	--> checkpointing ...

2024-10-21_20-19 : iter    2270000 <=> epoch 1.2273571464730393 | train loss 0.5909036398 | val loss 0.5912994742:
	--> training ...
	--> checkpointing ...

2024-10-21_21-00 : iter    2275000 <=> epoch 1.2300605763110855 | train loss 0.5901136398 | val loss 0.5900951028:
	--> training ...
	--> checkpointing ...

2024-10-21_21-40 : iter    2280000 <=> epoch 1.2327640061491318 | train loss 0.5913603902 | val loss 0.5894779563:
	--> training ...
	--> checkpointing ...

2024-10-21_22-21 : iter    2285000 <=> epoch 1.2354674359871782 | train loss 0.5913106799 | val loss 0.5902195573:
	--> training ...
	--> checkpointing ...

2024-10-21_23-01 : iter    2290000 <=> epoch 1.2381708658252246 | train loss 0.5901505351 | val loss 0.5907562375:
	--> training ...
	--> checkpointing ...

2024-10-21_23-42 : iter    2295000 <=> epoch 1.240874295663271 | train loss 0.5901214480 | val loss 0.5896646976:
	--> training ...
	--> checkpointing ...

2024-10-22_00-23 : iter    2300000 <=> epoch 1.2435777255013172 | train loss 0.5908174515 | val loss 0.5900213122:
	--> training ...
	--> checkpointing ...

2024-10-22_01-03 : iter    2305000 <=> epoch 1.2462811553393636 | train loss 0.5886975527 | val loss 0.5901811719:
	--> training ...
	--> checkpointing ...

2024-10-22_01-44 : iter    2310000 <=> epoch 1.24898458517741 | train loss 0.5879317522 | val loss 0.5891115069:
	--> training ...
