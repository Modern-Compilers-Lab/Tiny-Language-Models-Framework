|ITERS: 724340 / 724340 | COMP: 100.00% | RATE: 4.17 it./s | SPD: 0.2398 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 12.40 it./s | SPD: 0.0806 s/it.| ERT: (0, 0, 0, 0) |                                                                                                      

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:2.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 2)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 724340

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 30M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-09_13-04 : iter     0 <=> epoch 0 | train loss 4.0721 | val loss 4.0719:
	--> training ...
	--> checkpointing ...

2024-10-09_13-25 : iter       5000 <=> epoch 0.0207084954943204 | train loss 0.6486330628 | val loss 0.6478386521:
	--> training ...
	--> checkpointing ...

2024-10-09_13-47 : iter      10000 <=> epoch 0.0414169909886408 | train loss 0.6326983571 | val loss 0.6320916414:
	--> training ...
	--> checkpointing ...

2024-10-09_14-08 : iter      15000 <=> epoch 0.062125486482961206 | train loss 0.6232883334 | val loss 0.6229929328:
	--> training ...
	--> checkpointing ...

2024-10-09_14-30 : iter      20000 <=> epoch 0.0828339819772816 | train loss 0.6173292994 | val loss 0.6179413795:
	--> training ...
	--> checkpointing ...

2024-10-09_14-51 : iter      25000 <=> epoch 0.103542477471602 | train loss 0.6135791540 | val loss 0.6129406691:
	--> training ...
	--> checkpointing ...

2024-10-09_15-13 : iter      30000 <=> epoch 0.12425097296592241 | train loss 0.6092683673 | val loss 0.6087908149:
	--> training ...
	--> checkpointing ...

2024-10-09_15-34 : iter      35000 <=> epoch 0.1449594684602428 | train loss 0.6069210768 | val loss 0.6071806550:
	--> training ...
	--> checkpointing ...

2024-10-09_15-56 : iter      40000 <=> epoch 0.1656679639545632 | train loss 0.6037085652 | val loss 0.6050060987:
	--> training ...
	--> checkpointing ...

2024-10-09_16-17 : iter      45000 <=> epoch 0.18637645944888362 | train loss 0.6019010544 | val loss 0.6031681299:
	--> training ...
	--> checkpointing ...

2024-10-09_16-39 : iter      50000 <=> epoch 0.207084954943204 | train loss 0.6002768874 | val loss 0.6009790301:
	--> training ...
	--> checkpointing ...

2024-10-09_17-00 : iter      55000 <=> epoch 0.22779345043752441 | train loss 0.5989657044 | val loss 0.5996499658:
	--> training ...
	--> checkpointing ...

2024-10-09_17-21 : iter      60000 <=> epoch 0.24850194593184483 | train loss 0.5988626480 | val loss 0.5975606441:
	--> training ...
	--> checkpointing ...

2024-10-09_17-43 : iter      65000 <=> epoch 0.26921044142616524 | train loss 0.5974711180 | val loss 0.5970953107:
	--> training ...
	--> checkpointing ...

2024-10-09_18-04 : iter      70000 <=> epoch 0.2899189369204856 | train loss 0.5968521833 | val loss 0.5963213444:
	--> training ...
	--> checkpointing ...

2024-10-09_18-25 : iter      75000 <=> epoch 0.310627432414806 | train loss 0.5956401229 | val loss 0.5955625772:
	--> training ...
	--> checkpointing ...

2024-10-09_18-47 : iter      80000 <=> epoch 0.3313359279091264 | train loss 0.5956953764 | val loss 0.5950645208:
	--> training ...
	--> checkpointing ...

2024-10-09_19-08 : iter      85000 <=> epoch 0.3520444234034468 | train loss 0.5941060781 | val loss 0.5955973268:
	--> training ...
	--> checkpointing ...

2024-10-09_19-30 : iter      90000 <=> epoch 0.37275291889776724 | train loss 0.5953294039 | val loss 0.5939384699:
	--> training ...
	--> checkpointing ...

2024-10-09_19-51 : iter      95000 <=> epoch 0.39346141439208765 | train loss 0.5944986939 | val loss 0.5948089361:
	--> training ...
	--> checkpointing ...

2024-10-09_20-12 : iter     100000 <=> epoch 0.414169909886408 | train loss 0.5933428407 | val loss 0.5937035084:
	--> training ...
	--> checkpointing ...

2024-10-09_20-34 : iter     105000 <=> epoch 0.4348784053807284 | train loss 0.5948920846 | val loss 0.5937341452:
	--> training ...
	--> checkpointing ...

2024-10-09_20-55 : iter     110000 <=> epoch 0.45558690087504883 | train loss 0.5923600197 | val loss 0.5932673216:
	--> training ...
	--> checkpointing ...

2024-10-09_21-17 : iter     115000 <=> epoch 0.47629539636936924 | train loss 0.5932662487 | val loss 0.5927993059:
	--> training ...
	--> checkpointing ...

2024-10-09_21-38 : iter     120000 <=> epoch 0.49700389186368965 | train loss 0.5917171836 | val loss 0.5924140215:
	--> training ...
	--> checkpointing ...

2024-10-09_21-59 : iter     125000 <=> epoch 0.5177123873580101 | train loss 0.5928062201 | val loss 0.5923091769:
	--> training ...
	--> checkpointing ...

2024-10-09_22-21 : iter     130000 <=> epoch 0.5384208828523305 | train loss 0.5916078687 | val loss 0.5919001698:
	--> training ...
	--> checkpointing ...

2024-10-09_22-42 : iter     135000 <=> epoch 0.5591293783466509 | train loss 0.5914517641 | val loss 0.5914531350:
	--> training ...
	--> checkpointing ...

2024-10-09_23-04 : iter     140000 <=> epoch 0.5798378738409712 | train loss 0.5918772817 | val loss 0.5906670094:
	--> training ...
	--> checkpointing ...

2024-10-09_23-25 : iter     145000 <=> epoch 0.6005463693352916 | train loss 0.5930083394 | val loss 0.5908134580:
	--> training ...
	--> checkpointing ...

2024-10-09_23-46 : iter     150000 <=> epoch 0.621254864829612 | train loss 0.5909597874 | val loss 0.5911086798:
	--> training ...
	--> checkpointing ...

2024-10-10_00-08 : iter     155000 <=> epoch 0.6419633603239324 | train loss 0.5914132595 | val loss 0.5915185809:
	--> training ...
	--> checkpointing ...

2024-10-10_00-29 : iter     160000 <=> epoch 0.6626718558182528 | train loss 0.5914355516 | val loss 0.5910748839:
	--> training ...
	--> checkpointing ...

2024-10-10_00-50 : iter     165000 <=> epoch 0.6833803513125732 | train loss 0.5905448198 | val loss 0.5901796222:
	--> training ...
	--> checkpointing ...

2024-10-10_01-12 : iter     170000 <=> epoch 0.7040888468068937 | train loss 0.5898553729 | val loss 0.5907486081:
	--> training ...
	--> checkpointing ...

2024-10-10_01-33 : iter     175000 <=> epoch 0.7247973423012141 | train loss 0.5916041732 | val loss 0.5903862119:
	--> training ...
	--> checkpointing ...

2024-10-10_01-55 : iter     180000 <=> epoch 0.7455058377955345 | train loss 0.5903447866 | val loss 0.5907890797:
	--> training ...
	--> checkpointing ...

2024-10-10_02-16 : iter     185000 <=> epoch 0.7662143332898549 | train loss 0.5898457170 | val loss 0.5904752612:
	--> training ...
	--> checkpointing ...

2024-10-10_02-37 : iter     190000 <=> epoch 0.7869228287841753 | train loss 0.5883257985 | val loss 0.5896416306:
	--> training ...
	--> checkpointing ...

2024-10-10_02-59 : iter     195000 <=> epoch 0.8076313242784956 | train loss 0.5906881094 | val loss 0.5894140601:
	--> training ...
	--> checkpointing ...

2024-10-10_03-20 : iter     200000 <=> epoch 0.828339819772816 | train loss 0.5889428258 | val loss 0.5897924900:
	--> training ...
	--> checkpointing ...

2024-10-10_03-41 : iter     205000 <=> epoch 0.8490483152671364 | train loss 0.5899036527 | val loss 0.5896397829:
	--> training ...
	--> checkpointing ...

2024-10-10_04-03 : iter     210000 <=> epoch 0.8697568107614568 | train loss 0.5889390111 | val loss 0.5886118412:
	--> training ...
	--> checkpointing ...

2024-10-10_04-24 : iter     215000 <=> epoch 0.8904653062557772 | train loss 0.5892755389 | val loss 0.5895811915:
	--> training ...
	--> checkpointing ...

2024-10-10_04-46 : iter     220000 <=> epoch 0.9111738017500977 | train loss 0.5891183019 | val loss 0.5890749693:
	--> training ...
	--> checkpointing ...

2024-10-10_05-07 : iter     225000 <=> epoch 0.9318822972444181 | train loss 0.5893396139 | val loss 0.5893341899:
	--> training ...
	--> checkpointing ...

2024-10-10_05-28 : iter     230000 <=> epoch 0.9525907927387385 | train loss 0.5885409713 | val loss 0.5892865062:
	--> training ...
	--> checkpointing ...

2024-10-10_05-50 : iter     235000 <=> epoch 0.9732992882330589 | train loss 0.5890687108 | val loss 0.5891791582:
	--> training ...
	--> checkpointing ...

2024-10-10_06-11 : iter     240000 <=> epoch 0.9940077837273793 | train loss 0.5878486037 | val loss 0.5893529654:
	--> training ...
	--> checkpointing ...

2024-10-10_06-33 : iter     245000 <=> epoch 1.0147162792216997 | train loss 0.5897909999 | val loss 0.5890712142:
	--> training ...
	--> checkpointing ...

2024-10-10_06-54 : iter     250000 <=> epoch 1.0354247747160201 | train loss 0.5888210535 | val loss 0.5884371400:
	--> training ...
	--> checkpointing ...

2024-10-10_07-15 : iter     255000 <=> epoch 1.0561332702103405 | train loss 0.5891976357 | val loss 0.5887905955:
	--> training ...
	--> checkpointing ...

2024-10-10_07-37 : iter     260000 <=> epoch 1.076841765704661 | train loss 0.5899279714 | val loss 0.5885541439:
	--> training ...
	--> checkpointing ...

2024-10-10_07-58 : iter     265000 <=> epoch 1.0975502611989814 | train loss 0.5882729888 | val loss 0.5889977813:
	--> training ...
	--> checkpointing ...

2024-10-10_08-19 : iter     270000 <=> epoch 1.1182587566933018 | train loss 0.5896198750 | val loss 0.5890440345:
	--> training ...
	--> checkpointing ...

2024-10-10_08-41 : iter     275000 <=> epoch 1.138967252187622 | train loss 0.5887902975 | val loss 0.5889073014:
	--> training ...
	--> checkpointing ...

2024-10-10_09-02 : iter     280000 <=> epoch 1.1596757476819424 | train loss 0.5884449482 | val loss 0.5885747075:
	--> training ...
	--> checkpointing ...

2024-10-10_09-23 : iter     285000 <=> epoch 1.1803842431762628 | train loss 0.5876486897 | val loss 0.5886723399:
	--> training ...
	--> checkpointing ...

2024-10-10_09-45 : iter     290000 <=> epoch 1.2010927386705832 | train loss 0.5891917348 | val loss 0.5882495046:
	--> training ...
	--> checkpointing ...

2024-10-10_10-06 : iter     295000 <=> epoch 1.2218012341649036 | train loss 0.5883539915 | val loss 0.5888975859:
	--> training ...
	--> checkpointing ...

2024-10-10_10-27 : iter     300000 <=> epoch 1.242509729659224 | train loss 0.5886159539 | val loss 0.5881239176:
	--> training ...
	--> checkpointing ...

2024-10-10_10-49 : iter     305000 <=> epoch 1.2632182251535444 | train loss 0.5891333222 | val loss 0.5885199904:
	--> training ...
	--> checkpointing ...

2024-10-10_11-10 : iter     310000 <=> epoch 1.2839267206478648 | train loss 0.5880233645 | val loss 0.5884514451:
	--> training ...
	--> checkpointing ...

2024-10-10_11-32 : iter     315000 <=> epoch 1.3046352161421853 | train loss 0.5877235532 | val loss 0.5887535214:
	--> training ...
	--> checkpointing ...

2024-10-10_11-53 : iter     320000 <=> epoch 1.3253437116365057 | train loss 0.5875182152 | val loss 0.5891118646:
	--> training ...
	--> checkpointing ...

2024-10-10_12-14 : iter     325000 <=> epoch 1.346052207130826 | train loss 0.5879102349 | val loss 0.5881313086:
	--> training ...
	--> checkpointing ...

2024-10-10_12-36 : iter     330000 <=> epoch 1.3667607026251465 | train loss 0.5876731277 | val loss 0.5890187621:
	--> training ...
	--> checkpointing ...

2024-10-10_12-57 : iter     335000 <=> epoch 1.387469198119467 | train loss 0.5892154574 | val loss 0.5888087749:
	--> training ...
	--> checkpointing ...

2024-10-10_13-19 : iter     340000 <=> epoch 1.4081776936137873 | train loss 0.5890088081 | val loss 0.5880317092:
	--> training ...
	--> checkpointing ...

2024-10-10_13-40 : iter     345000 <=> epoch 1.4288861891081077 | train loss 0.5878803134 | val loss 0.5883288980:
	--> training ...
	--> checkpointing ...

2024-10-10_14-01 : iter     350000 <=> epoch 1.4495946846024281 | train loss 0.5880855322 | val loss 0.5886004567:
	--> training ...
	--> checkpointing ...

2024-10-10_14-23 : iter     355000 <=> epoch 1.4703031800967485 | train loss 0.5864857435 | val loss 0.5883942842:
	--> training ...
	--> checkpointing ...

2024-10-10_14-44 : iter     360000 <=> epoch 1.491011675591069 | train loss 0.5873395205 | val loss 0.5890499353:
	--> training ...
	--> checkpointing ...

2024-10-10_15-06 : iter     365000 <=> epoch 1.5117201710853894 | train loss 0.5880333185 | val loss 0.5882846713:
	--> training ...
	--> checkpointing ...

2024-10-10_15-27 : iter     370000 <=> epoch 1.5324286665797098 | train loss 0.5883294344 | val loss 0.5888553262:
	--> training ...
	--> checkpointing ...

2024-10-10_15-48 : iter     375000 <=> epoch 1.5531371620740302 | train loss 0.5870347619 | val loss 0.5875452161:
	--> training ...
	--> checkpointing ...

2024-10-10_16-10 : iter     380000 <=> epoch 1.5738456575683506 | train loss 0.5887179375 | val loss 0.5880810618:
	--> training ...
	--> checkpointing ...

2024-10-10_16-31 : iter     385000 <=> epoch 1.5945541530626708 | train loss 0.5880655646 | val loss 0.5880676508:
	--> training ...
	--> checkpointing ...

2024-10-10_16-53 : iter     390000 <=> epoch 1.6152626485569912 | train loss 0.5881457329 | val loss 0.5881127119:
	--> training ...
	--> checkpointing ...

2024-10-10_17-14 : iter     395000 <=> epoch 1.6359711440513116 | train loss 0.5879530907 | val loss 0.5877034068:
	--> training ...
	--> checkpointing ...

2024-10-10_17-36 : iter     400000 <=> epoch 1.656679639545632 | train loss 0.5886279345 | val loss 0.5875024199:
	--> training ...
	--> checkpointing ...

2024-10-10_17-57 : iter     405000 <=> epoch 1.6773881350399524 | train loss 0.5876488090 | val loss 0.5867013931:
	--> training ...
	--> checkpointing ...

2024-10-10_18-18 : iter     410000 <=> epoch 1.6980966305342728 | train loss 0.5880309939 | val loss 0.5893798470:
	--> training ...
	--> checkpointing ...

2024-10-10_18-40 : iter     415000 <=> epoch 1.7188051260285933 | train loss 0.5887418389 | val loss 0.5886464715:
	--> training ...
	--> checkpointing ...

2024-10-10_19-01 : iter     420000 <=> epoch 1.7395136215229137 | train loss 0.5879591107 | val loss 0.5883253813:
	--> training ...
	--> checkpointing ...

2024-10-10_19-23 : iter     425000 <=> epoch 1.760222117017234 | train loss 0.5882583857 | val loss 0.5874993205:
	--> training ...
	--> checkpointing ...

2024-10-10_19-44 : iter     430000 <=> epoch 1.7809306125115545 | train loss 0.5872439146 | val loss 0.5871424675:
	--> training ...
	--> checkpointing ...

2024-10-10_20-06 : iter     435000 <=> epoch 1.801639108005875 | train loss 0.5874518752 | val loss 0.5881195664:
	--> training ...
	--> checkpointing ...

2024-10-10_20-27 : iter     440000 <=> epoch 1.8223476035001953 | train loss 0.5863511562 | val loss 0.5870066881:
	--> training ...
	--> checkpointing ...

2024-10-10_20-49 : iter     445000 <=> epoch 1.8430560989945157 | train loss 0.5870087743 | val loss 0.5878985524:
	--> training ...
	--> checkpointing ...

2024-10-10_21-10 : iter     450000 <=> epoch 1.8637645944888361 | train loss 0.5889277458 | val loss 0.5888029337:
	--> training ...
	--> checkpointing ...

2024-10-10_21-32 : iter     455000 <=> epoch 1.8844730899831565 | train loss 0.5863550901 | val loss 0.5858499408:
	--> training ...
	--> checkpointing ...

2024-10-10_21-53 : iter     460000 <=> epoch 1.905181585477477 | train loss 0.5866948962 | val loss 0.5874372125:
	--> training ...
	--> checkpointing ...

2024-10-10_22-15 : iter     465000 <=> epoch 1.9258900809717974 | train loss 0.5876709819 | val loss 0.5869275331:
	--> training ...
	--> checkpointing ...

2024-10-10_22-36 : iter     470000 <=> epoch 1.9465985764661178 | train loss 0.5879076123 | val loss 0.5867723227:
	--> training ...
	--> checkpointing ...

2024-10-10_22-58 : iter     475000 <=> epoch 1.9673070719604382 | train loss 0.5874224901 | val loss 0.5866369009:
	--> training ...
	--> checkpointing ...

2024-10-10_23-19 : iter     480000 <=> epoch 1.9880155674547586 | train loss 0.5873510838 | val loss 0.5869100690:
	--> training ...
	--> checkpointing ...

2024-10-10_23-41 : iter     485000 <=> epoch 2.008724062949079 | train loss 0.5881503820 | val loss 0.5883786678:
	--> training ...
	--> checkpointing ...

2024-10-11_00-02 : iter     490000 <=> epoch 2.0294325584433994 | train loss 0.5869695544 | val loss 0.5885624290:
	--> training ...
	--> checkpointing ...

2024-10-11_00-24 : iter     495000 <=> epoch 2.0501410539377196 | train loss 0.5869885683 | val loss 0.5864515901:
	--> training ...
	--> checkpointing ...

2024-10-11_00-45 : iter     500000 <=> epoch 2.0708495494320402 | train loss 0.5882430673 | val loss 0.5862162113:
	--> training ...
	--> checkpointing ...

2024-10-11_01-07 : iter     505000 <=> epoch 2.0915580449263604 | train loss 0.5869278312 | val loss 0.5872196555:
	--> training ...
	--> checkpointing ...

2024-10-11_01-28 : iter     510000 <=> epoch 2.112266540420681 | train loss 0.5839192867 | val loss 0.5833986998:
	--> training ...
	--> checkpointing ...

2024-10-11_01-49 : iter     515000 <=> epoch 2.1329750359150013 | train loss 0.5833851695 | val loss 0.5839446783:
	--> training ...
	--> checkpointing ...

2024-10-11_02-11 : iter     520000 <=> epoch 2.153683531409322 | train loss 0.5840038657 | val loss 0.5842566490:
	--> training ...
	--> checkpointing ...

2024-10-11_02-32 : iter     525000 <=> epoch 2.174392026903642 | train loss 0.5828389525 | val loss 0.5829441547:
	--> training ...
	--> checkpointing ...

2024-10-11_02-54 : iter     530000 <=> epoch 2.1951005223979627 | train loss 0.5826304555 | val loss 0.5842596889:
	--> training ...
	--> checkpointing ...

2024-10-11_03-15 : iter     535000 <=> epoch 2.215809017892283 | train loss 0.5845504403 | val loss 0.5829950571:
	--> training ...
	--> checkpointing ...

2024-10-11_03-37 : iter     540000 <=> epoch 2.2365175133866035 | train loss 0.5831654072 | val loss 0.5836744905:
	--> training ...
	--> checkpointing ...

2024-10-11_03-58 : iter     545000 <=> epoch 2.2572260088809237 | train loss 0.5836147666 | val loss 0.5844149590:
	--> training ...
	--> checkpointing ...

2024-10-11_04-20 : iter     550000 <=> epoch 2.277934504375244 | train loss 0.5832933784 | val loss 0.5827990174:
	--> training ...
	--> checkpointing ...

2024-10-11_04-41 : iter     555000 <=> epoch 2.2986429998695646 | train loss 0.5816803575 | val loss 0.5820692778:
	--> training ...
	--> checkpointing ...

2024-10-11_05-03 : iter     560000 <=> epoch 2.3193514953638847 | train loss 0.5826379061 | val loss 0.5828715563:
	--> training ...
	--> checkpointing ...

2024-10-11_05-24 : iter     565000 <=> epoch 2.3400599908582054 | train loss 0.5828589797 | val loss 0.5833522677:
	--> training ...
	--> checkpointing ...

2024-10-11_05-46 : iter     570000 <=> epoch 2.3607684863525256 | train loss 0.5828084946 | val loss 0.5834184885:
	--> training ...
	--> checkpointing ...

2024-10-11_06-07 : iter     575000 <=> epoch 2.381476981846846 | train loss 0.5826178789 | val loss 0.5834191442:
	--> training ...
	--> checkpointing ...

2024-10-11_06-28 : iter     580000 <=> epoch 2.4021854773411664 | train loss 0.5817533731 | val loss 0.5823770761:
	--> training ...
	--> checkpointing ...

2024-10-11_06-50 : iter     585000 <=> epoch 2.422893972835487 | train loss 0.5833316445 | val loss 0.5818587542:
	--> training ...
	--> checkpointing ...

2024-10-11_07-11 : iter     590000 <=> epoch 2.443602468329807 | train loss 0.5826039910 | val loss 0.5828957558:
	--> training ...
	--> checkpointing ...

2024-10-11_07-33 : iter     595000 <=> epoch 2.464310963824128 | train loss 0.5823333859 | val loss 0.5833419561:
	--> training ...
	--> checkpointing ...

2024-10-11_07-54 : iter     600000 <=> epoch 2.485019459318448 | train loss 0.5825483203 | val loss 0.5823547244:
	--> training ...
	--> checkpointing ...

2024-10-11_08-16 : iter     605000 <=> epoch 2.5057279548127687 | train loss 0.5821185112 | val loss 0.5828862786:
	--> training ...
	--> checkpointing ...

2024-10-11_08-37 : iter     610000 <=> epoch 2.526436450307089 | train loss 0.5837694407 | val loss 0.5828089118:
	--> training ...
	--> checkpointing ...

2024-10-11_08-59 : iter     615000 <=> epoch 2.5471449458014095 | train loss 0.5828197598 | val loss 0.5832040310:
	--> training ...
	--> checkpointing ...

2024-10-11_09-20 : iter     620000 <=> epoch 2.5678534412957297 | train loss 0.5827480555 | val loss 0.5823403597:
	--> training ...
	--> checkpointing ...

2024-10-11_09-41 : iter     625000 <=> epoch 2.5885619367900503 | train loss 0.5818713307 | val loss 0.5827413797:
	--> training ...
	--> checkpointing ...

2024-10-11_10-03 : iter     630000 <=> epoch 2.6092704322843705 | train loss 0.5825948119 | val loss 0.5830758810:
	--> training ...
	--> checkpointing ...

2024-10-11_10-24 : iter     635000 <=> epoch 2.629978927778691 | train loss 0.5832122564 | val loss 0.5829421878:
	--> training ...
	--> checkpointing ...

2024-10-11_10-46 : iter     640000 <=> epoch 2.6506874232730113 | train loss 0.5827443004 | val loss 0.5830931664:
	--> training ...
	--> checkpointing ...

2024-10-11_11-07 : iter     645000 <=> epoch 2.671395918767332 | train loss 0.5831827521 | val loss 0.5838836432:
	--> training ...
	--> checkpointing ...

2024-10-11_11-29 : iter     650000 <=> epoch 2.692104414261652 | train loss 0.5829688907 | val loss 0.5831720233:
	--> training ...
	--> checkpointing ...

2024-10-11_11-50 : iter     655000 <=> epoch 2.712812909755973 | train loss 0.5822166204 | val loss 0.5816525221:
	--> training ...
	--> checkpointing ...

2024-10-11_12-12 : iter     660000 <=> epoch 2.733521405250293 | train loss 0.5833228827 | val loss 0.5828500986:
	--> training ...
	--> checkpointing ...

2024-10-11_12-33 : iter     665000 <=> epoch 2.754229900744613 | train loss 0.5832424164 | val loss 0.5831258893:
	--> training ...
	--> checkpointing ...

2024-10-11_12-55 : iter     670000 <=> epoch 2.774938396238934 | train loss 0.5832017660 | val loss 0.5834575891:
	--> training ...
	--> checkpointing ...

2024-10-11_13-16 : iter     675000 <=> epoch 2.795646891733254 | train loss 0.5821489096 | val loss 0.5823979974:
	--> training ...
	--> checkpointing ...

2024-10-11_13-38 : iter     680000 <=> epoch 2.8163553872275746 | train loss 0.5823537707 | val loss 0.5821385980:
	--> training ...
	--> checkpointing ...

2024-10-11_13-59 : iter     685000 <=> epoch 2.837063882721895 | train loss 0.5829995275 | val loss 0.5815544724:
	--> training ...
	--> checkpointing ...

2024-10-11_14-20 : iter     690000 <=> epoch 2.8577723782162154 | train loss 0.5822420120 | val loss 0.5830807686:
	--> training ...
	--> checkpointing ...

2024-10-11_14-42 : iter     695000 <=> epoch 2.8784808737105356 | train loss 0.5832516551 | val loss 0.5820031166:
	--> training ...
	--> checkpointing ...

2024-10-11_15-03 : iter     700000 <=> epoch 2.8991893692048563 | train loss 0.5814936161 | val loss 0.5831848979:
	--> training ...
	--> checkpointing ...

2024-10-11_15-25 : iter     705000 <=> epoch 2.9198978646991764 | train loss 0.5820620060 | val loss 0.5821431875:
	--> training ...
	--> checkpointing ...

2024-10-11_15-46 : iter     710000 <=> epoch 2.940606360193497 | train loss 0.5827710032 | val loss 0.5825393796:
	--> training ...
	--> checkpointing ...

2024-10-11_16-08 : iter     715000 <=> epoch 2.9613148556878173 | train loss 0.5827577114 | val loss 0.5820543170:
	--> training ...
	--> checkpointing ...

2024-10-11_16-29 : iter     720000 <=> epoch 2.982023351182138 | train loss 0.5827410817 | val loss 0.5831268430:
	--> training ...
