|ITERS: 751563 / 751563 | COMP: 100.00% | RATE: 4.13 it./s | SPD: 0.2420 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 12.37 it./s | SPD: 0.0808 s/it.| ERT: (0, 0, 0, 0) |                                                                                                      

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:7.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 4)
	--> val.bin
	--> took (0, 0, 0, 1)

Setting train-hyperparams and util variables:
	--> max_iters = 751563

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 30M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-09_15-11 : iter     0 <=> epoch 0 | train loss 4.1464 | val loss 4.1462:
	--> training ...
	--> checkpointing ...

2024-10-09_15-32 : iter       5000 <=> epoch 0.019958381361345555 | train loss 0.6973161697 | val loss 0.6992985010:
	--> training ...
	--> checkpointing ...

2024-10-09_15-53 : iter      10000 <=> epoch 0.03991676272269111 | train loss 0.6800915599 | val loss 0.6786198616:
	--> training ...
	--> checkpointing ...

2024-10-09_16-15 : iter      15000 <=> epoch 0.05987514408403667 | train loss 0.6668831110 | val loss 0.6679112911:
	--> training ...
	--> checkpointing ...

2024-10-09_16-36 : iter      20000 <=> epoch 0.07983352544538222 | train loss 0.6602786183 | val loss 0.6592264175:
	--> training ...
	--> checkpointing ...

2024-10-09_16-57 : iter      25000 <=> epoch 0.09979190680672778 | train loss 0.6538727880 | val loss 0.6541639566:
	--> training ...
	--> checkpointing ...

2024-10-09_17-19 : iter      30000 <=> epoch 0.11975028816807334 | train loss 0.6476417780 | val loss 0.6483953595:
	--> training ...
	--> checkpointing ...

2024-10-09_17-40 : iter      35000 <=> epoch 0.1397086695294189 | train loss 0.6453818679 | val loss 0.6456683278:
	--> training ...
	--> checkpointing ...

2024-10-09_18-01 : iter      40000 <=> epoch 0.15966705089076444 | train loss 0.6435285211 | val loss 0.6417919993:
	--> training ...
	--> checkpointing ...

2024-10-09_18-23 : iter      45000 <=> epoch 0.17962543225211 | train loss 0.6406574845 | val loss 0.6420242190:
	--> training ...
	--> checkpointing ...

2024-10-09_18-44 : iter      50000 <=> epoch 0.19958381361345556 | train loss 0.6386896968 | val loss 0.6409647465:
	--> training ...
	--> checkpointing ...

2024-10-09_19-05 : iter      55000 <=> epoch 0.21954219497480112 | train loss 0.6386933923 | val loss 0.6385564208:
	--> training ...
	--> checkpointing ...

2024-10-09_19-27 : iter      60000 <=> epoch 0.2395005763361467 | train loss 0.6370784640 | val loss 0.6368101835:
	--> training ...
	--> checkpointing ...

2024-10-09_19-48 : iter      65000 <=> epoch 0.2594589576974922 | train loss 0.6363796592 | val loss 0.6371104717:
	--> training ...
	--> checkpointing ...

2024-10-09_20-09 : iter      70000 <=> epoch 0.2794173390588378 | train loss 0.6350244880 | val loss 0.6366245747:
	--> training ...
	--> checkpointing ...

2024-10-09_20-31 : iter      75000 <=> epoch 0.29937572042018334 | train loss 0.6340392828 | val loss 0.6343389750:
	--> training ...
	--> checkpointing ...

2024-10-09_20-52 : iter      80000 <=> epoch 0.3193341017815289 | train loss 0.6316221356 | val loss 0.6340455413:
	--> training ...
	--> checkpointing ...

2024-10-09_21-13 : iter      85000 <=> epoch 0.33929248314287447 | train loss 0.6325747967 | val loss 0.6316469312:
	--> training ...
	--> checkpointing ...

2024-10-09_21-35 : iter      90000 <=> epoch 0.35925086450422 | train loss 0.6329619288 | val loss 0.6327720881:
	--> training ...
	--> checkpointing ...

2024-10-09_21-56 : iter      95000 <=> epoch 0.3792092458655656 | train loss 0.6317972541 | val loss 0.6317209005:
	--> training ...
	--> checkpointing ...

2024-10-09_22-17 : iter     100000 <=> epoch 0.3991676272269111 | train loss 0.6310111880 | val loss 0.6312511563:
	--> training ...
	--> checkpointing ...

2024-10-09_22-39 : iter     105000 <=> epoch 0.41912600858825666 | train loss 0.6308122873 | val loss 0.6304669380:
	--> training ...
	--> checkpointing ...

2024-10-09_23-00 : iter     110000 <=> epoch 0.43908438994960225 | train loss 0.6297885776 | val loss 0.6306887865:
	--> training ...
	--> checkpointing ...

2024-10-09_23-21 : iter     115000 <=> epoch 0.4590427713109478 | train loss 0.6297054291 | val loss 0.6310639977:
	--> training ...
	--> checkpointing ...

2024-10-09_23-43 : iter     120000 <=> epoch 0.4790011526722934 | train loss 0.6298603415 | val loss 0.6295966506:
	--> training ...
	--> checkpointing ...

2024-10-10_00-04 : iter     125000 <=> epoch 0.4989595340336389 | train loss 0.6302847862 | val loss 0.6295006275:
	--> training ...
	--> checkpointing ...

2024-10-10_00-25 : iter     130000 <=> epoch 0.5189179153949844 | train loss 0.6280621290 | val loss 0.6276361942:
	--> training ...
	--> checkpointing ...

2024-10-10_00-47 : iter     135000 <=> epoch 0.53887629675633 | train loss 0.6278663278 | val loss 0.6292256117:
	--> training ...
	--> checkpointing ...

2024-10-10_01-08 : iter     140000 <=> epoch 0.5588346781176756 | train loss 0.6281758547 | val loss 0.6285893917:
	--> training ...
	--> checkpointing ...

2024-10-10_01-29 : iter     145000 <=> epoch 0.5787930594790212 | train loss 0.6275252700 | val loss 0.6300853491:
	--> training ...
	--> checkpointing ...

2024-10-10_01-51 : iter     150000 <=> epoch 0.5987514408403667 | train loss 0.6282652020 | val loss 0.6291118264:
	--> training ...
	--> checkpointing ...

2024-10-10_02-12 : iter     155000 <=> epoch 0.6187098222017122 | train loss 0.6269422770 | val loss 0.6273732781:
	--> training ...
	--> checkpointing ...

2024-10-10_02-33 : iter     160000 <=> epoch 0.6386682035630578 | train loss 0.6287382841 | val loss 0.6274878383:
	--> training ...
	--> checkpointing ...

2024-10-10_02-55 : iter     165000 <=> epoch 0.6586265849244034 | train loss 0.6271893382 | val loss 0.6273369789:
	--> training ...
	--> checkpointing ...

2024-10-10_03-16 : iter     170000 <=> epoch 0.6785849662857489 | train loss 0.6264318228 | val loss 0.6268429160:
	--> training ...
	--> checkpointing ...

2024-10-10_03-37 : iter     175000 <=> epoch 0.6985433476470945 | train loss 0.6264196038 | val loss 0.6256467700:
	--> training ...
	--> checkpointing ...

2024-10-10_03-59 : iter     180000 <=> epoch 0.71850172900844 | train loss 0.6264179349 | val loss 0.6266585588:
	--> training ...
	--> checkpointing ...

2024-10-10_04-20 : iter     185000 <=> epoch 0.7384601103697855 | train loss 0.6266707778 | val loss 0.6262595654:
	--> training ...
	--> checkpointing ...

2024-10-10_04-41 : iter     190000 <=> epoch 0.7584184917311312 | train loss 0.6252991557 | val loss 0.6270395517:
	--> training ...
	--> checkpointing ...

2024-10-10_05-03 : iter     195000 <=> epoch 0.7783768730924767 | train loss 0.6261727810 | val loss 0.6257721782:
	--> training ...
	--> checkpointing ...

2024-10-10_05-24 : iter     200000 <=> epoch 0.7983352544538223 | train loss 0.6269212961 | val loss 0.6251524687:
	--> training ...
	--> checkpointing ...

2024-10-10_05-45 : iter     205000 <=> epoch 0.8182936358151678 | train loss 0.6254268289 | val loss 0.6262418032:
	--> training ...
	--> checkpointing ...

2024-10-10_06-07 : iter     210000 <=> epoch 0.8382520171765133 | train loss 0.6257891655 | val loss 0.6260639429:
	--> training ...
	--> checkpointing ...

2024-10-10_06-28 : iter     215000 <=> epoch 0.858210398537859 | train loss 0.6254832149 | val loss 0.6263221502:
	--> training ...
	--> checkpointing ...

2024-10-10_06-49 : iter     220000 <=> epoch 0.8781687798992045 | train loss 0.6239971519 | val loss 0.6259750724:
	--> training ...
	--> checkpointing ...

2024-10-10_07-10 : iter     225000 <=> epoch 0.89812716126055 | train loss 0.6248303056 | val loss 0.6256013513:
	--> training ...
	--> checkpointing ...

2024-10-10_07-32 : iter     230000 <=> epoch 0.9180855426218956 | train loss 0.6260141134 | val loss 0.6254925728:
	--> training ...
	--> checkpointing ...

2024-10-10_07-53 : iter     235000 <=> epoch 0.9380439239832411 | train loss 0.6242570877 | val loss 0.6255659461:
	--> training ...
	--> checkpointing ...

2024-10-10_08-14 : iter     240000 <=> epoch 0.9580023053445867 | train loss 0.6250976920 | val loss 0.6248394847:
	--> training ...
	--> checkpointing ...

2024-10-10_08-36 : iter     245000 <=> epoch 0.9779606867059323 | train loss 0.6247977614 | val loss 0.6238224506:
	--> training ...
	--> checkpointing ...

2024-10-10_08-57 : iter     250000 <=> epoch 0.9979190680672778 | train loss 0.6240642071 | val loss 0.6244305372:
	--> training ...
	--> checkpointing ...

2024-10-10_09-18 : iter     255000 <=> epoch 1.0178774494286233 | train loss 0.6252833605 | val loss 0.6236957908:
	--> training ...
	--> checkpointing ...

2024-10-10_09-40 : iter     260000 <=> epoch 1.0378358307899689 | train loss 0.6250752211 | val loss 0.6251845956:
	--> training ...
	--> checkpointing ...

2024-10-10_10-01 : iter     265000 <=> epoch 1.0577942121513144 | train loss 0.6236062646 | val loss 0.6243252754:
	--> training ...
	--> checkpointing ...

2024-10-10_10-23 : iter     270000 <=> epoch 1.07775259351266 | train loss 0.6230927706 | val loss 0.6244320273:
	--> training ...
	--> checkpointing ...

2024-10-10_10-44 : iter     275000 <=> epoch 1.0977109748740057 | train loss 0.6233845949 | val loss 0.6254112720:
	--> training ...
	--> checkpointing ...

2024-10-10_11-05 : iter     280000 <=> epoch 1.1176693562353512 | train loss 0.6232066154 | val loss 0.6244766712:
	--> training ...
	--> checkpointing ...

2024-10-10_11-27 : iter     285000 <=> epoch 1.1376277375966968 | train loss 0.6230880618 | val loss 0.6222562790:
	--> training ...
	--> checkpointing ...

2024-10-10_11-48 : iter     290000 <=> epoch 1.1575861189580423 | train loss 0.6239823103 | val loss 0.6247279048:
	--> training ...
	--> checkpointing ...

2024-10-10_12-10 : iter     295000 <=> epoch 1.1775445003193878 | train loss 0.6233547330 | val loss 0.6232078075:
	--> training ...
	--> checkpointing ...

2024-10-10_12-31 : iter     300000 <=> epoch 1.1975028816807334 | train loss 0.6234112382 | val loss 0.6235531569:
	--> training ...
	--> checkpointing ...

2024-10-10_12-52 : iter     305000 <=> epoch 1.217461263042079 | train loss 0.6230621934 | val loss 0.6235837340:
	--> training ...
	--> checkpointing ...

2024-10-10_13-14 : iter     310000 <=> epoch 1.2374196444034244 | train loss 0.6226527691 | val loss 0.6244636178:
	--> training ...
	--> checkpointing ...

2024-10-10_13-35 : iter     315000 <=> epoch 1.25737802576477 | train loss 0.6222311258 | val loss 0.6232538819:
	--> training ...
	--> checkpointing ...

2024-10-10_13-56 : iter     320000 <=> epoch 1.2773364071261155 | train loss 0.6227076054 | val loss 0.6228784323:
	--> training ...
	--> checkpointing ...

2024-10-10_14-18 : iter     325000 <=> epoch 1.2972947884874613 | train loss 0.6240295768 | val loss 0.6235320568:
	--> training ...
	--> checkpointing ...

2024-10-10_14-39 : iter     330000 <=> epoch 1.3172531698488068 | train loss 0.6220242381 | val loss 0.6219905019:
	--> training ...
	--> checkpointing ...

2024-10-10_15-01 : iter     335000 <=> epoch 1.3372115512101523 | train loss 0.6224577427 | val loss 0.6227535605:
	--> training ...
	--> checkpointing ...

2024-10-10_15-22 : iter     340000 <=> epoch 1.3571699325714979 | train loss 0.6226491928 | val loss 0.6236935854:
	--> training ...
	--> checkpointing ...

2024-10-10_15-43 : iter     345000 <=> epoch 1.3771283139328434 | train loss 0.6228646040 | val loss 0.6231892109:
	--> training ...
	--> checkpointing ...

2024-10-10_16-05 : iter     350000 <=> epoch 1.397086695294189 | train loss 0.6243550181 | val loss 0.6238824725:
	--> training ...
	--> checkpointing ...

2024-10-10_16-26 : iter     355000 <=> epoch 1.4170450766555345 | train loss 0.6225925088 | val loss 0.6234818697:
	--> training ...
	--> checkpointing ...

2024-10-10_16-48 : iter     360000 <=> epoch 1.43700345801688 | train loss 0.6227939129 | val loss 0.6234216094:
	--> training ...
	--> checkpointing ...

2024-10-10_17-09 : iter     365000 <=> epoch 1.4569618393782255 | train loss 0.6226668358 | val loss 0.6237652302:
	--> training ...
	--> checkpointing ...

2024-10-10_17-31 : iter     370000 <=> epoch 1.476920220739571 | train loss 0.6222096682 | val loss 0.6231312156:
	--> training ...
	--> checkpointing ...

2024-10-10_17-52 : iter     375000 <=> epoch 1.4968786021009168 | train loss 0.6224411726 | val loss 0.6220250130:
	--> training ...
	--> checkpointing ...

2024-10-10_18-14 : iter     380000 <=> epoch 1.5168369834622624 | train loss 0.6224443316 | val loss 0.6223956943:
	--> training ...
	--> checkpointing ...

2024-10-10_18-35 : iter     385000 <=> epoch 1.536795364823608 | train loss 0.6216308475 | val loss 0.6218541265:
	--> training ...
	--> checkpointing ...

2024-10-10_18-57 : iter     390000 <=> epoch 1.5567537461849534 | train loss 0.6219851971 | val loss 0.6212952733:
	--> training ...
	--> checkpointing ...

2024-10-10_19-18 : iter     395000 <=> epoch 1.576712127546299 | train loss 0.6218774915 | val loss 0.6206645966:
	--> training ...
	--> checkpointing ...

2024-10-10_19-40 : iter     400000 <=> epoch 1.5966705089076445 | train loss 0.6218605638 | val loss 0.6205844879:
	--> training ...
	--> checkpointing ...

2024-10-10_20-02 : iter     405000 <=> epoch 1.61662889026899 | train loss 0.6237429380 | val loss 0.6207584739:
	--> training ...
	--> checkpointing ...

2024-10-10_20-23 : iter     410000 <=> epoch 1.6365872716303356 | train loss 0.6229447126 | val loss 0.6219416857:
	--> training ...
	--> checkpointing ...

2024-10-10_20-45 : iter     415000 <=> epoch 1.656545652991681 | train loss 0.6218744516 | val loss 0.6229819655:
	--> training ...
	--> checkpointing ...

2024-10-10_21-06 : iter     420000 <=> epoch 1.6765040343530266 | train loss 0.6224551201 | val loss 0.6221411228:
	--> training ...
	--> checkpointing ...

2024-10-10_21-28 : iter     425000 <=> epoch 1.6964624157143724 | train loss 0.6220103502 | val loss 0.6220317483:
	--> training ...
	--> checkpointing ...

2024-10-10_21-49 : iter     430000 <=> epoch 1.716420797075718 | train loss 0.6207413673 | val loss 0.6221910119:
	--> training ...
	--> checkpointing ...

2024-10-10_22-11 : iter     435000 <=> epoch 1.7363791784370635 | train loss 0.6215525270 | val loss 0.6217254996:
	--> training ...
	--> checkpointing ...

2024-10-10_22-32 : iter     440000 <=> epoch 1.756337559798409 | train loss 0.6208320856 | val loss 0.6236290336:
	--> training ...
	--> checkpointing ...

2024-10-10_22-54 : iter     445000 <=> epoch 1.7762959411597545 | train loss 0.6200928092 | val loss 0.6206796169:
	--> training ...
	--> checkpointing ...

2024-10-10_23-15 : iter     450000 <=> epoch 1.7962543225211 | train loss 0.6211196184 | val loss 0.6215139627:
	--> training ...
	--> checkpointing ...

2024-10-10_23-37 : iter     455000 <=> epoch 1.8162127038824456 | train loss 0.6229388714 | val loss 0.6212553382:
	--> training ...
	--> checkpointing ...

2024-10-10_23-58 : iter     460000 <=> epoch 1.8361710852437911 | train loss 0.6216613650 | val loss 0.6214269996:
	--> training ...
	--> checkpointing ...

2024-10-11_00-20 : iter     465000 <=> epoch 1.8561294666051367 | train loss 0.6201618910 | val loss 0.6223296523:
	--> training ...
	--> checkpointing ...

2024-10-11_00-41 : iter     470000 <=> epoch 1.8760878479664822 | train loss 0.6210573912 | val loss 0.6218398809:
	--> training ...
	--> checkpointing ...

2024-10-11_01-03 : iter     475000 <=> epoch 1.896046229327828 | train loss 0.6199187636 | val loss 0.6211573482:
	--> training ...
	--> checkpointing ...

2024-10-11_01-24 : iter     480000 <=> epoch 1.9160046106891735 | train loss 0.6217393279 | val loss 0.6216410398:
	--> training ...
	--> checkpointing ...

2024-10-11_01-45 : iter     485000 <=> epoch 1.935962992050519 | train loss 0.6204336882 | val loss 0.6218535900:
	--> training ...
	--> checkpointing ...

2024-10-11_02-07 : iter     490000 <=> epoch 1.9559213734118646 | train loss 0.6216737628 | val loss 0.6218536496:
	--> training ...
	--> checkpointing ...

2024-10-11_02-28 : iter     495000 <=> epoch 1.97587975477321 | train loss 0.6206604838 | val loss 0.6207810640:
	--> training ...
	--> checkpointing ...

2024-10-11_02-50 : iter     500000 <=> epoch 1.9958381361345556 | train loss 0.6218838692 | val loss 0.6216488481:
	--> training ...
	--> checkpointing ...

2024-10-11_03-11 : iter     505000 <=> epoch 2.015796517495901 | train loss 0.6209592819 | val loss 0.6199796200:
	--> training ...
	--> checkpointing ...

2024-10-11_03-33 : iter     510000 <=> epoch 2.0357548988572467 | train loss 0.6210979223 | val loss 0.6200850010:
	--> training ...
	--> checkpointing ...

2024-10-11_03-54 : iter     515000 <=> epoch 2.0557132802185922 | train loss 0.6214624047 | val loss 0.6208817959:
	--> training ...
	--> checkpointing ...

2024-10-11_04-16 : iter     520000 <=> epoch 2.0756716615799378 | train loss 0.6215026975 | val loss 0.6214280128:
	--> training ...
	--> checkpointing ...

2024-10-11_04-37 : iter     525000 <=> epoch 2.0956300429412833 | train loss 0.6188517809 | val loss 0.6220405102:
	--> training ...
	--> checkpointing ...

2024-10-11_04-59 : iter     530000 <=> epoch 2.115588424302629 | train loss 0.6145653725 | val loss 0.6173102856:
	--> training ...
	--> checkpointing ...

2024-10-11_05-20 : iter     535000 <=> epoch 2.1355468056639744 | train loss 0.6156731844 | val loss 0.6164057851:
	--> training ...
	--> checkpointing ...

2024-10-11_05-42 : iter     540000 <=> epoch 2.15550518702532 | train loss 0.6165163517 | val loss 0.6153637767:
	--> training ...
	--> checkpointing ...

2024-10-11_06-03 : iter     545000 <=> epoch 2.1754635683866654 | train loss 0.6139765978 | val loss 0.6144061685:
	--> training ...
	--> checkpointing ...

2024-10-11_06-25 : iter     550000 <=> epoch 2.1954219497480114 | train loss 0.6139591336 | val loss 0.6140408516:
	--> training ...
	--> checkpointing ...

2024-10-11_06-46 : iter     555000 <=> epoch 2.215380331109357 | train loss 0.6139960885 | val loss 0.6147211194:
	--> training ...
	--> checkpointing ...

2024-10-11_07-08 : iter     560000 <=> epoch 2.2353387124707025 | train loss 0.6137153506 | val loss 0.6140964031:
	--> training ...
	--> checkpointing ...

2024-10-11_07-29 : iter     565000 <=> epoch 2.255297093832048 | train loss 0.6145539284 | val loss 0.6153873801:
	--> training ...
	--> checkpointing ...

2024-10-11_07-51 : iter     570000 <=> epoch 2.2752554751933936 | train loss 0.6140283346 | val loss 0.6139575243:
	--> training ...
	--> checkpointing ...

2024-10-11_08-13 : iter     575000 <=> epoch 2.295213856554739 | train loss 0.6140133142 | val loss 0.6125743389:
	--> training ...
	--> checkpointing ...

2024-10-11_08-34 : iter     580000 <=> epoch 2.3151722379160846 | train loss 0.6143234372 | val loss 0.6143996119:
	--> training ...
	--> checkpointing ...

2024-10-11_08-56 : iter     585000 <=> epoch 2.33513061927743 | train loss 0.6142247319 | val loss 0.6149392724:
	--> training ...
	--> checkpointing ...

2024-10-11_09-17 : iter     590000 <=> epoch 2.3550890006387757 | train loss 0.6132678986 | val loss 0.6141647696:
	--> training ...
	--> checkpointing ...

2024-10-11_09-39 : iter     595000 <=> epoch 2.375047382000121 | train loss 0.6155437827 | val loss 0.6125339270:
	--> training ...
	--> checkpointing ...

2024-10-11_10-00 : iter     600000 <=> epoch 2.3950057633614668 | train loss 0.6137561798 | val loss 0.6144207716:
	--> training ...
	--> checkpointing ...

2024-10-11_10-22 : iter     605000 <=> epoch 2.4149641447228123 | train loss 0.6129572988 | val loss 0.6142228246:
	--> training ...
	--> checkpointing ...

2024-10-11_10-43 : iter     610000 <=> epoch 2.434922526084158 | train loss 0.6149243712 | val loss 0.6132642627:
	--> training ...
	--> checkpointing ...

2024-10-11_11-05 : iter     615000 <=> epoch 2.4548809074455034 | train loss 0.6135951877 | val loss 0.6127362251:
	--> training ...
	--> checkpointing ...

2024-10-11_11-26 : iter     620000 <=> epoch 2.474839288806849 | train loss 0.6131331921 | val loss 0.6132907867:
	--> training ...
	--> checkpointing ...

2024-10-11_11-48 : iter     625000 <=> epoch 2.4947976701681944 | train loss 0.6143524647 | val loss 0.6137174368:
	--> training ...
	--> checkpointing ...

2024-10-11_12-09 : iter     630000 <=> epoch 2.51475605152954 | train loss 0.6139179468 | val loss 0.6145719886:
	--> training ...
	--> checkpointing ...

2024-10-11_12-31 : iter     635000 <=> epoch 2.5347144328908855 | train loss 0.6138179898 | val loss 0.6134761572:
	--> training ...
	--> checkpointing ...

2024-10-11_12-52 : iter     640000 <=> epoch 2.554672814252231 | train loss 0.6142427921 | val loss 0.6136518121:
	--> training ...
	--> checkpointing ...

2024-10-11_13-14 : iter     645000 <=> epoch 2.5746311956135766 | train loss 0.6145276427 | val loss 0.6138334870:
	--> training ...
	--> checkpointing ...

2024-10-11_13-35 : iter     650000 <=> epoch 2.5945895769749225 | train loss 0.6130513549 | val loss 0.6136681437:
	--> training ...
	--> checkpointing ...

2024-10-11_13-57 : iter     655000 <=> epoch 2.614547958336268 | train loss 0.6125521660 | val loss 0.6121166945:
	--> training ...
	--> checkpointing ...

2024-10-11_14-18 : iter     660000 <=> epoch 2.6345063396976136 | train loss 0.6107878089 | val loss 0.6121088862:
	--> training ...
	--> checkpointing ...

2024-10-11_14-40 : iter     665000 <=> epoch 2.654464721058959 | train loss 0.6138211489 | val loss 0.6121589541:
	--> training ...
	--> checkpointing ...

2024-10-11_15-01 : iter     670000 <=> epoch 2.6744231024203047 | train loss 0.6126914024 | val loss 0.6134684682:
	--> training ...
	--> checkpointing ...

2024-10-11_15-23 : iter     675000 <=> epoch 2.69438148378165 | train loss 0.6124826074 | val loss 0.6127744317:
	--> training ...
	--> checkpointing ...

2024-10-11_15-44 : iter     680000 <=> epoch 2.7143398651429957 | train loss 0.6127335429 | val loss 0.6123162508:
	--> training ...
	--> checkpointing ...

2024-10-11_16-06 : iter     685000 <=> epoch 2.7342982465043413 | train loss 0.6133151054 | val loss 0.6136717796:
	--> training ...
	--> checkpointing ...

2024-10-11_16-27 : iter     690000 <=> epoch 2.754256627865687 | train loss 0.6134115458 | val loss 0.6138945222:
	--> training ...
	--> checkpointing ...

2024-10-11_16-49 : iter     695000 <=> epoch 2.7742150092270323 | train loss 0.6116712689 | val loss 0.6153911948:
	--> training ...
	--> checkpointing ...

2024-10-11_17-10 : iter     700000 <=> epoch 2.794173390588378 | train loss 0.6129686236 | val loss 0.6128761172:
	--> training ...
	--> checkpointing ...

2024-10-11_17-32 : iter     705000 <=> epoch 2.8141317719497234 | train loss 0.6124277711 | val loss 0.6134656668:
	--> training ...
	--> checkpointing ...

2024-10-11_17-53 : iter     710000 <=> epoch 2.834090153311069 | train loss 0.6129399538 | val loss 0.6126028895:
	--> training ...
	--> checkpointing ...

2024-10-11_18-15 : iter     715000 <=> epoch 2.8540485346724145 | train loss 0.6134241819 | val loss 0.6126595736:
	--> training ...
	--> checkpointing ...

2024-10-11_18-36 : iter     720000 <=> epoch 2.87400691603376 | train loss 0.6119027734 | val loss 0.6139447093:
	--> training ...
	--> checkpointing ...

2024-10-11_18-58 : iter     725000 <=> epoch 2.8939652973951056 | train loss 0.6115673780 | val loss 0.6120256782:
	--> training ...
	--> checkpointing ...

2024-10-11_19-19 : iter     730000 <=> epoch 2.913923678756451 | train loss 0.6140525937 | val loss 0.6134140491:
	--> training ...
	--> checkpointing ...

2024-10-11_19-41 : iter     735000 <=> epoch 2.9338820601177966 | train loss 0.6130908132 | val loss 0.6123091578:
	--> training ...
	--> checkpointing ...

2024-10-11_20-02 : iter     740000 <=> epoch 2.953840441479142 | train loss 0.6120184064 | val loss 0.6135334969:
	--> training ...
	--> checkpointing ...

2024-10-11_20-24 : iter     745000 <=> epoch 2.9737988228404877 | train loss 0.6134517193 | val loss 0.6135453582:
	--> training ...
	--> checkpointing ...

2024-10-11_20-45 : iter     750000 <=> epoch 2.9937572042018337 | train loss 0.6126343012 | val loss 0.6135575175:
	--> training ...
