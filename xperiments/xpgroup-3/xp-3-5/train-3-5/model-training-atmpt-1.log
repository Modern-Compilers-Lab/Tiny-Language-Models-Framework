|ITERS: 400858 / 400858 | COMP: 100.00% | RATE: 1.21 it./s | SPD: 0.8253 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 4.05 it./s | SPD: 0.2469 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:5.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 400858

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable
	--> loading vocab_size

Creating the model:

The model has 62M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-09_13-14 : iter     0 <=> epoch 0 | train loss 4.4562 | val loss 4.4558:
	--> training ...
	--> checkpointing ...

2024-11-09_14-27 : iter       5000 <=> epoch 0.021204472154669998 | train loss 1.0962433815 | val loss 1.0957726240:
	--> training ...
	--> checkpointing ...

2024-11-09_15-40 : iter      10000 <=> epoch 0.042408944309339995 | train loss 1.0641652346 | val loss 1.0635238886:
	--> training ...
	--> checkpointing ...

2024-11-09_16-53 : iter      15000 <=> epoch 0.06361341646400999 | train loss 1.0376971960 | val loss 1.0384635925:
	--> training ...
	--> checkpointing ...

2024-11-09_18-06 : iter      20000 <=> epoch 0.08481788861867999 | train loss 1.0216073990 | val loss 1.0195095539:
	--> training ...
	--> checkpointing ...

2024-11-09_19-19 : iter      25000 <=> epoch 0.10602236077334998 | train loss 1.0136632919 | val loss 1.0133835077:
	--> training ...
	--> checkpointing ...

2024-11-09_20-31 : iter      30000 <=> epoch 0.12722683292801998 | train loss 1.0070236921 | val loss 1.0073760748:
	--> training ...
	--> checkpointing ...

2024-11-09_21-44 : iter      35000 <=> epoch 0.14843130508268998 | train loss 0.9998158216 | val loss 1.0004963875:
	--> training ...
	--> checkpointing ...

2024-11-09_22-57 : iter      40000 <=> epoch 0.16963577723735998 | train loss 0.9988477826 | val loss 0.9967751503:
	--> training ...
	--> checkpointing ...

2024-11-10_00-10 : iter      45000 <=> epoch 0.19084024939202995 | train loss 0.9960383773 | val loss 0.9956465960:
	--> training ...
	--> checkpointing ...

2024-11-10_01-22 : iter      50000 <=> epoch 0.21204472154669995 | train loss 0.9936407804 | val loss 0.9930983782:
	--> training ...
	--> checkpointing ...

2024-11-10_02-35 : iter      55000 <=> epoch 0.23324919370136996 | train loss 0.9910484552 | val loss 0.9907071590:
	--> training ...
	--> checkpointing ...

2024-11-10_03-48 : iter      60000 <=> epoch 0.25445366585603996 | train loss 0.9889991879 | val loss 0.9897148609:
	--> training ...
	--> checkpointing ...

2024-11-10_05-00 : iter      65000 <=> epoch 0.27565813801070993 | train loss 0.9877308011 | val loss 0.9887777567:
	--> training ...
	--> checkpointing ...

2024-11-10_06-13 : iter      70000 <=> epoch 0.29686261016537996 | train loss 0.9889855385 | val loss 0.9871347547:
	--> training ...
	--> checkpointing ...

2024-11-10_07-26 : iter      75000 <=> epoch 0.31806708232004993 | train loss 0.9875992537 | val loss 0.9877525568:
	--> training ...
	--> checkpointing ...

2024-11-10_08-39 : iter      80000 <=> epoch 0.33927155447471996 | train loss 0.9857162833 | val loss 0.9851084948:
	--> training ...
	--> checkpointing ...

2024-11-10_09-51 : iter      85000 <=> epoch 0.36047602662938993 | train loss 0.9861140251 | val loss 0.9846511483:
	--> training ...
	--> checkpointing ...

2024-11-10_11-04 : iter      90000 <=> epoch 0.3816804987840599 | train loss 0.9853942990 | val loss 0.9862055182:
	--> training ...
	--> checkpointing ...

2024-11-10_12-17 : iter      95000 <=> epoch 0.40288497093872994 | train loss 0.9844591022 | val loss 0.9844624400:
	--> training ...
	--> checkpointing ...

2024-11-10_13-29 : iter     100000 <=> epoch 0.4240894430933999 | train loss 0.9850722551 | val loss 0.9847803712:
	--> training ...
	--> checkpointing ...

2024-11-10_14-42 : iter     105000 <=> epoch 0.44529391524806994 | train loss 0.9853296280 | val loss 0.9840049148:
	--> training ...
	--> checkpointing ...

2024-11-10_15-55 : iter     110000 <=> epoch 0.4664983874027399 | train loss 0.9853923321 | val loss 0.9858154655:
	--> training ...
	--> checkpointing ...

2024-11-10_17-07 : iter     115000 <=> epoch 0.48770285955740994 | train loss 0.9836162329 | val loss 0.9839599729:
	--> training ...
	--> checkpointing ...

2024-11-10_18-20 : iter     120000 <=> epoch 0.5089073317120799 | train loss 0.9845135212 | val loss 0.9848891497:
	--> training ...
	--> checkpointing ...

2024-11-10_19-33 : iter     125000 <=> epoch 0.5301118038667499 | train loss 0.9849425554 | val loss 0.9844417572:
	--> training ...
	--> checkpointing ...

2024-11-10_20-46 : iter     130000 <=> epoch 0.5513162760214199 | train loss 0.9844037890 | val loss 0.9833517075:
	--> training ...
	--> checkpointing ...

2024-11-10_21-58 : iter     135000 <=> epoch 0.5725207481760899 | train loss 0.9833866954 | val loss 0.9833209515:
	--> training ...
	--> checkpointing ...

2024-11-10_23-11 : iter     140000 <=> epoch 0.5937252203307599 | train loss 0.9835996032 | val loss 0.9815424085:
	--> training ...
	--> checkpointing ...

2024-11-11_00-24 : iter     145000 <=> epoch 0.61492969248543 | train loss 0.9834774733 | val loss 0.9829014540:
	--> training ...
	--> checkpointing ...

2024-11-11_01-37 : iter     150000 <=> epoch 0.6361341646400999 | train loss 0.9833082557 | val loss 0.9837440252:
	--> training ...
	--> checkpointing ...

2024-11-11_02-49 : iter     155000 <=> epoch 0.6573386367947699 | train loss 0.9832671285 | val loss 0.9831851125:
	--> training ...
	--> checkpointing ...

2024-11-11_04-02 : iter     160000 <=> epoch 0.6785431089494399 | train loss 0.9824324250 | val loss 0.9832220078:
	--> training ...
	--> checkpointing ...

2024-11-11_05-15 : iter     165000 <=> epoch 0.6997475811041098 | train loss 0.9829018712 | val loss 0.9827832580:
	--> training ...
	--> checkpointing ...

2024-11-11_06-27 : iter     170000 <=> epoch 0.7209520532587799 | train loss 0.9820182920 | val loss 0.9822500348:
	--> training ...
	--> checkpointing ...

2024-11-11_07-40 : iter     175000 <=> epoch 0.7421565254134499 | train loss 0.9822583199 | val loss 0.9820521474:
	--> training ...
	--> checkpointing ...

2024-11-11_08-53 : iter     180000 <=> epoch 0.7633609975681198 | train loss 0.9832093716 | val loss 0.9829756021:
	--> training ...
	--> checkpointing ...

2024-11-11_10-05 : iter     185000 <=> epoch 0.7845654697227898 | train loss 0.9841170907 | val loss 0.9819805026:
	--> training ...
	--> checkpointing ...

2024-11-11_11-18 : iter     190000 <=> epoch 0.8057699418774599 | train loss 0.9815348983 | val loss 0.9821107388:
	--> training ...
	--> checkpointing ...

2024-11-11_12-31 : iter     195000 <=> epoch 0.8269744140321299 | train loss 0.9814785719 | val loss 0.9833292365:
	--> training ...
	--> checkpointing ...

2024-11-11_13-44 : iter     200000 <=> epoch 0.8481788861867998 | train loss 0.9824922085 | val loss 0.9826463461:
	--> training ...
	--> checkpointing ...

2024-11-11_14-56 : iter     205000 <=> epoch 0.8693833583414698 | train loss 0.9826874137 | val loss 0.9829779863:
	--> training ...
	--> checkpointing ...

2024-11-11_16-09 : iter     210000 <=> epoch 0.8905878304961399 | train loss 0.9828540087 | val loss 0.9818823934:
	--> training ...
	--> checkpointing ...

2024-11-11_17-22 : iter     215000 <=> epoch 0.9117923026508098 | train loss 0.9822865725 | val loss 0.9812403917:
	--> training ...
	--> checkpointing ...

2024-11-11_18-34 : iter     220000 <=> epoch 0.9329967748054798 | train loss 0.9822441936 | val loss 0.9822044969:
	--> training ...
	--> checkpointing ...

2024-11-11_19-47 : iter     225000 <=> epoch 0.9542012469601499 | train loss 0.9807956815 | val loss 0.9806692004:
	--> training ...
	--> checkpointing ...

2024-11-11_21-00 : iter     230000 <=> epoch 0.9754057191148199 | train loss 0.9829158783 | val loss 0.9826447964:
	--> training ...
	--> checkpointing ...

2024-11-11_22-13 : iter     235000 <=> epoch 0.9966101912694898 | train loss 0.9820179343 | val loss 0.9814561605:
	--> training ...
	--> checkpointing ...

2024-11-11_23-26 : iter     240000 <=> epoch 1.0178146634241598 | train loss 0.9827150106 | val loss 0.9818907380:
	--> training ...
	--> checkpointing ...

2024-11-12_00-39 : iter     245000 <=> epoch 1.0390191355788299 | train loss 0.9818902016 | val loss 0.9829272628:
	--> training ...
	--> checkpointing ...

2024-11-12_01-51 : iter     250000 <=> epoch 1.0602236077334999 | train loss 0.9820193648 | val loss 0.9834597707:
	--> training ...
	--> checkpointing ...

2024-11-12_03-04 : iter     255000 <=> epoch 1.08142807988817 | train loss 0.9805406928 | val loss 0.9811030030:
	--> training ...
	--> checkpointing ...

2024-11-12_04-17 : iter     260000 <=> epoch 1.1026325520428397 | train loss 0.9810130000 | val loss 0.9803543091:
	--> training ...
	--> checkpointing ...

2024-11-12_05-30 : iter     265000 <=> epoch 1.1238370241975097 | train loss 0.9820990562 | val loss 0.9811752439:
	--> training ...
	--> checkpointing ...

2024-11-12_06-43 : iter     270000 <=> epoch 1.1450414963521798 | train loss 0.9833810329 | val loss 0.9821977019:
	--> training ...
	--> checkpointing ...

2024-11-12_07-56 : iter     275000 <=> epoch 1.1662459685068498 | train loss 0.9819804430 | val loss 0.9819787145:
	--> training ...
	--> checkpointing ...

2024-11-12_09-08 : iter     280000 <=> epoch 1.1874504406615198 | train loss 0.9820940495 | val loss 0.9820834994:
	--> training ...
	--> checkpointing ...

2024-11-12_10-21 : iter     285000 <=> epoch 1.2086549128161899 | train loss 0.9791575670 | val loss 0.9783156514:
	--> training ...
	--> checkpointing ...

2024-11-12_11-34 : iter     290000 <=> epoch 1.22985938497086 | train loss 0.9784985781 | val loss 0.9786515832:
	--> training ...
	--> checkpointing ...

2024-11-12_12-46 : iter     295000 <=> epoch 1.2510638571255297 | train loss 0.9773532748 | val loss 0.9780331850:
	--> training ...
	--> checkpointing ...

2024-11-12_13-59 : iter     300000 <=> epoch 1.2722683292801997 | train loss 0.9782741070 | val loss 0.9776390791:
	--> training ...
	--> checkpointing ...

2024-11-12_15-12 : iter     305000 <=> epoch 1.2934728014348698 | train loss 0.9782559872 | val loss 0.9790053964:
	--> training ...
	--> checkpointing ...

2024-11-12_16-25 : iter     310000 <=> epoch 1.3146772735895398 | train loss 0.9774573445 | val loss 0.9771341681:
	--> training ...
	--> checkpointing ...

2024-11-12_17-38 : iter     315000 <=> epoch 1.3358817457442098 | train loss 0.9783588648 | val loss 0.9786622524:
	--> training ...
	--> checkpointing ...

2024-11-12_18-50 : iter     320000 <=> epoch 1.3570862178988798 | train loss 0.9779949188 | val loss 0.9775053263:
	--> training ...
	--> checkpointing ...

2024-11-12_20-03 : iter     325000 <=> epoch 1.3782906900535496 | train loss 0.9780398607 | val loss 0.9781430960:
	--> training ...
	--> checkpointing ...

2024-11-12_21-16 : iter     330000 <=> epoch 1.3994951622082197 | train loss 0.9776912928 | val loss 0.9785536528:
	--> training ...
	--> checkpointing ...

2024-11-12_22-29 : iter     335000 <=> epoch 1.4206996343628897 | train loss 0.9773638844 | val loss 0.9775009155:
	--> training ...
	--> checkpointing ...

2024-11-12_23-41 : iter     340000 <=> epoch 1.4419041065175597 | train loss 0.9781555533 | val loss 0.9782373905:
	--> training ...
	--> checkpointing ...

2024-11-13_00-54 : iter     345000 <=> epoch 1.4631085786722298 | train loss 0.9785389304 | val loss 0.9780405760:
	--> training ...
	--> checkpointing ...

2024-11-13_02-07 : iter     350000 <=> epoch 1.4843130508268998 | train loss 0.9773620963 | val loss 0.9777832031:
	--> training ...
	--> checkpointing ...

2024-11-13_03-20 : iter     355000 <=> epoch 1.5055175229815698 | train loss 0.9780557752 | val loss 0.9768338203:
	--> training ...
	--> checkpointing ...

2024-11-13_04-33 : iter     360000 <=> epoch 1.5267219951362396 | train loss 0.9771998525 | val loss 0.9782209396:
	--> training ...
	--> checkpointing ...

2024-11-13_05-45 : iter     365000 <=> epoch 1.5479264672909097 | train loss 0.9770489335 | val loss 0.9775471687:
	--> training ...
	--> checkpointing ...

2024-11-13_06-58 : iter     370000 <=> epoch 1.5691309394455797 | train loss 0.9769212604 | val loss 0.9772792459:
	--> training ...
	--> checkpointing ...

2024-11-13_08-11 : iter     375000 <=> epoch 1.5903354116002497 | train loss 0.9761832356 | val loss 0.9773784280:
	--> training ...
	--> checkpointing ...

2024-11-13_09-24 : iter     380000 <=> epoch 1.6115398837549197 | train loss 0.9777842760 | val loss 0.9788641334:
	--> training ...
	--> checkpointing ...

2024-11-13_10-37 : iter     385000 <=> epoch 1.6327443559095898 | train loss 0.9776299000 | val loss 0.9778043628:
	--> training ...
	--> checkpointing ...

2024-11-13_11-50 : iter     390000 <=> epoch 1.6539488280642598 | train loss 0.9776098132 | val loss 0.9762362838:
	--> training ...
	--> checkpointing ...

2024-11-13_13-02 : iter     395000 <=> epoch 1.6751533002189296 | train loss 0.9772618413 | val loss 0.9774016142:
	--> training ...
	--> checkpointing ...

2024-11-13_14-15 : iter     400000 <=> epoch 1.6963577723735996 | train loss 0.9779480100 | val loss 0.9787949324:
	--> training ...
