|ITERS: 135645 / 135645 | COMP: 100.00% | RATE: 2.67 it./s | SPD: 0.3748 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 8.35 it./s | SPD: 0.1197 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:1.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 0)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 135645

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the vocab_size:
	--> loading

Creating the model:

The model has 30M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-03_23-46 : iter     0 <=> epoch 0 | train loss 4.2978 | val loss 4.2977:
	--> training ...
	--> checkpointing ...

2024-11-04_00-19 : iter       5000 <=> epoch 0.03686068585942045 | train loss 1.0937597752 | val loss 1.0951026678:
	--> training ...
	--> checkpointing ...

2024-11-04_00-52 : iter      10000 <=> epoch 0.0737213717188409 | train loss 1.0639003515 | val loss 1.0633288622:
	--> training ...
	--> checkpointing ...

2024-11-04_01-26 : iter      15000 <=> epoch 0.11058205757826134 | train loss 1.0521855354 | val loss 1.0525736809:
	--> training ...
	--> checkpointing ...

2024-11-04_01-59 : iter      20000 <=> epoch 0.1474427434376818 | train loss 1.0393414497 | val loss 1.0419150591:
	--> training ...
	--> checkpointing ...

2024-11-04_02-32 : iter      25000 <=> epoch 0.18430342929710225 | train loss 1.0320349932 | val loss 1.0330826044:
	--> training ...
	--> checkpointing ...

2024-11-04_03-06 : iter      30000 <=> epoch 0.22116411515652268 | train loss 1.0290671587 | val loss 1.0287590027:
	--> training ...
	--> checkpointing ...

2024-11-04_03-39 : iter      35000 <=> epoch 0.25802480101594316 | train loss 1.0254977942 | val loss 1.0251425505:
	--> training ...
	--> checkpointing ...

2024-11-04_04-12 : iter      40000 <=> epoch 0.2948854868753636 | train loss 1.0223033428 | val loss 1.0248385668:
	--> training ...
	--> checkpointing ...

2024-11-04_04-45 : iter      45000 <=> epoch 0.331746172734784 | train loss 1.0201675892 | val loss 1.0196263790:
	--> training ...
	--> checkpointing ...

2024-11-04_05-19 : iter      50000 <=> epoch 0.3686068585942045 | train loss 1.0178287029 | val loss 1.0193626881:
	--> training ...
	--> checkpointing ...

2024-11-04_05-52 : iter      55000 <=> epoch 0.40546754445362493 | train loss 1.0176401138 | val loss 1.0182082653:
	--> training ...
	--> checkpointing ...

2024-11-04_06-25 : iter      60000 <=> epoch 0.44232823031304536 | train loss 1.0164512396 | val loss 1.0170204639:
	--> training ...
	--> checkpointing ...

2024-11-04_06-59 : iter      65000 <=> epoch 0.47918891617246584 | train loss 1.0141259432 | val loss 1.0147927999:
	--> training ...
	--> checkpointing ...

2024-11-04_07-32 : iter      70000 <=> epoch 0.5160496020318863 | train loss 1.0151326656 | val loss 1.0145425797:
	--> training ...
	--> checkpointing ...

2024-11-04_08-05 : iter      75000 <=> epoch 0.5529102878913067 | train loss 1.0120563507 | val loss 1.0146160126:
	--> training ...
	--> checkpointing ...

2024-11-04_08-39 : iter      80000 <=> epoch 0.5897709737507272 | train loss 1.0128411055 | val loss 1.0135630369:
	--> training ...
	--> checkpointing ...

2024-11-04_09-12 : iter      85000 <=> epoch 0.6266316596101477 | train loss 1.0128009319 | val loss 1.0114798546:
	--> training ...
	--> checkpointing ...

2024-11-04_09-45 : iter      90000 <=> epoch 0.663492345469568 | train loss 1.0108695030 | val loss 1.0110666752:
	--> training ...
	--> checkpointing ...

2024-11-04_10-18 : iter      95000 <=> epoch 0.7003530313289885 | train loss 1.0098528862 | val loss 1.0099991560:
	--> training ...
	--> checkpointing ...

2024-11-04_10-52 : iter     100000 <=> epoch 0.737213717188409 | train loss 1.0060338974 | val loss 1.0068851709:
	--> training ...
	--> checkpointing ...

2024-11-04_11-25 : iter     105000 <=> epoch 0.7740744030478294 | train loss 1.0051207542 | val loss 1.0061026812:
	--> training ...
	--> checkpointing ...

2024-11-04_11-58 : iter     110000 <=> epoch 0.8109350889072499 | train loss 1.0037748814 | val loss 1.0039083958:
	--> training ...
	--> checkpointing ...

2024-11-04_12-31 : iter     115000 <=> epoch 0.8477957747666703 | train loss 1.0037480593 | val loss 1.0043597221:
	--> training ...
	--> checkpointing ...

2024-11-04_13-05 : iter     120000 <=> epoch 0.8846564606260907 | train loss 1.0042593479 | val loss 1.0040303469:
	--> training ...
	--> checkpointing ...

2024-11-04_13-38 : iter     125000 <=> epoch 0.9215171464855112 | train loss 1.0046089888 | val loss 1.0035855770:
	--> training ...
	--> checkpointing ...

2024-11-04_14-11 : iter     130000 <=> epoch 0.9583778323449317 | train loss 1.0034201145 | val loss 1.0035220385:
	--> training ...
	--> checkpointing ...

2024-11-04_14-45 : iter     135000 <=> epoch 0.995238518204352 | train loss 1.0038588047 | val loss 1.0044442415:
	--> training ...
