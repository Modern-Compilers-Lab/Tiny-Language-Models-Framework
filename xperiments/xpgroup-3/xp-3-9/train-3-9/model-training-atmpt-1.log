|ITERS: 200410 / 200410 | COMP: 100.00% | RATE: 1.21 it./s | SPD: 0.8263 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 4.03 it./s | SPD: 0.2480 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:4.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 200410

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable
	--> loading vocab_size

Creating the model:

The model has 62M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-12_11-09 : iter     0 <=> epoch 0 | train loss 4.4562 | val loss 4.4558:
	--> training ...
	--> checkpointing ...

2024-11-12_12-22 : iter       5000 <=> epoch 0.042412856250428344 | train loss 1.0809428692 | val loss 1.0801299810:
	--> training ...
	--> checkpointing ...

2024-11-12_13-36 : iter      10000 <=> epoch 0.08482571250085669 | train loss 1.0554141998 | val loss 1.0553915501:
	--> training ...
	--> checkpointing ...

2024-11-12_14-49 : iter      15000 <=> epoch 0.12723856875128503 | train loss 1.0776667595 | val loss 1.0777808428:
	--> training ...
	--> checkpointing ...

2024-11-12_16-01 : iter      20000 <=> epoch 0.16965142500171337 | train loss 1.0185791254 | val loss 1.0167878866:
	--> training ...
	--> checkpointing ...

2024-11-12_17-14 : iter      25000 <=> epoch 0.21206428125214172 | train loss 1.0107645988 | val loss 1.0108678341:
	--> training ...
	--> checkpointing ...

2024-11-12_18-27 : iter      30000 <=> epoch 0.25447713750257006 | train loss 1.0041815042 | val loss 1.0038769245:
	--> training ...
	--> checkpointing ...

2024-11-12_19-40 : iter      35000 <=> epoch 0.2968899937529984 | train loss 0.9980397820 | val loss 1.0000029802:
	--> training ...
	--> checkpointing ...

2024-11-12_20-53 : iter      40000 <=> epoch 0.33930285000342675 | train loss 0.9966440797 | val loss 0.9962171912:
	--> training ...
	--> checkpointing ...

2024-11-12_22-06 : iter      45000 <=> epoch 0.3817157062538551 | train loss 0.9934627414 | val loss 0.9936307669:
	--> training ...
	--> checkpointing ...

2024-11-12_23-18 : iter      50000 <=> epoch 0.42412856250428344 | train loss 0.9930258989 | val loss 0.9919796586:
	--> training ...
	--> checkpointing ...

2024-11-13_00-31 : iter      55000 <=> epoch 0.4665414187547118 | train loss 0.9913854003 | val loss 0.9908462763:
	--> training ...
	--> checkpointing ...

2024-11-13_01-44 : iter      60000 <=> epoch 0.5089542750051401 | train loss 0.9911066294 | val loss 0.9896487594:
	--> training ...
	--> checkpointing ...

2024-11-13_02-56 : iter      65000 <=> epoch 0.5513671312555685 | train loss 0.9896712899 | val loss 0.9895051718:
	--> training ...
	--> checkpointing ...

2024-11-13_04-09 : iter      70000 <=> epoch 0.5937799875059968 | train loss 0.9886838794 | val loss 0.9899509549:
	--> training ...
	--> checkpointing ...

2024-11-13_05-22 : iter      75000 <=> epoch 0.6361928437564252 | train loss 0.9875580668 | val loss 0.9883605242:
	--> training ...
	--> checkpointing ...

2024-11-13_06-35 : iter      80000 <=> epoch 0.6786057000068535 | train loss 0.9865198731 | val loss 0.9867262840:
	--> training ...
	--> checkpointing ...

2024-11-13_07-47 : iter      85000 <=> epoch 0.7210185562572818 | train loss 0.9880149961 | val loss 0.9890586734:
	--> training ...
	--> checkpointing ...

2024-11-13_09-00 : iter      90000 <=> epoch 0.7634314125077102 | train loss 0.9876448512 | val loss 0.9872369766:
	--> training ...
	--> checkpointing ...

2024-11-13_10-13 : iter      95000 <=> epoch 0.8058442687581385 | train loss 0.9877061844 | val loss 0.9869032502:
	--> training ...
	--> checkpointing ...

2024-11-13_11-26 : iter     100000 <=> epoch 0.8482571250085669 | train loss 0.9851624966 | val loss 0.9849075675:
	--> training ...
	--> checkpointing ...

2024-11-13_12-39 : iter     105000 <=> epoch 0.8906699812589952 | train loss 0.9866681695 | val loss 0.9851671457:
	--> training ...
	--> checkpointing ...

2024-11-13_13-51 : iter     110000 <=> epoch 0.9330828375094236 | train loss 0.9862125516 | val loss 0.9863309264:
	--> training ...
	--> checkpointing ...

2024-11-13_15-04 : iter     115000 <=> epoch 0.9754956937598519 | train loss 0.9844921231 | val loss 0.9850694537:
	--> training ...
	--> checkpointing ...

2024-11-13_16-17 : iter     120000 <=> epoch 1.0179085500102802 | train loss 0.9835470915 | val loss 0.9852633476:
	--> training ...
	--> checkpointing ...

2024-11-13_17-30 : iter     125000 <=> epoch 1.0603214062607085 | train loss 0.9834983945 | val loss 0.9834416509:
	--> training ...
	--> checkpointing ...

2024-11-13_18-43 : iter     130000 <=> epoch 1.102734262511137 | train loss 0.9836214185 | val loss 0.9842682481:
	--> training ...
	--> checkpointing ...

2024-11-13_19-55 : iter     135000 <=> epoch 1.1451471187615652 | train loss 0.9839194417 | val loss 0.9839098454:
	--> training ...
	--> checkpointing ...

2024-11-13_21-09 : iter     140000 <=> epoch 1.1875599750119936 | train loss 0.9837943316 | val loss 0.9824541211:
	--> training ...
	--> checkpointing ...

2024-11-13_22-21 : iter     145000 <=> epoch 1.2299728312624219 | train loss 0.9810946584 | val loss 0.9803643823:
	--> training ...
	--> checkpointing ...

2024-11-13_23-34 : iter     150000 <=> epoch 1.2723856875128503 | train loss 0.9840274453 | val loss 0.9833760262:
	--> training ...
	--> checkpointing ...

2024-11-14_00-47 : iter     155000 <=> epoch 1.3147985437632785 | train loss 0.9795531034 | val loss 0.9789084792:
	--> training ...
	--> checkpointing ...

2024-11-14_02-01 : iter     160000 <=> epoch 1.357211400013707 | train loss 0.9798334837 | val loss 0.9800812602:
	--> training ...
	--> checkpointing ...

2024-11-14_03-14 : iter     165000 <=> epoch 1.3996242562641352 | train loss 0.9786452055 | val loss 0.9791683555:
	--> training ...
	--> checkpointing ...

2024-11-14_04-27 : iter     170000 <=> epoch 1.4420371125145637 | train loss 0.9804217815 | val loss 0.9791669846:
	--> training ...
	--> checkpointing ...

2024-11-14_05-39 : iter     175000 <=> epoch 1.484449968764992 | train loss 0.9788324237 | val loss 0.9783558846:
	--> training ...
	--> checkpointing ...

2024-11-14_06-52 : iter     180000 <=> epoch 1.5268628250154204 | train loss 0.9782843590 | val loss 0.9799680114:
	--> training ...
	--> checkpointing ...

2024-11-14_08-05 : iter     185000 <=> epoch 1.5692756812658486 | train loss 0.9800370336 | val loss 0.9792320728:
	--> training ...
	--> checkpointing ...

2024-11-14_09-18 : iter     190000 <=> epoch 1.611688537516277 | train loss 0.9791815281 | val loss 0.9799667001:
	--> training ...
	--> checkpointing ...

2024-11-14_10-31 : iter     195000 <=> epoch 1.6541013937667053 | train loss 0.9794373512 | val loss 0.9786069989:
	--> training ...
	--> checkpointing ...

2024-11-14_11-44 : iter     200000 <=> epoch 1.6965142500171337 | train loss 0.9785112143 | val loss 0.9788730145:
	--> training ...
