|ITERS: 202968 / 202968 | COMP: 100.00% | RATE: 2.68 it./s | SPD: 0.3727 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 8.38 it./s | SPD: 0.1193 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:0.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 202968

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 30M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-03_21-31 : iter     0 <=> epoch 0 | train loss 4.3147 | val loss 4.3152:
	--> training ...
	--> checkpointing ...

2024-11-03_22-04 : iter       5000 <=> epoch 0.024634317598495353 | train loss 1.1585433483 | val loss 1.1596645117:
	--> training ...
	--> checkpointing ...

2024-11-03_22-37 : iter      10000 <=> epoch 0.04926863519699071 | train loss 0.6517273188 | val loss 0.6525146961:
	--> training ...
	--> checkpointing ...

2024-11-03_23-11 : iter      15000 <=> epoch 0.07390295279548606 | train loss 0.6396386623 | val loss 0.6409029365:
	--> training ...
	--> checkpointing ...

2024-11-03_23-44 : iter      20000 <=> epoch 0.09853727039398141 | train loss 0.6345175505 | val loss 0.6339493394:
	--> training ...
	--> checkpointing ...

2024-11-04_00-17 : iter      25000 <=> epoch 0.12317158799247677 | train loss 0.6272384524 | val loss 0.6273297668:
	--> training ...
	--> checkpointing ...

2024-11-04_00-50 : iter      30000 <=> epoch 0.14780590559097212 | train loss 0.6199881434 | val loss 0.6205305457:
	--> training ...
	--> checkpointing ...

2024-11-04_01-23 : iter      35000 <=> epoch 0.17244022318946747 | train loss 0.6168040633 | val loss 0.6175574660:
	--> training ...
	--> checkpointing ...

2024-11-04_01-56 : iter      40000 <=> epoch 0.19707454078796283 | train loss 0.6159058213 | val loss 0.6152667999:
	--> training ...
	--> checkpointing ...

2024-11-04_02-29 : iter      45000 <=> epoch 0.22170885838645818 | train loss 0.6134810448 | val loss 0.6132474542:
	--> training ...
	--> checkpointing ...

2024-11-04_03-03 : iter      50000 <=> epoch 0.24634317598495353 | train loss 0.6129780412 | val loss 0.6127742529:
	--> training ...
	--> checkpointing ...

2024-11-04_03-36 : iter      55000 <=> epoch 0.2709774935834489 | train loss 0.6117499471 | val loss 0.6116439104:
	--> training ...
	--> checkpointing ...

2024-11-04_04-09 : iter      60000 <=> epoch 0.29561181118194424 | train loss 0.6117204428 | val loss 0.6113309860:
	--> training ...
	--> checkpointing ...

2024-11-04_04-42 : iter      65000 <=> epoch 0.3202461287804396 | train loss 0.6098291874 | val loss 0.6103605032:
	--> training ...
	--> checkpointing ...

2024-11-04_05-15 : iter      70000 <=> epoch 0.34488044637893495 | train loss 0.6094033718 | val loss 0.6083982587:
	--> training ...
	--> checkpointing ...

2024-11-04_05-48 : iter      75000 <=> epoch 0.3695147639774303 | train loss 0.6083617210 | val loss 0.6085768342:
	--> training ...
	--> checkpointing ...

2024-11-04_06-21 : iter      80000 <=> epoch 0.39414908157592565 | train loss 0.6073181629 | val loss 0.6080152392:
	--> training ...
	--> checkpointing ...

2024-11-04_06-55 : iter      85000 <=> epoch 0.41878339917442103 | train loss 0.6071763039 | val loss 0.6087166071:
	--> training ...
	--> checkpointing ...

2024-11-04_07-30 : iter      90000 <=> epoch 0.44341771677291636 | train loss 0.6063625813 | val loss 0.6059780121:
	--> training ...
	--> checkpointing ...

2024-11-04_08-03 : iter      95000 <=> epoch 0.46805203437141174 | train loss 0.6061509848 | val loss 0.6060642004:
	--> training ...
	--> checkpointing ...

2024-11-04_08-37 : iter     100000 <=> epoch 0.49268635196990707 | train loss 0.6060056686 | val loss 0.6045885682:
	--> training ...
	--> checkpointing ...

2024-11-04_09-10 : iter     105000 <=> epoch 0.5173206695684024 | train loss 0.6046887040 | val loss 0.6057553887:
	--> training ...
	--> checkpointing ...

2024-11-04_09-43 : iter     110000 <=> epoch 0.5419549871668978 | train loss 0.6047725677 | val loss 0.6051459908:
	--> training ...
	--> checkpointing ...

2024-11-04_10-16 : iter     115000 <=> epoch 0.5665893047653932 | train loss 0.6046866179 | val loss 0.6035770178:
	--> training ...
	--> checkpointing ...

2024-11-04_10-49 : iter     120000 <=> epoch 0.5912236223638885 | train loss 0.6044380665 | val loss 0.6037259102:
	--> training ...
	--> checkpointing ...

2024-11-04_11-22 : iter     125000 <=> epoch 0.6158579399623838 | train loss 0.6027631164 | val loss 0.6033869982:
	--> training ...
	--> checkpointing ...

2024-11-04_11-55 : iter     130000 <=> epoch 0.6404922575608792 | train loss 0.6035461426 | val loss 0.6033751965:
	--> training ...
	--> checkpointing ...

2024-11-04_12-29 : iter     135000 <=> epoch 0.6651265751593746 | train loss 0.6027262807 | val loss 0.6033973098:
	--> training ...
	--> checkpointing ...

2024-11-04_13-02 : iter     140000 <=> epoch 0.6897608927578699 | train loss 0.6023555398 | val loss 0.6025338769:
	--> training ...
	--> checkpointing ...

2024-11-04_13-35 : iter     145000 <=> epoch 0.7143952103563652 | train loss 0.6000927687 | val loss 0.5993494391:
	--> training ...
	--> checkpointing ...

2024-11-04_14-08 : iter     150000 <=> epoch 0.7390295279548607 | train loss 0.5992920995 | val loss 0.5998193622:
	--> training ...
	--> checkpointing ...

2024-11-04_14-41 : iter     155000 <=> epoch 0.763663845553356 | train loss 0.5991098285 | val loss 0.5987696648:
	--> training ...
	--> checkpointing ...

2024-11-04_15-14 : iter     160000 <=> epoch 0.7882981631518513 | train loss 0.6000386477 | val loss 0.5996069312:
	--> training ...
	--> checkpointing ...

2024-11-04_15-47 : iter     165000 <=> epoch 0.8129324807503467 | train loss 0.5998801589 | val loss 0.5989014506:
	--> training ...
	--> checkpointing ...

2024-11-04_16-20 : iter     170000 <=> epoch 0.8375667983488421 | train loss 0.5989528298 | val loss 0.5987290740:
	--> training ...
	--> checkpointing ...

2024-11-04_16-53 : iter     175000 <=> epoch 0.8622011159473374 | train loss 0.5992602110 | val loss 0.5990600586:
	--> training ...
	--> checkpointing ...

2024-11-04_17-26 : iter     180000 <=> epoch 0.8868354335458327 | train loss 0.5991858840 | val loss 0.5989730358:
	--> training ...
	--> checkpointing ...

2024-11-04_18-00 : iter     185000 <=> epoch 0.9114697511443282 | train loss 0.5986902118 | val loss 0.5984228253:
	--> training ...
	--> checkpointing ...

2024-11-04_18-33 : iter     190000 <=> epoch 0.9361040687428235 | train loss 0.5988194346 | val loss 0.5988889337:
	--> training ...
	--> checkpointing ...

2024-11-04_19-06 : iter     195000 <=> epoch 0.9607383863413188 | train loss 0.5997282863 | val loss 0.5994575620:
	--> training ...
	--> checkpointing ...

2024-11-04_19-39 : iter     200000 <=> epoch 0.9853727039398141 | train loss 0.5987081528 | val loss 0.5999223590:
	--> training ...
