|ITERS: 317227 / 461264 | COMP: 68.77% | RATE: 1.21 it./s | SPD: 0.8281 s/it.| ERT: (1, 9, 8, 3)                                                                                                        
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 4.02 it./s | SPD: 0.2489 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:3.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 2)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 461264

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable
	--> loading vocab_size

Creating the model:

The model has 62M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-06_11-17 : iter     0 <=> epoch 0 | train loss 4.4187 | val loss 4.4188:
	--> training ...
	--> checkpointing ...

2024-11-06_12-30 : iter       5000 <=> epoch 0.018427587608127932 | train loss 1.1097807884 | val loss 1.1121672392:
	--> training ...
	--> checkpointing ...

2024-11-06_13-44 : iter      10000 <=> epoch 0.036855175216255864 | train loss 1.0810514688 | val loss 1.0823391676:
	--> training ...
	--> checkpointing ...

2024-11-06_14-57 : iter      15000 <=> epoch 0.05528276282438379 | train loss 1.0626281500 | val loss 1.0628453493:
	--> training ...
	--> checkpointing ...

2024-11-06_16-10 : iter      20000 <=> epoch 0.07371035043251173 | train loss 1.0494912863 | val loss 1.0503540039:
	--> training ...
	--> checkpointing ...

2024-11-06_17-24 : iter      25000 <=> epoch 0.09213793804063965 | train loss 1.0413150787 | val loss 1.0419546366:
	--> training ...
	--> checkpointing ...

2024-11-06_18-37 : iter      30000 <=> epoch 0.11056552564876758 | train loss 1.0354145765 | val loss 1.0338900089:
	--> training ...
	--> checkpointing ...

2024-11-06_19-50 : iter      35000 <=> epoch 0.12899311325689553 | train loss 1.0296328068 | val loss 1.0307106972:
	--> training ...
	--> checkpointing ...

2024-11-06_21-03 : iter      40000 <=> epoch 0.14742070086502346 | train loss 1.0267827511 | val loss 1.0266666412:
	--> training ...
	--> checkpointing ...

2024-11-06_22-16 : iter      45000 <=> epoch 0.16584828847315136 | train loss 1.0236763954 | val loss 1.0240749121:
	--> training ...
	--> checkpointing ...

2024-11-06_23-29 : iter      50000 <=> epoch 0.1842758760812793 | train loss 1.0223625898 | val loss 1.0211110115:
	--> training ...
	--> checkpointing ...

2024-11-07_00-42 : iter      55000 <=> epoch 0.20270346368940723 | train loss 1.0195534229 | val loss 1.0209062099:
	--> training ...
	--> checkpointing ...

2024-11-07_01-55 : iter      60000 <=> epoch 0.22113105129753516 | train loss 1.0188666582 | val loss 1.0177743435:
	--> training ...
	--> checkpointing ...

2024-11-07_03-08 : iter      65000 <=> epoch 0.2395586389056631 | train loss 1.0177538395 | val loss 1.0162827969:
	--> training ...
	--> checkpointing ...

2024-11-07_04-21 : iter      70000 <=> epoch 0.25798622651379105 | train loss 1.0177090168 | val loss 1.0158605576:
	--> training ...
	--> checkpointing ...

2024-11-07_05-34 : iter      75000 <=> epoch 0.27641381412191895 | train loss 1.0135500431 | val loss 1.0142002106:
	--> training ...
	--> checkpointing ...

2024-11-07_06-47 : iter      80000 <=> epoch 0.2948414017300469 | train loss 1.0129004717 | val loss 1.0136967897:
	--> training ...
	--> checkpointing ...

2024-11-07_08-00 : iter      85000 <=> epoch 0.3132689893381748 | train loss 1.0117130280 | val loss 1.0127426386:
	--> training ...
	--> checkpointing ...

2024-11-07_09-13 : iter      90000 <=> epoch 0.3316965769463027 | train loss 1.0113992691 | val loss 1.0115346909:
	--> training ...
	--> checkpointing ...

2024-11-07_10-26 : iter      95000 <=> epoch 0.3501241645544307 | train loss 1.0106093884 | val loss 1.0106569529:
	--> training ...
	--> checkpointing ...

2024-11-07_11-39 : iter     100000 <=> epoch 0.3685517521625586 | train loss 1.0100004673 | val loss 1.0098679066:
	--> training ...
	--> checkpointing ...

2024-11-07_12-52 : iter     105000 <=> epoch 0.38697933977068655 | train loss 1.0089323521 | val loss 1.0096278191:
	--> training ...
	--> checkpointing ...

2024-11-07_14-05 : iter     110000 <=> epoch 0.40540692737881445 | train loss 1.0102350712 | val loss 1.0112204552:
	--> training ...
	--> checkpointing ...

2024-11-07_15-19 : iter     115000 <=> epoch 0.4238345149869424 | train loss 1.0105308294 | val loss 1.0093594790:
	--> training ...
	--> checkpointing ...

2024-11-07_16-32 : iter     120000 <=> epoch 0.4422621025950703 | train loss 1.0098146200 | val loss 1.0095900297:
	--> training ...
	--> checkpointing ...

2024-11-07_17-45 : iter     125000 <=> epoch 0.4606896902031983 | train loss 1.0075342655 | val loss 1.0065259933:
	--> training ...
	--> checkpointing ...

2024-11-07_18-58 : iter     130000 <=> epoch 0.4791172778113262 | train loss 1.0072072744 | val loss 1.0078125000:
	--> training ...
	--> checkpointing ...

2024-11-07_20-11 : iter     135000 <=> epoch 0.49754486541945414 | train loss 1.0077458620 | val loss 1.0083308220:
	--> training ...
	--> checkpointing ...

2024-11-07_21-24 : iter     140000 <=> epoch 0.5159724530275821 | train loss 1.0081537962 | val loss 1.0084021091:
	--> training ...
	--> checkpointing ...

2024-11-07_22-37 : iter     145000 <=> epoch 0.53440004063571 | train loss 1.0061049461 | val loss 1.0066852570:
	--> training ...
	--> checkpointing ...

2024-11-07_23-50 : iter     150000 <=> epoch 0.5528276282438379 | train loss 1.0070861578 | val loss 1.0068249702:
	--> training ...
	--> checkpointing ...

2024-11-08_01-03 : iter     155000 <=> epoch 0.5712552158519658 | train loss 1.0054968596 | val loss 1.0063601732:
	--> training ...
	--> checkpointing ...

2024-11-08_02-16 : iter     160000 <=> epoch 0.5896828034600938 | train loss 1.0071150064 | val loss 1.0062828064:
	--> training ...
	--> checkpointing ...

2024-11-08_03-30 : iter     165000 <=> epoch 0.6081103910682217 | train loss 1.0056154728 | val loss 1.0066217184:
	--> training ...
	--> checkpointing ...

2024-11-08_04-42 : iter     170000 <=> epoch 0.6265379786763496 | train loss 1.0067294836 | val loss 1.0058515072:
	--> training ...
	--> checkpointing ...

2024-11-08_05-56 : iter     175000 <=> epoch 0.6449655662844775 | train loss 1.0062685013 | val loss 1.0046749115:
	--> training ...
	--> checkpointing ...

2024-11-08_07-09 : iter     180000 <=> epoch 0.6633931538926054 | train loss 1.0055859089 | val loss 1.0047990084:
	--> training ...
	--> checkpointing ...

2024-11-08_08-22 : iter     185000 <=> epoch 0.6818207415007335 | train loss 1.2241783142 | val loss 1.2238783836:
	--> training ...
	--> checkpointing ...

2024-11-08_09-35 : iter     190000 <=> epoch 0.7002483291088614 | train loss 1.0034506321 | val loss 1.0040271282:
	--> training ...
	--> checkpointing ...

2024-11-08_10-48 : iter     195000 <=> epoch 0.7186759167169893 | train loss 1.0051214695 | val loss 1.0050796270:
	--> training ...
	--> checkpointing ...

2024-11-08_12-01 : iter     200000 <=> epoch 0.7371035043251172 | train loss 1.0057567358 | val loss 1.0048266649:
	--> training ...
	--> checkpointing ...

2024-11-08_13-14 : iter     205000 <=> epoch 0.7555310919332452 | train loss 1.0064306259 | val loss 1.0059134960:
	--> training ...
	--> checkpointing ...

2024-11-08_14-28 : iter     210000 <=> epoch 0.7739586795413731 | train loss 1.0045578480 | val loss 1.0066171885:
	--> training ...
	--> checkpointing ...

2024-11-08_15-41 : iter     215000 <=> epoch 0.792386267149501 | train loss 1.0041775703 | val loss 1.0044562817:
	--> training ...
	--> checkpointing ...

2024-11-08_16-54 : iter     220000 <=> epoch 0.8108138547576289 | train loss 1.0062787533 | val loss 1.0052883625:
	--> training ...
	--> checkpointing ...

2024-11-08_18-07 : iter     225000 <=> epoch 0.8292414423657569 | train loss 1.0054607391 | val loss 1.0051532984:
	--> training ...
	--> checkpointing ...

2024-11-08_19-20 : iter     230000 <=> epoch 0.8476690299738848 | train loss 1.0052326918 | val loss 1.0044609308:
	--> training ...
	--> checkpointing ...

2024-11-08_20-33 : iter     235000 <=> epoch 0.8660966175820127 | train loss 1.0039838552 | val loss 1.0052996874:
	--> training ...
	--> checkpointing ...

2024-11-08_21-46 : iter     240000 <=> epoch 0.8845242051901406 | train loss 1.0044085979 | val loss 1.0050868988:
	--> training ...
	--> checkpointing ...

2024-11-08_22-59 : iter     245000 <=> epoch 0.9029517927982686 | train loss 1.0051169395 | val loss 1.0035959482:
	--> training ...
	--> checkpointing ...

2024-11-09_00-12 : iter     250000 <=> epoch 0.9213793804063966 | train loss 1.0046602488 | val loss 1.0030738115:
	--> training ...
	--> checkpointing ...

2024-11-09_01-25 : iter     255000 <=> epoch 0.9398069680145245 | train loss 1.0025832653 | val loss 1.0037455559:
	--> training ...
	--> checkpointing ...

2024-11-09_02-38 : iter     260000 <=> epoch 0.9582345556226524 | train loss 1.0034186840 | val loss 1.0035182238:
	--> training ...
	--> checkpointing ...

2024-11-09_03-51 : iter     265000 <=> epoch 0.9766621432307803 | train loss 1.0045635700 | val loss 1.0046434402:
	--> training ...
	--> checkpointing ...

2024-11-09_05-04 : iter     270000 <=> epoch 0.9950897308389083 | train loss 1.0035244226 | val loss 1.0038931370:
	--> training ...
	--> checkpointing ...

2024-11-09_06-17 : iter     275000 <=> epoch 1.013517318447036 | train loss 1.0044445992 | val loss 1.0027949810:
	--> training ...
	--> checkpointing ...

2024-11-09_07-30 : iter     280000 <=> epoch 1.0319449060551642 | train loss 1.0038280487 | val loss 1.0036233664:
	--> training ...
	--> checkpointing ...

2024-11-09_08-43 : iter     285000 <=> epoch 1.050372493663292 | train loss 1.0037090778 | val loss 1.0037636757:
	--> training ...
	--> checkpointing ...

2024-11-09_09-56 : iter     290000 <=> epoch 1.06880008127142 | train loss 1.0043016672 | val loss 1.0055336952:
	--> training ...
	--> checkpointing ...

2024-11-09_11-09 : iter     295000 <=> epoch 1.087227668879548 | train loss 1.0030697584 | val loss 1.0036286116:
	--> training ...
	--> checkpointing ...

2024-11-09_12-22 : iter     300000 <=> epoch 1.1056552564876758 | train loss 1.0027271509 | val loss 1.0040001869:
	--> training ...
	--> checkpointing ...

2024-11-09_13-35 : iter     305000 <=> epoch 1.1240828440958037 | train loss 1.0047793388 | val loss 1.0037183762:
	--> training ...
	--> checkpointing ...

2024-11-09_14-48 : iter     310000 <=> epoch 1.1425104317039316 | train loss 1.0019787550 | val loss 1.0022619963:
	--> training ...
	--> checkpointing ...

2024-11-09_16-01 : iter     315000 <=> epoch 1.1609380193120595 | train loss 1.0025132895 | val loss 1.0031564236:
	--> training ...
