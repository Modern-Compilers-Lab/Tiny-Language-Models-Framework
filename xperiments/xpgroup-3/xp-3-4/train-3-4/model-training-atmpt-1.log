|ITERS: 400858 / 400858 | COMP: 100.00% | RATE: 1.77 it./s | SPD: 0.5641 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 5.66 it./s | SPD: 0.1767 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:5.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 2)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 400858

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable
	--> loading vocab_size

Creating the model:

The model has 61M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-06_12-22 : iter     0 <=> epoch 0 | train loss 4.4067 | val loss 4.4070:
	--> training ...
	--> checkpointing ...

2024-11-06_13-12 : iter       5000 <=> epoch 0.021204472154669998 | train loss 1.0722820759 | val loss 1.0709338188:
	--> training ...
	--> checkpointing ...

2024-11-06_14-02 : iter      10000 <=> epoch 0.042408944309339995 | train loss 1.0537722111 | val loss 1.0526436567:
	--> training ...
	--> checkpointing ...

2024-11-06_14-51 : iter      15000 <=> epoch 0.06361341646400999 | train loss 1.0275874138 | val loss 1.0284742117:
	--> training ...
	--> checkpointing ...

2024-11-06_15-41 : iter      20000 <=> epoch 0.08481788861867999 | train loss 1.0144699812 | val loss 1.0144056082:
	--> training ...
	--> checkpointing ...

2024-11-06_16-31 : iter      25000 <=> epoch 0.10602236077334998 | train loss 1.0121177435 | val loss 1.0123398304:
	--> training ...
	--> checkpointing ...

2024-11-06_17-21 : iter      30000 <=> epoch 0.12722683292801998 | train loss 1.0037372112 | val loss 1.0036556721:
	--> training ...
	--> checkpointing ...

2024-11-06_18-11 : iter      35000 <=> epoch 0.14843130508268998 | train loss 1.0023801327 | val loss 1.0015256405:
	--> training ...
	--> checkpointing ...

2024-11-06_19-01 : iter      40000 <=> epoch 0.16963577723735998 | train loss 0.9972182512 | val loss 0.9975070953:
	--> training ...
	--> checkpointing ...

2024-11-06_19-50 : iter      45000 <=> epoch 0.19084024939202995 | train loss 0.9930607677 | val loss 0.9933974743:
	--> training ...
	--> checkpointing ...

2024-11-06_20-40 : iter      50000 <=> epoch 0.21204472154669995 | train loss 0.9913106561 | val loss 0.9904818535:
	--> training ...
	--> checkpointing ...

2024-11-06_21-30 : iter      55000 <=> epoch 0.23324919370136996 | train loss 0.9910746217 | val loss 0.9910033345:
	--> training ...
	--> checkpointing ...

2024-11-06_22-20 : iter      60000 <=> epoch 0.25445366585603996 | train loss 0.9895168543 | val loss 0.9887535572:
	--> training ...
	--> checkpointing ...

2024-11-06_23-10 : iter      65000 <=> epoch 0.27565813801070993 | train loss 0.9892807603 | val loss 0.9885771871:
	--> training ...
	--> checkpointing ...

2024-11-07_00-00 : iter      70000 <=> epoch 0.29686261016537996 | train loss 0.9894071221 | val loss 0.9895803928:
	--> training ...
	--> checkpointing ...

2024-11-07_00-50 : iter      75000 <=> epoch 0.31806708232004993 | train loss 0.9886047840 | val loss 0.9872861505:
	--> training ...
	--> checkpointing ...

2024-11-07_01-40 : iter      80000 <=> epoch 0.33927155447471996 | train loss 0.9876372814 | val loss 0.9868243933:
	--> training ...
	--> checkpointing ...

2024-11-07_02-30 : iter      85000 <=> epoch 0.36047602662938993 | train loss 0.9877514243 | val loss 0.9872015119:
	--> training ...
	--> checkpointing ...

2024-11-07_03-20 : iter      90000 <=> epoch 0.3816804987840599 | train loss 0.9863094091 | val loss 0.9859957099:
	--> training ...
	--> checkpointing ...

2024-11-07_04-09 : iter      95000 <=> epoch 0.40288497093872994 | train loss 0.9857088327 | val loss 0.9861430526:
	--> training ...
	--> checkpointing ...

2024-11-07_04-59 : iter     100000 <=> epoch 0.4240894430933999 | train loss 0.9841995835 | val loss 0.9857332706:
	--> training ...
	--> checkpointing ...

2024-11-07_05-49 : iter     105000 <=> epoch 0.44529391524806994 | train loss 0.9847761393 | val loss 0.9850105643:
	--> training ...
	--> checkpointing ...

2024-11-07_06-39 : iter     110000 <=> epoch 0.4664983874027399 | train loss 0.9859349728 | val loss 0.9850928783:
	--> training ...
	--> checkpointing ...

2024-11-07_07-29 : iter     115000 <=> epoch 0.48770285955740994 | train loss 0.9842998385 | val loss 0.9846911430:
	--> training ...
	--> checkpointing ...

2024-11-07_08-19 : iter     120000 <=> epoch 0.5089073317120799 | train loss 0.9845691323 | val loss 0.9842359424:
	--> training ...
	--> checkpointing ...

2024-11-07_09-09 : iter     125000 <=> epoch 0.5301118038667499 | train loss 0.9846234918 | val loss 0.9841333628:
	--> training ...
	--> checkpointing ...

2024-11-07_09-59 : iter     130000 <=> epoch 0.5513162760214199 | train loss 0.9846801162 | val loss 0.9853466153:
	--> training ...
	--> checkpointing ...

2024-11-07_10-49 : iter     135000 <=> epoch 0.5725207481760899 | train loss 0.9833337069 | val loss 0.9846615195:
	--> training ...
	--> checkpointing ...

2024-11-07_11-38 : iter     140000 <=> epoch 0.5937252203307599 | train loss 0.9840945601 | val loss 0.9829360247:
	--> training ...
	--> checkpointing ...

2024-11-07_12-28 : iter     145000 <=> epoch 0.61492969248543 | train loss 0.9828376174 | val loss 0.9833284616:
	--> training ...
	--> checkpointing ...

2024-11-07_13-18 : iter     150000 <=> epoch 0.6361341646400999 | train loss 0.9831687808 | val loss 0.9852486253:
	--> training ...
	--> checkpointing ...

2024-11-07_14-08 : iter     155000 <=> epoch 0.6573386367947699 | train loss 0.9823489785 | val loss 0.9839197397:
	--> training ...
	--> checkpointing ...

2024-11-07_14-58 : iter     160000 <=> epoch 0.6785431089494399 | train loss 0.9831214547 | val loss 0.9828662276:
	--> training ...
	--> checkpointing ...

2024-11-07_15-48 : iter     165000 <=> epoch 0.6997475811041098 | train loss 0.9835071564 | val loss 0.9833961129:
	--> training ...
	--> checkpointing ...

2024-11-07_16-38 : iter     170000 <=> epoch 0.7209520532587799 | train loss 0.9838465452 | val loss 0.9829840660:
	--> training ...
	--> checkpointing ...

2024-11-07_17-28 : iter     175000 <=> epoch 0.7421565254134499 | train loss 0.9828693867 | val loss 0.9840656519:
	--> training ...
	--> checkpointing ...

2024-11-07_18-18 : iter     180000 <=> epoch 0.7633609975681198 | train loss 0.9837108850 | val loss 0.9836835861:
	--> training ...
	--> checkpointing ...

2024-11-07_19-08 : iter     185000 <=> epoch 0.7845654697227898 | train loss 0.9836084843 | val loss 0.9832449555:
	--> training ...
	--> checkpointing ...

2024-11-07_19-58 : iter     190000 <=> epoch 0.8057699418774599 | train loss 0.9845330119 | val loss 0.9819810987:
	--> training ...
	--> checkpointing ...

2024-11-07_20-48 : iter     195000 <=> epoch 0.8269744140321299 | train loss 0.9828374386 | val loss 0.9832005501:
	--> training ...
	--> checkpointing ...

2024-11-07_21-37 : iter     200000 <=> epoch 0.8481788861867998 | train loss 0.9822109938 | val loss 0.9824258089:
	--> training ...
	--> checkpointing ...

2024-11-07_22-27 : iter     205000 <=> epoch 0.8693833583414698 | train loss 0.9828532934 | val loss 0.9822118878:
	--> training ...
	--> checkpointing ...

2024-11-07_23-17 : iter     210000 <=> epoch 0.8905878304961399 | train loss 0.9830297232 | val loss 0.9827280641:
	--> training ...
	--> checkpointing ...

2024-11-08_00-07 : iter     215000 <=> epoch 0.9117923026508098 | train loss 0.9796913266 | val loss 0.9824504256:
	--> training ...
	--> checkpointing ...

2024-11-08_00-57 : iter     220000 <=> epoch 0.9329967748054798 | train loss 0.9826409817 | val loss 0.9823912978:
	--> training ...
	--> checkpointing ...

2024-11-08_01-47 : iter     225000 <=> epoch 0.9542012469601499 | train loss 0.9826204777 | val loss 0.9816937447:
	--> training ...
	--> checkpointing ...

2024-11-08_02-37 : iter     230000 <=> epoch 0.9754057191148199 | train loss 0.9837381840 | val loss 0.9826288819:
	--> training ...
	--> checkpointing ...

2024-11-08_03-27 : iter     235000 <=> epoch 0.9966101912694898 | train loss 0.9829962850 | val loss 0.9824060202:
	--> training ...
	--> checkpointing ...

2024-11-08_04-17 : iter     240000 <=> epoch 1.0178146634241598 | train loss 0.9828386903 | val loss 0.9828469157:
	--> training ...
	--> checkpointing ...

2024-11-08_05-07 : iter     245000 <=> epoch 1.0390191355788299 | train loss 0.9807095528 | val loss 0.9825880527:
	--> training ...
	--> checkpointing ...

2024-11-08_05-57 : iter     250000 <=> epoch 1.0602236077334999 | train loss 0.9823163748 | val loss 0.9817510843:
	--> training ...
	--> checkpointing ...

2024-11-08_06-46 : iter     255000 <=> epoch 1.08142807988817 | train loss 0.9820277095 | val loss 0.9814024568:
	--> training ...
	--> checkpointing ...

2024-11-08_07-36 : iter     260000 <=> epoch 1.1026325520428397 | train loss 0.9813617468 | val loss 0.9821988344:
	--> training ...
	--> checkpointing ...

2024-11-08_08-26 : iter     265000 <=> epoch 1.1238370241975097 | train loss 0.9817992449 | val loss 0.9817611575:
	--> training ...
	--> checkpointing ...

2024-11-08_09-16 : iter     270000 <=> epoch 1.1450414963521798 | train loss 0.9828310013 | val loss 0.9828388095:
	--> training ...
	--> checkpointing ...

2024-11-08_10-06 : iter     275000 <=> epoch 1.1662459685068498 | train loss 0.9820464849 | val loss 0.9817626476:
	--> training ...
	--> checkpointing ...

2024-11-08_10-56 : iter     280000 <=> epoch 1.1874504406615198 | train loss 0.9812391996 | val loss 0.9814879298:
	--> training ...
	--> checkpointing ...

2024-11-08_11-46 : iter     285000 <=> epoch 1.2086549128161899 | train loss 0.9800502062 | val loss 0.9787505269:
	--> training ...
	--> checkpointing ...

2024-11-08_12-36 : iter     290000 <=> epoch 1.22985938497086 | train loss 0.9792601466 | val loss 0.9787063003:
	--> training ...
	--> checkpointing ...

2024-11-08_13-26 : iter     295000 <=> epoch 1.2510638571255297 | train loss 0.9774887562 | val loss 0.9783585668:
	--> training ...
	--> checkpointing ...

2024-11-08_14-16 : iter     300000 <=> epoch 1.2722683292801997 | train loss 0.9782077074 | val loss 0.9781391621:
	--> training ...
	--> checkpointing ...

2024-11-08_15-06 : iter     305000 <=> epoch 1.2934728014348698 | train loss 0.9791792035 | val loss 0.9777699113:
	--> training ...
	--> checkpointing ...

2024-11-08_15-56 : iter     310000 <=> epoch 1.3146772735895398 | train loss 0.9778586626 | val loss 0.9783729315:
	--> training ...
	--> checkpointing ...

2024-11-08_16-45 : iter     315000 <=> epoch 1.3358817457442098 | train loss 0.9776862264 | val loss 0.9781717062:
	--> training ...
	--> checkpointing ...

2024-11-08_17-35 : iter     320000 <=> epoch 1.3570862178988798 | train loss 0.9783812165 | val loss 0.9774543047:
	--> training ...
	--> checkpointing ...

2024-11-08_18-25 : iter     325000 <=> epoch 1.3782906900535496 | train loss 0.9784193039 | val loss 0.9786059856:
	--> training ...
	--> checkpointing ...

2024-11-08_19-15 : iter     330000 <=> epoch 1.3994951622082197 | train loss 0.9775270224 | val loss 0.9779465795:
	--> training ...
	--> checkpointing ...

2024-11-08_20-05 : iter     335000 <=> epoch 1.4206996343628897 | train loss 0.9771490097 | val loss 0.9772512317:
	--> training ...
	--> checkpointing ...

2024-11-08_20-55 : iter     340000 <=> epoch 1.4419041065175597 | train loss 0.9773074985 | val loss 0.9772040248:
	--> training ...
	--> checkpointing ...

2024-11-08_21-45 : iter     345000 <=> epoch 1.4631085786722298 | train loss 0.9779347777 | val loss 0.9782949090:
	--> training ...
	--> checkpointing ...

2024-11-08_22-35 : iter     350000 <=> epoch 1.4843130508268998 | train loss 0.9775318503 | val loss 0.9773774743:
	--> training ...
	--> checkpointing ...

2024-11-08_23-25 : iter     355000 <=> epoch 1.5055175229815698 | train loss 0.9776254296 | val loss 0.9768137932:
	--> training ...
	--> checkpointing ...

2024-11-09_00-15 : iter     360000 <=> epoch 1.5267219951362396 | train loss 0.9762307405 | val loss 0.9774063230:
	--> training ...
	--> checkpointing ...

2024-11-09_01-05 : iter     365000 <=> epoch 1.5479264672909097 | train loss 0.9776256084 | val loss 0.9777517319:
	--> training ...
	--> checkpointing ...

2024-11-09_01-55 : iter     370000 <=> epoch 1.5691309394455797 | train loss 0.9778150916 | val loss 0.9773250222:
	--> training ...
	--> checkpointing ...

2024-11-09_02-45 : iter     375000 <=> epoch 1.5903354116002497 | train loss 0.9773004055 | val loss 0.9774492979:
	--> training ...
	--> checkpointing ...

2024-11-09_03-34 : iter     380000 <=> epoch 1.6115398837549197 | train loss 0.9781972170 | val loss 0.9776176810:
	--> training ...
	--> checkpointing ...

2024-11-09_04-24 : iter     385000 <=> epoch 1.6327443559095898 | train loss 0.9765610099 | val loss 0.9775184989:
	--> training ...
	--> checkpointing ...

2024-11-09_05-14 : iter     390000 <=> epoch 1.6539488280642598 | train loss 0.9782078862 | val loss 0.9788340926:
	--> training ...
	--> checkpointing ...

2024-11-09_06-04 : iter     395000 <=> epoch 1.6751533002189296 | train loss 0.9765900373 | val loss 0.9781731963:
	--> training ...
	--> checkpointing ...

2024-11-09_06-54 : iter     400000 <=> epoch 1.6963577723735996 | train loss 0.9779331088 | val loss 0.9783284068:
	--> training ...
