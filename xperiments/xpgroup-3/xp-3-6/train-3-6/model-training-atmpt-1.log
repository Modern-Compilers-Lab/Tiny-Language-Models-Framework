|ITERS: 250 / 690058 | COMP: 0.04% | RATE: 1.78 it./s | SPD: 0.5627 s/it.| ERT: (4, 11, 49, 18)                                                                                                         
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 5.73 it./s | SPD: 0.1744 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:6.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 3)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 690058

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 61M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-12_11-32 : iter     0 <=> epoch 0 | train loss 4.1442 | val loss 4.1445:
	--> training ...
