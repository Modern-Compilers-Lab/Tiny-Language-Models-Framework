|ITERS: 464320 / 464320 | COMP: 100.00% | RATE: 1.21 it./s | SPD: 0.8258 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 4.03 it./s | SPD: 0.2481 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:3.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 464320

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable
	--> loading vocab_size

Creating the model:

The model has 62M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-09_17-05 : iter     0 <=> epoch 0 | train loss 4.4455 | val loss 4.4458:
	--> training ...
	--> checkpointing ...

2024-11-09_18-18 : iter       5000 <=> epoch 0.018306317518957997 | train loss 1.2067189217 | val loss 1.2056943178:
	--> training ...
	--> checkpointing ...

2024-11-09_19-32 : iter      10000 <=> epoch 0.036612635037915994 | train loss 1.1800670624 | val loss 1.1790528297:
	--> training ...
	--> checkpointing ...

2024-11-09_20-45 : iter      15000 <=> epoch 0.05491895255687399 | train loss 1.1645987034 | val loss 1.1645131111:
	--> training ...
	--> checkpointing ...

2024-11-09_21-58 : iter      20000 <=> epoch 0.07322527007583199 | train loss 1.1513212919 | val loss 1.1531915665:
	--> training ...
	--> checkpointing ...

2024-11-09_23-12 : iter      25000 <=> epoch 0.09153158759478999 | train loss 1.1379407644 | val loss 1.1396080256:
	--> training ...
	--> checkpointing ...

2024-11-10_00-25 : iter      30000 <=> epoch 0.10983790511374798 | train loss 1.1279902458 | val loss 1.1282478571:
	--> training ...
	--> checkpointing ...

2024-11-10_01-38 : iter      35000 <=> epoch 0.12814422263270597 | train loss 1.1205790043 | val loss 1.1207556725:
	--> training ...
	--> checkpointing ...

2024-11-10_02-51 : iter      40000 <=> epoch 0.14645054015166398 | train loss 1.1178478003 | val loss 1.1174576283:
	--> training ...
	--> checkpointing ...

2024-11-10_04-04 : iter      45000 <=> epoch 0.16475685767062198 | train loss 1.1121473312 | val loss 1.1134012938:
	--> training ...
	--> checkpointing ...

2024-11-10_05-17 : iter      50000 <=> epoch 0.18306317518957999 | train loss 1.1091969013 | val loss 1.1092282534:
	--> training ...
	--> checkpointing ...

2024-11-10_06-29 : iter      55000 <=> epoch 0.201369492708538 | train loss 1.1068803072 | val loss 1.1066043377:
	--> training ...
	--> checkpointing ...

2024-11-10_07-42 : iter      60000 <=> epoch 0.21967581022749597 | train loss 1.1046223640 | val loss 1.1055608988:
	--> training ...
	--> checkpointing ...

2024-11-10_08-55 : iter      65000 <=> epoch 0.23798212774645397 | train loss 1.1032083035 | val loss 1.1033437252:
	--> training ...
	--> checkpointing ...

2024-11-10_10-08 : iter      70000 <=> epoch 0.25628844526541195 | train loss 1.1025729179 | val loss 1.1028654575:
	--> training ...
	--> checkpointing ...

2024-11-10_11-21 : iter      75000 <=> epoch 0.27459476278436995 | train loss 1.1025490761 | val loss 1.1022950411:
	--> training ...
	--> checkpointing ...

2024-11-10_12-34 : iter      80000 <=> epoch 0.29290108030332795 | train loss 1.0995045900 | val loss 1.1005314589:
	--> training ...
	--> checkpointing ...

2024-11-10_13-47 : iter      85000 <=> epoch 0.31120739782228596 | train loss 1.0995856524 | val loss 1.0994985104:
	--> training ...
	--> checkpointing ...

2024-11-10_15-00 : iter      90000 <=> epoch 0.32951371534124396 | train loss 1.1002146006 | val loss 1.0993064642:
	--> training ...
	--> checkpointing ...

2024-11-10_16-13 : iter      95000 <=> epoch 0.34782003286020197 | train loss 1.1016113758 | val loss 1.1003365517:
	--> training ...
	--> checkpointing ...

2024-11-10_17-26 : iter     100000 <=> epoch 0.36612635037915997 | train loss 1.0976493359 | val loss 1.0972282887:
	--> training ...
	--> checkpointing ...

2024-11-10_18-40 : iter     105000 <=> epoch 0.384432667898118 | train loss 1.0970612764 | val loss 1.0977433920:
	--> training ...
	--> checkpointing ...

2024-11-10_19-53 : iter     110000 <=> epoch 0.402738985417076 | train loss 1.0963879824 | val loss 1.0968173742:
	--> training ...
	--> checkpointing ...

2024-11-10_21-06 : iter     115000 <=> epoch 0.4210453029360339 | train loss 1.0957103968 | val loss 1.0984389782:
	--> training ...
	--> checkpointing ...

2024-11-10_22-19 : iter     120000 <=> epoch 0.43935162045499193 | train loss 1.0938636065 | val loss 1.0966227055:
	--> training ...
	--> checkpointing ...

2024-11-10_23-32 : iter     125000 <=> epoch 0.45765793797394994 | train loss 1.0959066153 | val loss 1.0943595171:
	--> training ...
	--> checkpointing ...

2024-11-11_00-45 : iter     130000 <=> epoch 0.47596425549290794 | train loss 1.0962196589 | val loss 1.0951904058:
	--> training ...
	--> checkpointing ...

2024-11-11_01-58 : iter     135000 <=> epoch 0.49427057301186594 | train loss 1.0966153145 | val loss 1.0963338614:
	--> training ...
	--> checkpointing ...

2024-11-11_03-11 : iter     140000 <=> epoch 0.5125768905308239 | train loss 1.0953363180 | val loss 1.0940297842:
	--> training ...
	--> checkpointing ...

2024-11-11_04-24 : iter     145000 <=> epoch 0.530883208049782 | train loss 1.0946143866 | val loss 1.0947092772:
	--> training ...
	--> checkpointing ...

2024-11-11_05-37 : iter     150000 <=> epoch 0.5491895255687399 | train loss 1.0937292576 | val loss 1.0937644243:
	--> training ...
	--> checkpointing ...

2024-11-11_06-50 : iter     155000 <=> epoch 0.567495843087698 | train loss 1.0943146944 | val loss 1.0932055712:
	--> training ...
	--> checkpointing ...

2024-11-11_08-03 : iter     160000 <=> epoch 0.5858021606066559 | train loss 1.0923130512 | val loss 1.0928891897:
	--> training ...
	--> checkpointing ...

2024-11-11_09-16 : iter     165000 <=> epoch 0.604108478125614 | train loss 1.0935928822 | val loss 1.0927289724:
	--> training ...
	--> checkpointing ...

2024-11-11_10-29 : iter     170000 <=> epoch 0.6224147956445719 | train loss 1.0933024883 | val loss 1.0939286947:
	--> training ...
	--> checkpointing ...

2024-11-11_11-42 : iter     175000 <=> epoch 0.64072111316353 | train loss 1.0924016237 | val loss 1.0923962593:
	--> training ...
	--> checkpointing ...

2024-11-11_12-55 : iter     180000 <=> epoch 0.6590274306824879 | train loss 1.0930604935 | val loss 1.0933684111:
	--> training ...
	--> checkpointing ...

2024-11-11_14-08 : iter     185000 <=> epoch 0.6773337482014459 | train loss 1.0917809010 | val loss 1.0929549932:
	--> training ...
	--> checkpointing ...

2024-11-11_15-21 : iter     190000 <=> epoch 0.6956400657204039 | train loss 1.0918325186 | val loss 1.0926842690:
	--> training ...
	--> checkpointing ...

2024-11-11_16-34 : iter     195000 <=> epoch 0.7139463832393619 | train loss 1.0931872129 | val loss 1.0918051004:
	--> training ...
	--> checkpointing ...

2024-11-11_17-47 : iter     200000 <=> epoch 0.7322527007583199 | train loss 1.0921405554 | val loss 1.0931228399:
	--> training ...
	--> checkpointing ...

2024-11-11_19-00 : iter     205000 <=> epoch 0.7505590182772779 | train loss 1.0933796167 | val loss 1.0928381681:
	--> training ...
	--> checkpointing ...

2024-11-11_20-13 : iter     210000 <=> epoch 0.768865335796236 | train loss 1.0910910368 | val loss 1.0918768644:
	--> training ...
	--> checkpointing ...

2024-11-11_21-26 : iter     215000 <=> epoch 0.7871716533151939 | train loss 1.0901187658 | val loss 1.0920045376:
	--> training ...
	--> checkpointing ...

2024-11-11_22-39 : iter     220000 <=> epoch 0.805477970834152 | train loss 1.0901128054 | val loss 1.0906879902:
	--> training ...
	--> checkpointing ...

2024-11-11_23-52 : iter     225000 <=> epoch 0.8237842883531099 | train loss 1.0887421370 | val loss 1.0902093649:
	--> training ...
	--> checkpointing ...

2024-11-12_01-05 : iter     230000 <=> epoch 0.8420906058720679 | train loss 1.0913445950 | val loss 1.0915864706:
	--> training ...
	--> checkpointing ...

2024-11-12_02-18 : iter     235000 <=> epoch 0.8603969233910259 | train loss 1.0914278030 | val loss 1.0916919708:
	--> training ...
	--> checkpointing ...

2024-11-12_03-31 : iter     240000 <=> epoch 0.8787032409099839 | train loss 1.0896790028 | val loss 1.0908671618:
	--> training ...
	--> checkpointing ...

2024-11-12_04-44 : iter     245000 <=> epoch 0.8970095584289419 | train loss 1.0917011499 | val loss 1.0914036036:
	--> training ...
	--> checkpointing ...

2024-11-12_05-57 : iter     250000 <=> epoch 0.9153158759478999 | train loss 1.0924763680 | val loss 1.0914905071:
	--> training ...
	--> checkpointing ...

2024-11-12_07-10 : iter     255000 <=> epoch 0.9336221934668579 | train loss 1.0902513266 | val loss 1.0911405087:
	--> training ...
	--> checkpointing ...

2024-11-12_08-23 : iter     260000 <=> epoch 0.9519285109858159 | train loss 1.0913507938 | val loss 1.0924171209:
	--> training ...
	--> checkpointing ...

2024-11-12_09-36 : iter     265000 <=> epoch 0.9702348285047739 | train loss 1.0893887281 | val loss 1.0913667679:
	--> training ...
	--> checkpointing ...

2024-11-12_10-49 : iter     270000 <=> epoch 0.9885411460237319 | train loss 1.0905121565 | val loss 1.0896140337:
	--> training ...
	--> checkpointing ...

2024-11-12_12-02 : iter     275000 <=> epoch 1.0068474635426898 | train loss 1.0903731585 | val loss 1.0898898840:
	--> training ...
	--> checkpointing ...

2024-11-12_13-15 : iter     280000 <=> epoch 1.0251537810616478 | train loss 1.0899754763 | val loss 1.0914939642:
	--> training ...
	--> checkpointing ...

2024-11-12_14-28 : iter     285000 <=> epoch 1.043460098580606 | train loss 1.0888669491 | val loss 1.0898073912:
	--> training ...
	--> checkpointing ...

2024-11-12_15-41 : iter     290000 <=> epoch 1.061766416099564 | train loss 1.0897958279 | val loss 1.0899913311:
	--> training ...
	--> checkpointing ...

2024-11-12_16-54 : iter     295000 <=> epoch 1.0800727336185219 | train loss 1.0896010399 | val loss 1.0906693935:
	--> training ...
	--> checkpointing ...

2024-11-12_18-07 : iter     300000 <=> epoch 1.0983790511374798 | train loss 1.0910061598 | val loss 1.0897121429:
	--> training ...
	--> checkpointing ...

2024-11-12_19-20 : iter     305000 <=> epoch 1.116685368656438 | train loss 1.1016101837 | val loss 1.1021196842:
	--> training ...
	--> checkpointing ...

2024-11-12_20-33 : iter     310000 <=> epoch 1.134991686175396 | train loss 1.0905044079 | val loss 1.0905975103:
	--> training ...
	--> checkpointing ...

2024-11-12_21-46 : iter     315000 <=> epoch 1.1532980036943539 | train loss 1.1065981388 | val loss 1.1080312729:
	--> training ...
	--> checkpointing ...

2024-11-12_22-59 : iter     320000 <=> epoch 1.1716043212133118 | train loss 1.0926643610 | val loss 1.0938808918:
	--> training ...
	--> checkpointing ...

2024-11-13_00-12 : iter     325000 <=> epoch 1.1899106387322698 | train loss 1.0907217264 | val loss 1.0920600891:
	--> training ...
	--> checkpointing ...

2024-11-13_01-25 : iter     330000 <=> epoch 1.208216956251228 | train loss 1.0860987902 | val loss 1.0854628086:
	--> training ...
	--> checkpointing ...

2024-11-13_02-38 : iter     335000 <=> epoch 1.2265232737701859 | train loss 1.0869125128 | val loss 1.0860623121:
	--> training ...
	--> checkpointing ...

2024-11-13_03-51 : iter     340000 <=> epoch 1.2448295912891438 | train loss 1.0843491554 | val loss 1.0864145756:
	--> training ...
	--> checkpointing ...

2024-11-13_05-04 : iter     345000 <=> epoch 1.2631359088081018 | train loss 1.0866467953 | val loss 1.0858918428:
	--> training ...
	--> checkpointing ...

2024-11-13_06-17 : iter     350000 <=> epoch 1.28144222632706 | train loss 1.0853472948 | val loss 1.0838123560:
	--> training ...
	--> checkpointing ...

2024-11-13_07-30 : iter     355000 <=> epoch 1.299748543846018 | train loss 1.0838561058 | val loss 1.0850352049:
	--> training ...
	--> checkpointing ...

2024-11-13_08-43 : iter     360000 <=> epoch 1.3180548613649758 | train loss 1.0842231512 | val loss 1.0846946239:
	--> training ...
	--> checkpointing ...

2024-11-13_09-56 : iter     365000 <=> epoch 1.3363611788839338 | train loss 1.0842800140 | val loss 1.0840721130:
	--> training ...
	--> checkpointing ...

2024-11-13_11-09 : iter     370000 <=> epoch 1.3546674964028917 | train loss 1.0840188265 | val loss 1.0847305059:
	--> training ...
	--> checkpointing ...

2024-11-13_12-22 : iter     375000 <=> epoch 1.37297381392185 | train loss 1.0851769447 | val loss 1.0835502148:
	--> training ...
	--> checkpointing ...

2024-11-13_13-35 : iter     380000 <=> epoch 1.3912801314408079 | train loss 1.0844246149 | val loss 1.0837736130:
	--> training ...
	--> checkpointing ...

2024-11-13_14-48 : iter     385000 <=> epoch 1.4095864489597658 | train loss 1.0837619305 | val loss 1.0843890905:
	--> training ...
	--> checkpointing ...

2024-11-13_16-01 : iter     390000 <=> epoch 1.4278927664787238 | train loss 1.0845985413 | val loss 1.0847741365:
	--> training ...
	--> checkpointing ...

2024-11-13_17-14 : iter     395000 <=> epoch 1.446199083997682 | train loss 1.0831227303 | val loss 1.0834734440:
	--> training ...
	--> checkpointing ...

2024-11-13_18-27 : iter     400000 <=> epoch 1.4645054015166399 | train loss 1.0842950344 | val loss 1.0832973719:
	--> training ...
	--> checkpointing ...

2024-11-13_19-40 : iter     405000 <=> epoch 1.4828117190355978 | train loss 1.0847434998 | val loss 1.0833332539:
	--> training ...
	--> checkpointing ...

2024-11-13_20-53 : iter     410000 <=> epoch 1.5011180365545558 | train loss 1.0851486921 | val loss 1.0844459534:
	--> training ...
	--> checkpointing ...

2024-11-13_22-06 : iter     415000 <=> epoch 1.5194243540735137 | train loss 1.0849102736 | val loss 1.0845991373:
	--> training ...
	--> checkpointing ...

2024-11-13_23-19 : iter     420000 <=> epoch 1.537730671592472 | train loss 1.0844514370 | val loss 1.0843291283:
	--> training ...
	--> checkpointing ...

2024-11-14_00-31 : iter     425000 <=> epoch 1.5560369891114298 | train loss 1.0832706690 | val loss 1.0841050148:
	--> training ...
	--> checkpointing ...

2024-11-14_01-44 : iter     430000 <=> epoch 1.5743433066303878 | train loss 1.0842354298 | val loss 1.0847566128:
	--> training ...
	--> checkpointing ...

2024-11-14_02-57 : iter     435000 <=> epoch 1.5926496241493457 | train loss 1.0843142271 | val loss 1.0841681957:
	--> training ...
	--> checkpointing ...

2024-11-14_04-11 : iter     440000 <=> epoch 1.610955941668304 | train loss 1.0836142302 | val loss 1.0835858583:
	--> training ...
	--> checkpointing ...

2024-11-14_05-24 : iter     445000 <=> epoch 1.6292622591872619 | train loss 1.0838146210 | val loss 1.0851799250:
	--> training ...
	--> checkpointing ...

2024-11-14_06-36 : iter     450000 <=> epoch 1.6475685767062198 | train loss 1.0828682184 | val loss 1.0830401182:
	--> training ...
	--> checkpointing ...

2024-11-14_07-50 : iter     455000 <=> epoch 1.6658748942251778 | train loss 1.0829631090 | val loss 1.0832407475:
	--> training ...
	--> checkpointing ...

2024-11-14_09-03 : iter     460000 <=> epoch 1.6841812117441357 | train loss 1.0835368633 | val loss 1.0824499130:
	--> training ...
