{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6424d0cc30>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockModel(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.layer = torch.nn.Linear(3, 1, bias = False)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0043,  0.3097, -0.4752]], requires_grad=True)\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mm = MockModel()\n",
    "print(mm.layer.weight)\n",
    "print(mm.layer.weight.requires_grad)\n",
    "print(mm.layer.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.requires_grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockModel(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.layer = torch.nn.Linear(3, 2, bias = False)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm = MockModel()\n",
    "mm.layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]], requires_grad=True)\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "layer = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float, requires_grad=True)\n",
    "print(layer)\n",
    "print(layer.requires_grad)\n",
    "print(layer.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = mm(x)\n",
    "y = torch.matmul(x, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22., 28.],\n",
      "        [49., 64.]], grad_fn=<MmBackward0>)\n",
      "True\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2326648/61410144.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(y.grad)\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(y.requires_grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.functional.cross_entropy(y, torch.tensor([0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = torch.tensor([0, 1])\n",
    "targets[0].view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0012, grad_fn=<NllLossBackward0>)\n",
      "True\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2326648/2363823690.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(loss.grad)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "print(loss.requires_grad)\n",
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]], requires_grad=True)\n",
      "True\n",
      "tensor([[-0.4988,  0.4988],\n",
      "        [-0.9975,  0.9975],\n",
      "        [-1.4963,  1.4963]])\n"
     ]
    }
   ],
   "source": [
    "print(layer)\n",
    "print(layer.requires_grad)\n",
    "print(layer.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4988,  0.4988],\n",
      "        [-0.9975,  0.9975],\n",
      "        [-1.4963,  1.4963]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "layer = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float, requires_grad=True)\n",
    "y = torch.matmul(x, layer)\n",
    "loss = torch.nn.functional.cross_entropy(y, torch.tensor([0, 1]))\n",
    "loss.backward()\n",
    "grad0 = layer.grad\n",
    "print(grad0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1, 2], [3, 4], [5, 6]])[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9975,  0.9975],\n",
      "        [-1.9951,  1.9951],\n",
      "        [-2.9926,  2.9926]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "layer = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float, requires_grad=True)\n",
    "y = torch.matmul(x, layer)\n",
    "loss = torch.nn.functional.cross_entropy(y, torch.tensor([0]))\n",
    "loss.backward()\n",
    "grad1 = layer.grad\n",
    "print(grad1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2236e-06, -1.4305e-06],\n",
      "        [ 1.5295e-06, -1.7881e-06],\n",
      "        [ 1.8354e-06, -2.1458e-06]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[4.0, 5.0, 6.0]])\n",
    "layer = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float, requires_grad=True)\n",
    "y = torch.matmul(x, layer)\n",
    "loss = torch.nn.functional.cross_entropy(y, torch.tensor([1]))\n",
    "loss.backward()\n",
    "grad2 =layer.grad\n",
    "print(grad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor([[-0.9975,  0.9975],\n",
    "        [-1.9951,  1.9951],\n",
    "        [-2.9926,  2.9926]])\n",
    "t2 = torch.tensor([[ 1.2236e-06, -1.4305e-06],\n",
    "        [ 1.5295e-06, -1.7881e-06],\n",
    "        [ 1.8354e-06, -2.1458e-06]])\n",
    "(grad1 + grad2) / 2 == grad0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinylm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
