|ITERS: 400858 / 400858 | COMP: 100.00% | RATE: 2.91 it./s | SPD: 0.3439 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 10.02 it./s | SPD: 0.0998 s/it.| ERT: (0, 0, 0, 0) |                                                                                                      

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:0.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 400858

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable
	--> loading vocab_size

Creating the model:

The model has 1M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-08_20-05 : iter     0 <=> epoch 0 | train loss 4.2484 | val loss 4.2482:
	--> training ...
	--> checkpointing ...

2024-11-08_20-38 : iter       5000 <=> epoch 0.021204472154669998 | train loss 1.1139237881 | val loss 1.1140904427:
	--> training ...
	--> checkpointing ...

2024-11-08_21-09 : iter      10000 <=> epoch 0.042408944309339995 | train loss 1.0783390999 | val loss 1.0785285234:
	--> training ...
	--> checkpointing ...

2024-11-08_21-40 : iter      15000 <=> epoch 0.06361341646400999 | train loss 1.0658142567 | val loss 1.0665292740:
	--> training ...
	--> checkpointing ...

2024-11-08_22-12 : iter      20000 <=> epoch 0.08481788861867999 | train loss 1.0579963923 | val loss 1.0591320992:
	--> training ...
	--> checkpointing ...

2024-11-08_22-43 : iter      25000 <=> epoch 0.10602236077334998 | train loss 1.0487226248 | val loss 1.0493410826:
	--> training ...
	--> checkpointing ...

2024-11-08_23-15 : iter      30000 <=> epoch 0.12722683292801998 | train loss 1.0396381617 | val loss 1.0411508083:
	--> training ...
	--> checkpointing ...

2024-11-08_23-46 : iter      35000 <=> epoch 0.14843130508268998 | train loss 1.0308737755 | val loss 1.0316166878:
	--> training ...
	--> checkpointing ...

2024-11-09_00-18 : iter      40000 <=> epoch 0.16963577723735998 | train loss 1.0280357599 | val loss 1.0276806355:
	--> training ...
	--> checkpointing ...

2024-11-09_00-49 : iter      45000 <=> epoch 0.19084024939202995 | train loss 1.0261405706 | val loss 1.0257513523:
	--> training ...
	--> checkpointing ...

2024-11-09_01-21 : iter      50000 <=> epoch 0.21204472154669995 | train loss 1.0242472887 | val loss 1.0243233442:
	--> training ...
	--> checkpointing ...

2024-11-09_01-53 : iter      55000 <=> epoch 0.23324919370136996 | train loss 1.0229879618 | val loss 1.0226423740:
	--> training ...
	--> checkpointing ...

2024-11-09_02-25 : iter      60000 <=> epoch 0.25445366585603996 | train loss 1.0174448490 | val loss 1.0176080465:
	--> training ...
	--> checkpointing ...

2024-11-09_02-57 : iter      65000 <=> epoch 0.27565813801070993 | train loss 1.0157407522 | val loss 1.0169250965:
	--> training ...
	--> checkpointing ...

2024-11-09_03-29 : iter      70000 <=> epoch 0.29686261016537996 | train loss 1.0131347179 | val loss 1.0126839876:
	--> training ...
	--> checkpointing ...

2024-11-09_04-01 : iter      75000 <=> epoch 0.31806708232004993 | train loss 1.0117713213 | val loss 1.0097001791:
	--> training ...
	--> checkpointing ...

2024-11-09_04-33 : iter      80000 <=> epoch 0.33927155447471996 | train loss 1.0094960928 | val loss 1.0088758469:
	--> training ...
	--> checkpointing ...

2024-11-09_05-05 : iter      85000 <=> epoch 0.36047602662938993 | train loss 1.0074830055 | val loss 1.0080996752:
	--> training ...
	--> checkpointing ...

2024-11-09_05-36 : iter      90000 <=> epoch 0.3816804987840599 | train loss 1.0100100040 | val loss 1.0089089870:
	--> training ...
	--> checkpointing ...

2024-11-09_06-07 : iter      95000 <=> epoch 0.40288497093872994 | train loss 1.0067936182 | val loss 1.0059995651:
	--> training ...
	--> checkpointing ...

2024-11-09_06-39 : iter     100000 <=> epoch 0.4240894430933999 | train loss 1.0051440001 | val loss 1.0051119328:
	--> training ...
	--> checkpointing ...

2024-11-09_07-12 : iter     105000 <=> epoch 0.44529391524806994 | train loss 1.0045692921 | val loss 1.0042475462:
	--> training ...
	--> checkpointing ...

2024-11-09_07-44 : iter     110000 <=> epoch 0.4664983874027399 | train loss 1.0075293779 | val loss 1.0067398548:
	--> training ...
	--> checkpointing ...

2024-11-09_08-16 : iter     115000 <=> epoch 0.48770285955740994 | train loss 1.0035564899 | val loss 1.0036752224:
	--> training ...
	--> checkpointing ...

2024-11-09_08-47 : iter     120000 <=> epoch 0.5089073317120799 | train loss 1.0031063557 | val loss 1.0021809340:
	--> training ...
	--> checkpointing ...

2024-11-09_09-18 : iter     125000 <=> epoch 0.5301118038667499 | train loss 1.0021167994 | val loss 1.0007992983:
	--> training ...
	--> checkpointing ...

2024-11-09_09-50 : iter     130000 <=> epoch 0.5513162760214199 | train loss 1.0030380487 | val loss 1.0037020445:
	--> training ...
	--> checkpointing ...

2024-11-09_10-21 : iter     135000 <=> epoch 0.5725207481760899 | train loss 1.0016982555 | val loss 1.0016808510:
	--> training ...
	--> checkpointing ...

2024-11-09_10-52 : iter     140000 <=> epoch 0.5937252203307599 | train loss 1.0015884638 | val loss 1.0012557507:
	--> training ...
	--> checkpointing ...

2024-11-09_11-23 : iter     145000 <=> epoch 0.61492969248543 | train loss 1.0022507906 | val loss 1.0032831430:
	--> training ...
	--> checkpointing ...

2024-11-09_11-55 : iter     150000 <=> epoch 0.6361341646400999 | train loss 1.0011889935 | val loss 1.0022405386:
	--> training ...
	--> checkpointing ...

2024-11-09_12-26 : iter     155000 <=> epoch 0.6573386367947699 | train loss 1.0020474195 | val loss 1.0031341314:
	--> training ...
	--> checkpointing ...

2024-11-09_12-57 : iter     160000 <=> epoch 0.6785431089494399 | train loss 1.0022377968 | val loss 1.0018068552:
	--> training ...
	--> checkpointing ...

2024-11-09_13-29 : iter     165000 <=> epoch 0.6997475811041098 | train loss 1.0032527447 | val loss 1.0014464855:
	--> training ...
	--> checkpointing ...

2024-11-09_14-01 : iter     170000 <=> epoch 0.7209520532587799 | train loss 1.0006151199 | val loss 0.9997768998:
	--> training ...
	--> checkpointing ...

2024-11-09_14-33 : iter     175000 <=> epoch 0.7421565254134499 | train loss 1.0006780624 | val loss 0.9991330504:
	--> training ...
	--> checkpointing ...

2024-11-09_15-06 : iter     180000 <=> epoch 0.7633609975681198 | train loss 1.0006029606 | val loss 0.9995638132:
	--> training ...
	--> checkpointing ...

2024-11-09_15-38 : iter     185000 <=> epoch 0.7845654697227898 | train loss 0.9992701411 | val loss 0.9997480512:
	--> training ...
	--> checkpointing ...

2024-11-09_16-10 : iter     190000 <=> epoch 0.8057699418774599 | train loss 1.0003619194 | val loss 0.9996452928:
	--> training ...
	--> checkpointing ...

2024-11-09_16-42 : iter     195000 <=> epoch 0.8269744140321299 | train loss 1.0002795458 | val loss 0.9997752309:
	--> training ...
	--> checkpointing ...

2024-11-09_17-14 : iter     200000 <=> epoch 0.8481788861867998 | train loss 0.9994171858 | val loss 0.9983401895:
	--> training ...
	--> checkpointing ...

2024-11-09_17-46 : iter     205000 <=> epoch 0.8693833583414698 | train loss 0.9985752702 | val loss 0.9989317656:
	--> training ...
	--> checkpointing ...

2024-11-09_18-18 : iter     210000 <=> epoch 0.8905878304961399 | train loss 0.9986116886 | val loss 0.9981440306:
	--> training ...
	--> checkpointing ...

2024-11-09_18-50 : iter     215000 <=> epoch 0.9117923026508098 | train loss 0.9976970553 | val loss 0.9966452718:
	--> training ...
	--> checkpointing ...

2024-11-09_19-22 : iter     220000 <=> epoch 0.9329967748054798 | train loss 0.9984185696 | val loss 0.9981205463:
	--> training ...
	--> checkpointing ...

2024-11-09_19-54 : iter     225000 <=> epoch 0.9542012469601499 | train loss 0.9976123571 | val loss 0.9996342063:
	--> training ...
	--> checkpointing ...

2024-11-09_20-26 : iter     230000 <=> epoch 0.9754057191148199 | train loss 0.9985291958 | val loss 0.9980562925:
	--> training ...
	--> checkpointing ...

2024-11-09_20-58 : iter     235000 <=> epoch 0.9966101912694898 | train loss 0.9971010089 | val loss 0.9972646236:
	--> training ...
	--> checkpointing ...

2024-11-09_21-29 : iter     240000 <=> epoch 1.0178146634241598 | train loss 1.0014734268 | val loss 1.0004575253:
	--> training ...
	--> checkpointing ...

2024-11-09_22-00 : iter     245000 <=> epoch 1.0390191355788299 | train loss 0.9974400401 | val loss 0.9974717498:
	--> training ...
	--> checkpointing ...

2024-11-09_22-31 : iter     250000 <=> epoch 1.0602236077334999 | train loss 0.9961189032 | val loss 0.9959348440:
	--> training ...
	--> checkpointing ...

2024-11-09_23-03 : iter     255000 <=> epoch 1.08142807988817 | train loss 0.9966402650 | val loss 0.9962728024:
	--> training ...
	--> checkpointing ...

2024-11-09_23-34 : iter     260000 <=> epoch 1.1026325520428397 | train loss 0.9964430332 | val loss 0.9968494773:
	--> training ...
	--> checkpointing ...

2024-11-10_00-05 : iter     265000 <=> epoch 1.1238370241975097 | train loss 0.9971097708 | val loss 0.9968003035:
	--> training ...
	--> checkpointing ...

2024-11-10_00-35 : iter     270000 <=> epoch 1.1450414963521798 | train loss 0.9970604777 | val loss 0.9966094494:
	--> training ...
	--> checkpointing ...

2024-11-10_01-07 : iter     275000 <=> epoch 1.1662459685068498 | train loss 0.9965305328 | val loss 0.9966181517:
	--> training ...
	--> checkpointing ...

2024-11-10_01-38 : iter     280000 <=> epoch 1.1874504406615198 | train loss 0.9955176711 | val loss 0.9957290292:
	--> training ...
	--> checkpointing ...

2024-11-10_02-09 : iter     285000 <=> epoch 1.2086549128161899 | train loss 0.9890210032 | val loss 0.9906446934:
	--> training ...
	--> checkpointing ...

2024-11-10_02-40 : iter     290000 <=> epoch 1.22985938497086 | train loss 0.9902584553 | val loss 0.9893990159:
	--> training ...
	--> checkpointing ...

2024-11-10_03-11 : iter     295000 <=> epoch 1.2510638571255297 | train loss 0.9880051017 | val loss 0.9887342453:
	--> training ...
	--> checkpointing ...

2024-11-10_03-42 : iter     300000 <=> epoch 1.2722683292801997 | train loss 0.9890027046 | val loss 0.9878565073:
	--> training ...
	--> checkpointing ...

2024-11-10_04-13 : iter     305000 <=> epoch 1.2934728014348698 | train loss 0.9875860214 | val loss 0.9885569215:
	--> training ...
	--> checkpointing ...

2024-11-10_04-44 : iter     310000 <=> epoch 1.3146772735895398 | train loss 0.9888040423 | val loss 0.9884593487:
	--> training ...
	--> checkpointing ...

2024-11-10_05-16 : iter     315000 <=> epoch 1.3358817457442098 | train loss 0.9878177047 | val loss 0.9897654653:
	--> training ...
	--> checkpointing ...

2024-11-10_05-47 : iter     320000 <=> epoch 1.3570862178988798 | train loss 0.9888345003 | val loss 0.9883156419:
	--> training ...
	--> checkpointing ...

2024-11-10_06-18 : iter     325000 <=> epoch 1.3782906900535496 | train loss 0.9883702993 | val loss 0.9878645539:
	--> training ...
	--> checkpointing ...

2024-11-10_06-50 : iter     330000 <=> epoch 1.3994951622082197 | train loss 0.9873136878 | val loss 0.9878261685:
	--> training ...
	--> checkpointing ...

2024-11-10_07-21 : iter     335000 <=> epoch 1.4206996343628897 | train loss 0.9885486364 | val loss 0.9867563844:
	--> training ...
	--> checkpointing ...

2024-11-10_07-51 : iter     340000 <=> epoch 1.4419041065175597 | train loss 0.9876627326 | val loss 0.9879864454:
	--> training ...
	--> checkpointing ...

2024-11-10_08-22 : iter     345000 <=> epoch 1.4631085786722298 | train loss 0.9878332019 | val loss 0.9873796701:
	--> training ...
	--> checkpointing ...

2024-11-10_08-53 : iter     350000 <=> epoch 1.4843130508268998 | train loss 0.9870905280 | val loss 0.9874314070:
	--> training ...
	--> checkpointing ...

2024-11-10_09-23 : iter     355000 <=> epoch 1.5055175229815698 | train loss 0.9867680669 | val loss 0.9875847697:
	--> training ...
	--> checkpointing ...

2024-11-10_09-54 : iter     360000 <=> epoch 1.5267219951362396 | train loss 0.9880052209 | val loss 0.9868654609:
	--> training ...
	--> checkpointing ...

2024-11-10_10-26 : iter     365000 <=> epoch 1.5479264672909097 | train loss 0.9875988364 | val loss 0.9875914454:
	--> training ...
	--> checkpointing ...

2024-11-10_10-57 : iter     370000 <=> epoch 1.5691309394455797 | train loss 0.9875912666 | val loss 0.9871785641:
	--> training ...
	--> checkpointing ...

2024-11-10_11-28 : iter     375000 <=> epoch 1.5903354116002497 | train loss 0.9869212508 | val loss 0.9874570370:
	--> training ...
	--> checkpointing ...

2024-11-10_11-59 : iter     380000 <=> epoch 1.6115398837549197 | train loss 0.9876238108 | val loss 0.9869500995:
	--> training ...
	--> checkpointing ...

2024-11-10_12-30 : iter     385000 <=> epoch 1.6327443559095898 | train loss 0.9878593087 | val loss 0.9880930781:
	--> training ...
	--> checkpointing ...

2024-11-10_13-02 : iter     390000 <=> epoch 1.6539488280642598 | train loss 0.9871758819 | val loss 0.9876868725:
	--> training ...
	--> checkpointing ...

2024-11-10_13-33 : iter     395000 <=> epoch 1.6751533002189296 | train loss 0.9880796671 | val loss 0.9869319797:
	--> training ...
	--> checkpointing ...

2024-11-10_14-04 : iter     400000 <=> epoch 1.6963577723735996 | train loss 0.9869489074 | val loss 0.9874028563:
	--> training ...
