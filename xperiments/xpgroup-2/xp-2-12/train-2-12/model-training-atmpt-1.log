|ITERS: 222562 / 222562 | COMP: 100.00% | RATE: 1.80 it./s | SPD: 0.5570 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 5.65 it./s | SPD: 0.1770 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:1.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 222562

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 61M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-29_20-42 : iter     0 <=> epoch 0 | train loss 4.1517 | val loss 4.1517:
	--> training ...
	--> checkpointing ...

2024-10-29_21-32 : iter       5000 <=> epoch 0.02246556094638398 | train loss 0.7103555202 | val loss 0.7092111111:
	--> training ...
	--> checkpointing ...

2024-10-29_22-21 : iter      10000 <=> epoch 0.04493112189276796 | train loss 0.6918721795 | val loss 0.6931170821:
	--> training ...
	--> checkpointing ...

2024-10-29_23-10 : iter      15000 <=> epoch 0.06739668283915194 | train loss 0.6786344051 | val loss 0.6798340678:
	--> training ...
	--> checkpointing ...

2024-10-30_00-00 : iter      20000 <=> epoch 0.08986224378553592 | train loss 0.6721922159 | val loss 0.6726720929:
	--> training ...
	--> checkpointing ...

2024-10-30_00-49 : iter      25000 <=> epoch 0.11232780473191989 | train loss 0.6661365032 | val loss 0.6663658023:
	--> training ...
	--> checkpointing ...

2024-10-30_01-39 : iter      30000 <=> epoch 0.13479336567830388 | train loss 0.6625449061 | val loss 0.6630552411:
	--> training ...
	--> checkpointing ...

2024-10-30_02-28 : iter      35000 <=> epoch 0.15725892662468785 | train loss 0.6593340039 | val loss 0.6606383324:
	--> training ...
	--> checkpointing ...

2024-10-30_03-18 : iter      40000 <=> epoch 0.17972448757107184 | train loss 0.6575465798 | val loss 0.6582129002:
	--> training ...
	--> checkpointing ...

2024-10-30_04-07 : iter      45000 <=> epoch 0.2021900485174558 | train loss 0.6561279297 | val loss 0.6568415761:
	--> training ...
	--> checkpointing ...

2024-10-30_04-56 : iter      50000 <=> epoch 0.22465560946383978 | train loss 0.6554858685 | val loss 0.6555349231:
	--> training ...
	--> checkpointing ...

2024-10-30_05-46 : iter      55000 <=> epoch 0.24712117041022377 | train loss 0.6527047157 | val loss 0.6542494297:
	--> training ...
	--> checkpointing ...

2024-10-30_06-35 : iter      60000 <=> epoch 0.26958673135660777 | train loss 0.6515738368 | val loss 0.6522173882:
	--> training ...
	--> checkpointing ...

2024-10-30_07-25 : iter      65000 <=> epoch 0.29205229230299173 | train loss 0.6505554914 | val loss 0.6513889432:
	--> training ...
	--> checkpointing ...

2024-10-30_08-14 : iter      70000 <=> epoch 0.3145178532493757 | train loss 0.6508789062 | val loss 0.6514471173:
	--> training ...
	--> checkpointing ...

2024-10-30_09-04 : iter      75000 <=> epoch 0.33698341419575967 | train loss 0.6496489644 | val loss 0.6503661275:
	--> training ...
	--> checkpointing ...

2024-10-30_09-53 : iter      80000 <=> epoch 0.3594489751421437 | train loss 0.6486310959 | val loss 0.6486641765:
	--> training ...
	--> checkpointing ...

2024-10-30_10-43 : iter      85000 <=> epoch 0.38191453608852766 | train loss 0.6486687660 | val loss 0.6492002606:
	--> training ...
	--> checkpointing ...

2024-10-30_11-32 : iter      90000 <=> epoch 0.4043800970349116 | train loss 0.6472750902 | val loss 0.6481701136:
	--> training ...
	--> checkpointing ...

2024-10-30_12-21 : iter      95000 <=> epoch 0.4268456579812956 | train loss 0.6471514106 | val loss 0.6481040120:
	--> training ...
	--> checkpointing ...

2024-10-30_13-11 : iter     100000 <=> epoch 0.44931121892767956 | train loss 0.6478770375 | val loss 0.6476837993:
	--> training ...
	--> checkpointing ...

2024-10-30_14-00 : iter     105000 <=> epoch 0.4717767798740636 | train loss 0.6476032138 | val loss 0.6478012800:
	--> training ...
	--> checkpointing ...

2024-10-30_14-50 : iter     110000 <=> epoch 0.49424234082044755 | train loss 0.6465154886 | val loss 0.6476984024:
	--> training ...
	--> checkpointing ...

2024-10-30_15-39 : iter     115000 <=> epoch 0.5167079017668316 | train loss 0.6506707668 | val loss 0.6519681215:
	--> training ...
	--> checkpointing ...

2024-10-30_16-29 : iter     120000 <=> epoch 0.5391734627132155 | train loss 0.6464384794 | val loss 0.6466740966:
	--> training ...
	--> checkpointing ...

2024-10-30_17-18 : iter     125000 <=> epoch 0.5616390236595995 | train loss 0.6477543712 | val loss 0.6471809149:
	--> training ...
	--> checkpointing ...

2024-10-30_18-07 : iter     130000 <=> epoch 0.5841045846059835 | train loss 0.6474447846 | val loss 0.6461166739:
	--> training ...
	--> checkpointing ...

2024-10-30_18-57 : iter     135000 <=> epoch 0.6065701455523674 | train loss 0.6473715305 | val loss 0.6469191909:
	--> training ...
	--> checkpointing ...

2024-10-30_19-46 : iter     140000 <=> epoch 0.6290357064987514 | train loss 0.6455990672 | val loss 0.6459611654:
	--> training ...
	--> checkpointing ...

2024-10-30_20-36 : iter     145000 <=> epoch 0.6515012674451354 | train loss 0.6461247206 | val loss 0.6457979679:
	--> training ...
	--> checkpointing ...

2024-10-30_21-25 : iter     150000 <=> epoch 0.6739668283915193 | train loss 0.6451142430 | val loss 0.6457335949:
	--> training ...
	--> checkpointing ...

2024-10-30_22-15 : iter     155000 <=> epoch 0.6964323893379033 | train loss 0.6439870596 | val loss 0.6452506781:
	--> training ...
	--> checkpointing ...

2024-10-30_23-04 : iter     160000 <=> epoch 0.7188979502842874 | train loss 0.6420515180 | val loss 0.6417152286:
	--> training ...
	--> checkpointing ...

2024-10-30_23-53 : iter     165000 <=> epoch 0.7413635112306713 | train loss 0.6406632066 | val loss 0.6425571442:
	--> training ...
	--> checkpointing ...

2024-10-31_00-43 : iter     170000 <=> epoch 0.7638290721770553 | train loss 0.6417049766 | val loss 0.6401955485:
	--> training ...
	--> checkpointing ...

2024-10-31_01-32 : iter     175000 <=> epoch 0.7862946331234393 | train loss 0.6416655779 | val loss 0.6421360373:
	--> training ...
	--> checkpointing ...

2024-10-31_02-22 : iter     180000 <=> epoch 0.8087601940698232 | train loss 0.6406155229 | val loss 0.6400738358:
	--> training ...
	--> checkpointing ...

2024-10-31_03-11 : iter     185000 <=> epoch 0.8312257550162072 | train loss 0.6399716139 | val loss 0.6412512064:
	--> training ...
	--> checkpointing ...

2024-10-31_04-01 : iter     190000 <=> epoch 0.8536913159625912 | train loss 0.6395921707 | val loss 0.6410459280:
	--> training ...
	--> checkpointing ...

2024-10-31_04-50 : iter     195000 <=> epoch 0.8761568769089751 | train loss 0.6398431659 | val loss 0.6404023170:
	--> training ...
	--> checkpointing ...

2024-10-31_05-39 : iter     200000 <=> epoch 0.8986224378553591 | train loss 0.6408071518 | val loss 0.6391036510:
	--> training ...
	--> checkpointing ...

2024-10-31_06-29 : iter     205000 <=> epoch 0.9210879988017432 | train loss 0.6410627365 | val loss 0.6396651268:
	--> training ...
	--> checkpointing ...

2024-10-31_07-18 : iter     210000 <=> epoch 0.9435535597481272 | train loss 0.6388857365 | val loss 0.6401684284:
	--> training ...
	--> checkpointing ...

2024-10-31_08-08 : iter     215000 <=> epoch 0.9660191206945111 | train loss 0.6385735869 | val loss 0.6415484548:
	--> training ...
	--> checkpointing ...

2024-10-31_08-57 : iter     220000 <=> epoch 0.9884846816408951 | train loss 0.6401080489 | val loss 0.6405478716:
	--> training ...
