# DESCRIPTION
After we got good results with the 12 attention heads and 12 attention blocks 30M model we decided to scale it 
to 60M size with RMSNorm and SiLU trained on 30M data
# OBTENTION

# META-DATA

# MODELS-LOCATION

