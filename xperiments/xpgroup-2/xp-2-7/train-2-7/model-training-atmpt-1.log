|ITERS: 222562 / 222562 | COMP: 100.00% | RATE: 2.80 it./s | SPD: 0.3570 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 8.52 it./s | SPD: 0.1173 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:0.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 2)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 222562

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 30M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-28_03-21 : iter     0 <=> epoch 0 | train loss 4.2913 | val loss 4.2916:
	--> training ...
	--> checkpointing ...

2024-10-28_03-53 : iter       5000 <=> epoch 0.02246556094638398 | train loss 0.7220085263 | val loss 0.7211714387:
	--> training ...
	--> checkpointing ...

2024-10-28_04-25 : iter      10000 <=> epoch 0.04493112189276796 | train loss 0.6862968206 | val loss 0.6866623163:
	--> training ...
	--> checkpointing ...

2024-10-28_04-57 : iter      15000 <=> epoch 0.06739668283915194 | train loss 0.6741377115 | val loss 0.6753057837:
	--> training ...
	--> checkpointing ...

2024-10-28_05-28 : iter      20000 <=> epoch 0.08986224378553592 | train loss 0.6696486473 | val loss 0.6694575548:
	--> training ...
	--> checkpointing ...

2024-10-28_06-00 : iter      25000 <=> epoch 0.11232780473191989 | train loss 0.6645687819 | val loss 0.6647557616:
	--> training ...
	--> checkpointing ...

2024-10-28_06-32 : iter      30000 <=> epoch 0.13479336567830388 | train loss 0.6612996459 | val loss 0.6613683105:
	--> training ...
	--> checkpointing ...

2024-10-28_07-03 : iter      35000 <=> epoch 0.15725892662468785 | train loss 0.6590371132 | val loss 0.6582194567:
	--> training ...
	--> checkpointing ...

2024-10-28_07-35 : iter      40000 <=> epoch 0.17972448757107184 | train loss 0.6583253145 | val loss 0.6565362215:
	--> training ...
	--> checkpointing ...

2024-10-28_08-07 : iter      45000 <=> epoch 0.2021900485174558 | train loss 0.6569731236 | val loss 0.6571624875:
	--> training ...
	--> checkpointing ...

2024-10-28_08-39 : iter      50000 <=> epoch 0.22465560946383978 | train loss 0.6549459696 | val loss 0.6543652415:
	--> training ...
	--> checkpointing ...

2024-10-28_09-10 : iter      55000 <=> epoch 0.24712117041022377 | train loss 0.6536002159 | val loss 0.6530945897:
	--> training ...
	--> checkpointing ...

2024-10-28_09-42 : iter      60000 <=> epoch 0.26958673135660777 | train loss 0.6522852778 | val loss 0.6526564956:
	--> training ...
	--> checkpointing ...

2024-10-28_10-14 : iter      65000 <=> epoch 0.29205229230299173 | train loss 0.6514474154 | val loss 0.6515390873:
	--> training ...
	--> checkpointing ...

2024-10-28_10-45 : iter      70000 <=> epoch 0.3145178532493757 | train loss 0.6501137614 | val loss 0.6495762467:
	--> training ...
	--> checkpointing ...

2024-10-28_11-17 : iter      75000 <=> epoch 0.33698341419575967 | train loss 0.6504789591 | val loss 0.6497238278:
	--> training ...
	--> checkpointing ...

2024-10-28_11-49 : iter      80000 <=> epoch 0.3594489751421437 | train loss 0.6492323875 | val loss 0.6507857442:
	--> training ...
	--> checkpointing ...

2024-10-28_12-20 : iter      85000 <=> epoch 0.38191453608852766 | train loss 0.6486629844 | val loss 0.6496434212:
	--> training ...
	--> checkpointing ...

2024-10-28_12-52 : iter      90000 <=> epoch 0.4043800970349116 | train loss 0.6486928463 | val loss 0.6501647234:
	--> training ...
	--> checkpointing ...

2024-10-28_13-24 : iter      95000 <=> epoch 0.4268456579812956 | train loss 0.6490738392 | val loss 0.6469054818:
	--> training ...
	--> checkpointing ...

2024-10-28_13-55 : iter     100000 <=> epoch 0.44931121892767956 | train loss 0.6470472217 | val loss 0.6483367682:
	--> training ...
	--> checkpointing ...

2024-10-28_14-27 : iter     105000 <=> epoch 0.4717767798740636 | train loss 0.6463364363 | val loss 0.6477413774:
	--> training ...
	--> checkpointing ...

2024-10-28_14-59 : iter     110000 <=> epoch 0.49424234082044755 | train loss 0.6473230720 | val loss 0.6471026540:
	--> training ...
	--> checkpointing ...

2024-10-28_15-30 : iter     115000 <=> epoch 0.5167079017668316 | train loss 0.6468654275 | val loss 0.6486629844:
	--> training ...
	--> checkpointing ...

2024-10-28_16-02 : iter     120000 <=> epoch 0.5391734627132155 | train loss 0.6460055709 | val loss 0.6478927135:
	--> training ...
	--> checkpointing ...

2024-10-28_16-34 : iter     125000 <=> epoch 0.5616390236595995 | train loss 0.6464011073 | val loss 0.6473103762:
	--> training ...
	--> checkpointing ...

2024-10-28_17-06 : iter     130000 <=> epoch 0.5841045846059835 | train loss 0.6459652185 | val loss 0.6454064250:
	--> training ...
	--> checkpointing ...

2024-10-28_17-37 : iter     135000 <=> epoch 0.6065701455523674 | train loss 0.6462037563 | val loss 0.6458550096:
	--> training ...
	--> checkpointing ...

2024-10-28_18-09 : iter     140000 <=> epoch 0.6290357064987514 | train loss 0.6466703415 | val loss 0.6469940543:
	--> training ...
	--> checkpointing ...

2024-10-28_18-41 : iter     145000 <=> epoch 0.6515012674451354 | train loss 0.6443708539 | val loss 0.6457143426:
	--> training ...
	--> checkpointing ...

2024-10-28_19-12 : iter     150000 <=> epoch 0.6739668283915193 | train loss 0.6451575756 | val loss 0.6463823318:
	--> training ...
	--> checkpointing ...

2024-10-28_19-44 : iter     155000 <=> epoch 0.6964323893379033 | train loss 0.6444224119 | val loss 0.6459987760:
	--> training ...
	--> checkpointing ...

2024-10-28_20-16 : iter     160000 <=> epoch 0.7188979502842874 | train loss 0.6419017911 | val loss 0.6422610283:
	--> training ...
	--> checkpointing ...

2024-10-28_20-47 : iter     165000 <=> epoch 0.7413635112306713 | train loss 0.6410318017 | val loss 0.6422219872:
	--> training ...
	--> checkpointing ...

2024-10-28_21-19 : iter     170000 <=> epoch 0.7638290721770553 | train loss 0.6415397525 | val loss 0.6410732269:
	--> training ...
	--> checkpointing ...

2024-10-28_21-51 : iter     175000 <=> epoch 0.7862946331234393 | train loss 0.6401371360 | val loss 0.6411896348:
	--> training ...
	--> checkpointing ...

2024-10-28_22-22 : iter     180000 <=> epoch 0.8087601940698232 | train loss 0.6405760646 | val loss 0.6412112713:
	--> training ...
	--> checkpointing ...

2024-10-28_22-54 : iter     185000 <=> epoch 0.8312257550162072 | train loss 0.6400688887 | val loss 0.6411275864:
	--> training ...
	--> checkpointing ...

2024-10-28_23-26 : iter     190000 <=> epoch 0.8536913159625912 | train loss 0.6391305923 | val loss 0.6415991783:
	--> training ...
	--> checkpointing ...

2024-10-28_23-58 : iter     195000 <=> epoch 0.8761568769089751 | train loss 0.6416792274 | val loss 0.6405228376:
	--> training ...
	--> checkpointing ...

2024-10-29_00-29 : iter     200000 <=> epoch 0.8986224378553591 | train loss 0.6410608888 | val loss 0.6418940425:
	--> training ...
	--> checkpointing ...

2024-10-29_01-01 : iter     205000 <=> epoch 0.9210879988017432 | train loss 0.6409890652 | val loss 0.6417498589:
	--> training ...
	--> checkpointing ...

2024-10-29_01-33 : iter     210000 <=> epoch 0.9435535597481272 | train loss 0.6401462555 | val loss 0.6409106255:
	--> training ...
	--> checkpointing ...

2024-10-29_02-04 : iter     215000 <=> epoch 0.9660191206945111 | train loss 0.6405103207 | val loss 0.6418302059:
	--> training ...
	--> checkpointing ...

2024-10-29_02-36 : iter     220000 <=> epoch 0.9884846816408951 | train loss 0.6404592395 | val loss 0.6412001848:
	--> training ...
