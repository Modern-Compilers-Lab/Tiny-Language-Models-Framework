|ITERS: 240818 / 240818 | COMP: 100.00% | RATE: 3.29 it./s | SPD: 0.3039 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 9.97 it./s | SPD: 0.1003 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:6.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 2)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 240818

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 40M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-23_12-54 : iter     0 <=> epoch 0 | train loss 4.2285 | val loss 4.2282:
	--> training ...
	--> checkpointing ...

2024-10-23_13-21 : iter       5000 <=> epoch 0.020762526678703094 | train loss 0.7727757096 | val loss 0.7721095085:
	--> training ...
	--> checkpointing ...

2024-10-23_13-47 : iter      10000 <=> epoch 0.04152505335740619 | train loss 0.6940910816 | val loss 0.6955778599:
	--> training ...
	--> checkpointing ...

2024-10-23_14-14 : iter      15000 <=> epoch 0.062287580036109284 | train loss 0.6768187881 | val loss 0.6750335693:
	--> training ...
	--> checkpointing ...

2024-10-23_14-41 : iter      20000 <=> epoch 0.08305010671481237 | train loss 0.6674574018 | val loss 0.6681606770:
	--> training ...
	--> checkpointing ...

2024-10-23_15-08 : iter      25000 <=> epoch 0.10381263339351547 | train loss 0.6604800820 | val loss 0.6592708826:
	--> training ...
	--> checkpointing ...

2024-10-23_15-35 : iter      30000 <=> epoch 0.12457516007221857 | train loss 0.6518158913 | val loss 0.6522569656:
	--> training ...
	--> checkpointing ...

2024-10-23_16-01 : iter      35000 <=> epoch 0.14533768675092165 | train loss 0.6461594105 | val loss 0.6461861134:
	--> training ...
	--> checkpointing ...

2024-10-23_16-28 : iter      40000 <=> epoch 0.16610021342962475 | train loss 0.6418961287 | val loss 0.6415277123:
	--> training ...
	--> checkpointing ...

2024-10-23_16-55 : iter      45000 <=> epoch 0.18686274010832785 | train loss 0.6395624280 | val loss 0.6387909055:
	--> training ...
	--> checkpointing ...

2024-10-23_17-22 : iter      50000 <=> epoch 0.20762526678703094 | train loss 0.6377146244 | val loss 0.6375647187:
	--> training ...
	--> checkpointing ...

2024-10-23_17-48 : iter      55000 <=> epoch 0.22838779346573404 | train loss 0.6346463561 | val loss 0.6346337795:
	--> training ...
	--> checkpointing ...

2024-10-23_18-15 : iter      60000 <=> epoch 0.24915032014443714 | train loss 0.6334177852 | val loss 0.6339406967:
	--> training ...
	--> checkpointing ...

2024-10-23_18-42 : iter      65000 <=> epoch 0.2699128468231402 | train loss 0.6327639818 | val loss 0.6330423355:
	--> training ...
	--> checkpointing ...

2024-10-23_19-09 : iter      70000 <=> epoch 0.2906753735018433 | train loss 0.6306996942 | val loss 0.6322825551:
	--> training ...
	--> checkpointing ...

2024-10-23_19-35 : iter      75000 <=> epoch 0.3114379001805464 | train loss 0.6296340823 | val loss 0.6309409738:
	--> training ...
	--> checkpointing ...

2024-10-23_20-02 : iter      80000 <=> epoch 0.3322004268592495 | train loss 0.6281559467 | val loss 0.6284458637:
	--> training ...
	--> checkpointing ...

2024-10-23_20-29 : iter      85000 <=> epoch 0.3529629535379526 | train loss 0.6279739738 | val loss 0.6301654577:
	--> training ...
	--> checkpointing ...

2024-10-23_20-56 : iter      90000 <=> epoch 0.3737254802166557 | train loss 0.6284598708 | val loss 0.6288412809:
	--> training ...
	--> checkpointing ...

2024-10-23_21-22 : iter      95000 <=> epoch 0.3944880068953588 | train loss 0.6272037625 | val loss 0.6267901659:
	--> training ...
	--> checkpointing ...

2024-10-23_21-49 : iter     100000 <=> epoch 0.4152505335740619 | train loss 0.6271480918 | val loss 0.6272771955:
	--> training ...
	--> checkpointing ...

2024-10-23_22-16 : iter     105000 <=> epoch 0.436013060252765 | train loss 0.6269356608 | val loss 0.6271319985:
	--> training ...
	--> checkpointing ...

2024-10-23_22-43 : iter     110000 <=> epoch 0.4567755869314681 | train loss 0.6259735227 | val loss 0.6266941428:
	--> training ...
	--> checkpointing ...

2024-10-23_23-09 : iter     115000 <=> epoch 0.4775381136101712 | train loss 0.6247951388 | val loss 0.6263565421:
	--> training ...
	--> checkpointing ...

2024-10-23_23-36 : iter     120000 <=> epoch 0.4983006402888743 | train loss 0.6252874136 | val loss 0.6246799231:
	--> training ...
	--> checkpointing ...

2024-10-24_00-03 : iter     125000 <=> epoch 0.5190631669675774 | train loss 0.6246834397 | val loss 0.6246405244:
	--> training ...
	--> checkpointing ...

2024-10-24_00-30 : iter     130000 <=> epoch 0.5398256936462804 | train loss 0.6246469021 | val loss 0.6239257455:
	--> training ...
	--> checkpointing ...

2024-10-24_00-56 : iter     135000 <=> epoch 0.5605882203249836 | train loss 0.6232413054 | val loss 0.6243243217:
	--> training ...
	--> checkpointing ...

2024-10-24_01-23 : iter     140000 <=> epoch 0.5813507470036866 | train loss 0.6244230866 | val loss 0.6228234768:
	--> training ...
	--> checkpointing ...

2024-10-24_01-50 : iter     145000 <=> epoch 0.6021132736823898 | train loss 0.6235623360 | val loss 0.6251556277:
	--> training ...
	--> checkpointing ...

2024-10-24_02-17 : iter     150000 <=> epoch 0.6228758003610928 | train loss 0.6227494478 | val loss 0.6227909923:
	--> training ...
	--> checkpointing ...

2024-10-24_02-43 : iter     155000 <=> epoch 0.643638327039796 | train loss 0.6225483418 | val loss 0.6225010753:
	--> training ...
	--> checkpointing ...

2024-10-24_03-10 : iter     160000 <=> epoch 0.664400853718499 | train loss 0.6228366494 | val loss 0.6228457689:
	--> training ...
	--> checkpointing ...

2024-10-24_03-37 : iter     165000 <=> epoch 0.6851633803972021 | train loss 0.6216435432 | val loss 0.6217591166:
	--> training ...
	--> checkpointing ...

2024-10-24_04-04 : iter     170000 <=> epoch 0.7059259070759052 | train loss 0.6188719273 | val loss 0.6173799038:
	--> training ...
	--> checkpointing ...

2024-10-24_04-30 : iter     175000 <=> epoch 0.7266884337546083 | train loss 0.6171423793 | val loss 0.6164008379:
	--> training ...
	--> checkpointing ...

2024-10-24_04-57 : iter     180000 <=> epoch 0.7474509604333114 | train loss 0.6162958741 | val loss 0.6172717810:
	--> training ...
	--> checkpointing ...

2024-10-24_05-24 : iter     185000 <=> epoch 0.7682134871120145 | train loss 0.6133386493 | val loss 0.6149976254:
	--> training ...
	--> checkpointing ...

2024-10-24_05-51 : iter     190000 <=> epoch 0.7889760137907176 | train loss 0.6160451174 | val loss 0.6165362597:
	--> training ...
	--> checkpointing ...

2024-10-24_06-18 : iter     195000 <=> epoch 0.8097385404694207 | train loss 0.6138940454 | val loss 0.6157755852:
	--> training ...
	--> checkpointing ...

2024-10-24_06-45 : iter     200000 <=> epoch 0.8305010671481238 | train loss 0.6165976524 | val loss 0.6151520610:
	--> training ...
	--> checkpointing ...

2024-10-24_07-12 : iter     205000 <=> epoch 0.8512635938268268 | train loss 0.6142119169 | val loss 0.6144281626:
	--> training ...
	--> checkpointing ...

2024-10-24_07-39 : iter     210000 <=> epoch 0.87202612050553 | train loss 0.6139699221 | val loss 0.6145986319:
	--> training ...
	--> checkpointing ...

2024-10-24_08-06 : iter     215000 <=> epoch 0.892788647184233 | train loss 0.6157363057 | val loss 0.6169470549:
	--> training ...
	--> checkpointing ...

2024-10-24_08-33 : iter     220000 <=> epoch 0.9135511738629362 | train loss 0.6160130501 | val loss 0.6151723862:
	--> training ...
	--> checkpointing ...

2024-10-24_09-00 : iter     225000 <=> epoch 0.9343137005416392 | train loss 0.6157635450 | val loss 0.6137892008:
	--> training ...
	--> checkpointing ...

2024-10-24_09-27 : iter     230000 <=> epoch 0.9550762272203424 | train loss 0.6146363616 | val loss 0.6155411005:
	--> training ...
	--> checkpointing ...

2024-10-24_09-54 : iter     235000 <=> epoch 0.9758387538990454 | train loss 0.6154916883 | val loss 0.6154327393:
	--> training ...
	--> checkpointing ...

2024-10-24_10-21 : iter     240000 <=> epoch 0.9966012805777485 | train loss 0.6156462431 | val loss 0.6154305339:
	--> training ...
