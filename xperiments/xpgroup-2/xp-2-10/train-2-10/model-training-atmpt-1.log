|ITERS: 213427 / 222562 | COMP: 95.90% | RATE: 1.87 it./s | SPD: 0.5347 s/it.| ERT: (0, 1, 21, 24)                                                                                                      
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 5.91 it./s | SPD: 0.1691 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:3.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 222562

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 30M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-28_04-29 : iter     0 <=> epoch 0 | train loss 4.1478 | val loss 4.1479:
	--> training ...
	--> checkpointing ...

2024-10-28_05-16 : iter       5000 <=> epoch 0.02246556094638398 | train loss 0.6946978569 | val loss 0.6940505505:
	--> training ...
	--> checkpointing ...

2024-10-28_06-04 : iter      10000 <=> epoch 0.04493112189276796 | train loss 0.6759775877 | val loss 0.6777769327:
	--> training ...
	--> checkpointing ...

2024-10-28_06-51 : iter      15000 <=> epoch 0.06739668283915194 | train loss 0.6702638865 | val loss 0.6694869995:
	--> training ...
	--> checkpointing ...

2024-10-28_07-39 : iter      20000 <=> epoch 0.08986224378553592 | train loss 0.6638341546 | val loss 0.6636783481:
	--> training ...
	--> checkpointing ...

2024-10-28_08-26 : iter      25000 <=> epoch 0.11232780473191989 | train loss 0.6588971019 | val loss 0.6596229672:
	--> training ...
	--> checkpointing ...

2024-10-28_09-13 : iter      30000 <=> epoch 0.13479336567830388 | train loss 0.6596018076 | val loss 0.6573674679:
	--> training ...
	--> checkpointing ...

2024-10-28_10-01 : iter      35000 <=> epoch 0.15725892662468785 | train loss 0.6560157537 | val loss 0.6558519006:
	--> training ...
	--> checkpointing ...

2024-10-28_10-48 : iter      40000 <=> epoch 0.17972448757107184 | train loss 0.6540372372 | val loss 0.6526993513:
	--> training ...
	--> checkpointing ...

2024-10-28_11-36 : iter      45000 <=> epoch 0.2021900485174558 | train loss 0.6520547867 | val loss 0.6529275775:
	--> training ...
	--> checkpointing ...

2024-10-28_12-23 : iter      50000 <=> epoch 0.22465560946383978 | train loss 0.6518151760 | val loss 0.6501172781:
	--> training ...
	--> checkpointing ...

2024-10-28_13-11 : iter      55000 <=> epoch 0.24712117041022377 | train loss 0.6484447122 | val loss 0.6500630379:
	--> training ...
	--> checkpointing ...

2024-10-28_13-58 : iter      60000 <=> epoch 0.26958673135660777 | train loss 0.6492623687 | val loss 0.6488393545:
	--> training ...
	--> checkpointing ...

2024-10-28_14-45 : iter      65000 <=> epoch 0.29205229230299173 | train loss 0.6489576697 | val loss 0.6492803693:
	--> training ...
	--> checkpointing ...

2024-10-28_15-33 : iter      70000 <=> epoch 0.3145178532493757 | train loss 0.6492623687 | val loss 0.6483089924:
	--> training ...
	--> checkpointing ...

2024-10-28_16-20 : iter      75000 <=> epoch 0.33698341419575967 | train loss 0.6471224427 | val loss 0.6470729113:
	--> training ...
	--> checkpointing ...

2024-10-28_17-08 : iter      80000 <=> epoch 0.3594489751421437 | train loss 0.6470965147 | val loss 0.6490137577:
	--> training ...
	--> checkpointing ...

2024-10-28_17-55 : iter      85000 <=> epoch 0.38191453608852766 | train loss 0.6468433142 | val loss 0.6463021636:
	--> training ...
	--> checkpointing ...

2024-10-28_18-43 : iter      90000 <=> epoch 0.4043800970349116 | train loss 0.6474683881 | val loss 0.6466806531:
	--> training ...
	--> checkpointing ...

2024-10-28_19-30 : iter      95000 <=> epoch 0.4268456579812956 | train loss 0.6459107995 | val loss 0.6460963488:
	--> training ...
	--> checkpointing ...

2024-10-28_20-17 : iter     100000 <=> epoch 0.44931121892767956 | train loss 0.6455110312 | val loss 0.6468369961:
	--> training ...
	--> checkpointing ...

2024-10-28_21-05 : iter     105000 <=> epoch 0.4717767798740636 | train loss 0.6450205445 | val loss 0.6455571055:
	--> training ...
	--> checkpointing ...

2024-10-28_21-52 : iter     110000 <=> epoch 0.49424234082044755 | train loss 0.6446160674 | val loss 0.6450094581:
	--> training ...
	--> checkpointing ...

2024-10-28_22-39 : iter     115000 <=> epoch 0.5167079017668316 | train loss 0.6448196173 | val loss 0.6462903023:
	--> training ...
	--> checkpointing ...

2024-10-28_23-27 : iter     120000 <=> epoch 0.5391734627132155 | train loss 0.6449342370 | val loss 0.6446613669:
	--> training ...
	--> checkpointing ...

2024-10-29_00-14 : iter     125000 <=> epoch 0.5616390236595995 | train loss 0.6453797221 | val loss 0.6447628140:
	--> training ...
	--> checkpointing ...

2024-10-29_01-01 : iter     130000 <=> epoch 0.5841045846059835 | train loss 0.6442257762 | val loss 0.6442173123:
	--> training ...
	--> checkpointing ...

2024-10-29_01-49 : iter     135000 <=> epoch 0.6065701455523674 | train loss 0.6441754103 | val loss 0.6447583437:
	--> training ...
	--> checkpointing ...

2024-10-29_02-36 : iter     140000 <=> epoch 0.6290357064987514 | train loss 0.6447222829 | val loss 0.6451299191:
	--> training ...
	--> checkpointing ...

2024-10-29_03-23 : iter     145000 <=> epoch 0.6515012674451354 | train loss 0.6439059377 | val loss 0.6427822709:
	--> training ...
	--> checkpointing ...

2024-10-29_04-11 : iter     150000 <=> epoch 0.6739668283915193 | train loss 0.6431457996 | val loss 0.6442739367:
	--> training ...
	--> checkpointing ...

2024-10-29_04-58 : iter     155000 <=> epoch 0.6964323893379033 | train loss 0.6441583037 | val loss 0.6440044045:
	--> training ...
	--> checkpointing ...

2024-10-29_05-46 : iter     160000 <=> epoch 0.7188979502842874 | train loss 0.6387587190 | val loss 0.6396606565:
	--> training ...
	--> checkpointing ...

2024-10-29_06-33 : iter     165000 <=> epoch 0.7413635112306713 | train loss 0.6394759417 | val loss 0.6405425072:
	--> training ...
	--> checkpointing ...

2024-10-29_07-21 : iter     170000 <=> epoch 0.7638290721770553 | train loss 0.6386638880 | val loss 0.6396600008:
	--> training ...
	--> checkpointing ...

2024-10-29_08-08 : iter     175000 <=> epoch 0.7862946331234393 | train loss 0.6394275427 | val loss 0.6387233734:
	--> training ...
	--> checkpointing ...

2024-10-29_08-55 : iter     180000 <=> epoch 0.8087601940698232 | train loss 0.6381748319 | val loss 0.6378757358:
	--> training ...
	--> checkpointing ...

2024-10-29_09-43 : iter     185000 <=> epoch 0.8312257550162072 | train loss 0.6394609213 | val loss 0.6382375360:
	--> training ...
	--> checkpointing ...

2024-10-29_10-30 : iter     190000 <=> epoch 0.8536913159625912 | train loss 0.6376273632 | val loss 0.6387943029:
	--> training ...
	--> checkpointing ...

2024-10-29_11-18 : iter     195000 <=> epoch 0.8761568769089751 | train loss 0.6387380958 | val loss 0.6385147572:
	--> training ...
	--> checkpointing ...

2024-10-29_12-05 : iter     200000 <=> epoch 0.8986224378553591 | train loss 0.6372751594 | val loss 0.6394872665:
	--> training ...
	--> checkpointing ...

2024-10-29_12-53 : iter     205000 <=> epoch 0.9210879988017432 | train loss 0.6391197443 | val loss 0.6391368508:
	--> training ...
	--> checkpointing ...

2024-10-29_13-40 : iter     210000 <=> epoch 0.9435535597481272 | train loss 0.6387873292 | val loss 0.6393495798:
	--> training ...
