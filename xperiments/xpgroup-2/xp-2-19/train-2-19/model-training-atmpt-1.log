|ITERS: 222562 / 222562 | COMP: 100.00% | RATE: 1.39 it./s | SPD: 0.7206 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 4.57 it./s | SPD: 0.2189 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:1.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 222562

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 50M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-01_17-19 : iter     0 <=> epoch 0 | train loss 4.1542 | val loss 4.1546:
	--> training ...
	--> checkpointing ...

2024-11-01_18-23 : iter       5000 <=> epoch 0.02246556094638398 | train loss 0.7099579573 | val loss 0.7098120451:
	--> training ...
	--> checkpointing ...

2024-11-01_19-27 : iter      10000 <=> epoch 0.04493112189276796 | train loss 0.6906055212 | val loss 0.6895931363:
	--> training ...
	--> checkpointing ...

2024-11-01_20-31 : iter      15000 <=> epoch 0.06739668283915194 | train loss 0.6750203967 | val loss 0.6735355854:
	--> training ...
	--> checkpointing ...

2024-11-01_21-35 : iter      20000 <=> epoch 0.08986224378553592 | train loss 0.6673257351 | val loss 0.6677454114:
	--> training ...
	--> checkpointing ...

2024-11-01_22-38 : iter      25000 <=> epoch 0.11232780473191989 | train loss 0.6615517139 | val loss 0.6641882062:
	--> training ...
	--> checkpointing ...

2024-11-01_23-42 : iter      30000 <=> epoch 0.13479336567830388 | train loss 0.6581645608 | val loss 0.6581519246:
	--> training ...
	--> checkpointing ...

2024-11-02_00-46 : iter      35000 <=> epoch 0.15725892662468785 | train loss 0.6581254005 | val loss 0.6571761370:
	--> training ...
	--> checkpointing ...

2024-11-02_01-49 : iter      40000 <=> epoch 0.17972448757107184 | train loss 0.6536992192 | val loss 0.6554414630:
	--> training ...
	--> checkpointing ...

2024-11-02_02-53 : iter      45000 <=> epoch 0.2021900485174558 | train loss 0.6544005275 | val loss 0.6539110541:
	--> training ...
	--> checkpointing ...

2024-11-02_03-57 : iter      50000 <=> epoch 0.22465560946383978 | train loss 0.6521322131 | val loss 0.6513686180:
	--> training ...
	--> checkpointing ...

2024-11-02_05-00 : iter      55000 <=> epoch 0.24712117041022377 | train loss 0.6503748894 | val loss 0.6505474448:
	--> training ...
	--> checkpointing ...

2024-11-02_06-04 : iter      60000 <=> epoch 0.26958673135660777 | train loss 0.6492539048 | val loss 0.6491985917:
	--> training ...
	--> checkpointing ...

2024-11-02_07-08 : iter      65000 <=> epoch 0.29205229230299173 | train loss 0.6485207081 | val loss 0.6483042836:
	--> training ...
	--> checkpointing ...

2024-11-02_08-12 : iter      70000 <=> epoch 0.3145178532493757 | train loss 0.6483634710 | val loss 0.6484373212:
	--> training ...
	--> checkpointing ...

2024-11-02_09-15 : iter      75000 <=> epoch 0.33698341419575967 | train loss 0.6485595107 | val loss 0.6473889947:
	--> training ...
	--> checkpointing ...

2024-11-02_10-19 : iter      80000 <=> epoch 0.3594489751421437 | train loss 0.6483384967 | val loss 0.6471694708:
	--> training ...
	--> checkpointing ...

2024-11-02_11-23 : iter      85000 <=> epoch 0.38191453608852766 | train loss 0.6467789412 | val loss 0.6464517713:
	--> training ...
	--> checkpointing ...

2024-11-02_12-26 : iter      90000 <=> epoch 0.4043800970349116 | train loss 0.6461035609 | val loss 0.6462593675:
	--> training ...
	--> checkpointing ...

2024-11-02_13-30 : iter      95000 <=> epoch 0.4268456579812956 | train loss 0.6454105973 | val loss 0.6459763050:
	--> training ...
	--> checkpointing ...

2024-11-02_14-34 : iter     100000 <=> epoch 0.44931121892767956 | train loss 0.6465386748 | val loss 0.6454808712:
	--> training ...
	--> checkpointing ...

2024-11-02_15-37 : iter     105000 <=> epoch 0.4717767798740636 | train loss 0.6456827521 | val loss 0.6451965570:
	--> training ...
	--> checkpointing ...

2024-11-02_16-41 : iter     110000 <=> epoch 0.49424234082044755 | train loss 0.6443618536 | val loss 0.6451406479:
	--> training ...
	--> checkpointing ...

2024-11-02_17-45 : iter     115000 <=> epoch 0.5167079017668316 | train loss 0.6452643871 | val loss 0.6446611881:
	--> training ...
	--> checkpointing ...

2024-11-02_18-49 : iter     120000 <=> epoch 0.5391734627132155 | train loss 0.6445790529 | val loss 0.6446475387:
	--> training ...
	--> checkpointing ...

2024-11-02_19-52 : iter     125000 <=> epoch 0.5616390236595995 | train loss 0.6443156600 | val loss 0.6457313299:
	--> training ...
	--> checkpointing ...

2024-11-02_20-56 : iter     130000 <=> epoch 0.5841045846059835 | train loss 0.6439840794 | val loss 0.6444185376:
	--> training ...
	--> checkpointing ...

2024-11-02_22-00 : iter     135000 <=> epoch 0.6065701455523674 | train loss 0.6459323764 | val loss 0.6443215609:
	--> training ...
	--> checkpointing ...

2024-11-02_23-03 : iter     140000 <=> epoch 0.6290357064987514 | train loss 0.6435335279 | val loss 0.6445981264:
	--> training ...
	--> checkpointing ...

2024-11-03_00-07 : iter     145000 <=> epoch 0.6515012674451354 | train loss 0.6423470378 | val loss 0.6445747614:
	--> training ...
	--> checkpointing ...

2024-11-03_01-11 : iter     150000 <=> epoch 0.6739668283915193 | train loss 0.6422226429 | val loss 0.6441630721:
	--> training ...
	--> checkpointing ...

2024-11-03_02-14 : iter     155000 <=> epoch 0.6964323893379033 | train loss 0.6434972882 | val loss 0.6433462501:
	--> training ...
	--> checkpointing ...

2024-11-03_03-18 : iter     160000 <=> epoch 0.7188979502842874 | train loss 0.6410502195 | val loss 0.6412692070:
	--> training ...
	--> checkpointing ...

2024-11-03_04-22 : iter     165000 <=> epoch 0.7413635112306713 | train loss 0.6399994493 | val loss 0.6403757930:
	--> training ...
	--> checkpointing ...

2024-11-03_05-26 : iter     170000 <=> epoch 0.7638290721770553 | train loss 0.6396179199 | val loss 0.6385557652:
	--> training ...
	--> checkpointing ...

2024-11-03_06-29 : iter     175000 <=> epoch 0.7862946331234393 | train loss 0.6391106248 | val loss 0.6390222311:
	--> training ...
	--> checkpointing ...

2024-11-03_07-33 : iter     180000 <=> epoch 0.8087601940698232 | train loss 0.6397377253 | val loss 0.6395156384:
	--> training ...
	--> checkpointing ...

2024-11-03_08-37 : iter     185000 <=> epoch 0.8312257550162072 | train loss 0.6391481757 | val loss 0.6396717429:
	--> training ...
	--> checkpointing ...

2024-11-03_09-40 : iter     190000 <=> epoch 0.8536913159625912 | train loss 0.6385717392 | val loss 0.6395431757:
	--> training ...
	--> checkpointing ...

2024-11-03_10-44 : iter     195000 <=> epoch 0.8761568769089751 | train loss 0.6394665241 | val loss 0.6397853494:
	--> training ...
	--> checkpointing ...

2024-11-03_11-48 : iter     200000 <=> epoch 0.8986224378553591 | train loss 0.6395606399 | val loss 0.6390639544:
	--> training ...
	--> checkpointing ...

2024-11-03_12-51 : iter     205000 <=> epoch 0.9210879988017432 | train loss 0.6395341158 | val loss 0.6393243670:
	--> training ...
	--> checkpointing ...

2024-11-03_13-55 : iter     210000 <=> epoch 0.9435535597481272 | train loss 0.6399017572 | val loss 0.6390137076:
	--> training ...
	--> checkpointing ...

2024-11-03_14-59 : iter     215000 <=> epoch 0.9660191206945111 | train loss 0.6390884519 | val loss 0.6400510669:
	--> training ...
	--> checkpointing ...

2024-11-03_16-02 : iter     220000 <=> epoch 0.9884846816408951 | train loss 0.6387582421 | val loss 0.6394479275:
	--> training ...
