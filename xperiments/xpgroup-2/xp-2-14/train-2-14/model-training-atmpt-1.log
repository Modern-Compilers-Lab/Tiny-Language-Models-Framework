|ITERS: 222562 / 222562 | COMP: 100.00% | RATE: 1.81 it./s | SPD: 0.5539 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 5.67 it./s | SPD: 0.1764 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:0.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 2)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 222562

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 61M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-29_22-31 : iter     0 <=> epoch 0 | train loss 398.1740 | val loss 398.1758:
	--> training ...
	--> checkpointing ...

2024-10-29_23-20 : iter       5000 <=> epoch 0.02246556094638398 | train loss 2.0315823555 | val loss 2.0303502083:
	--> training ...
	--> checkpointing ...

2024-10-30_00-09 : iter      10000 <=> epoch 0.04493112189276796 | train loss 0.9082673192 | val loss 0.9068811536:
	--> training ...
	--> checkpointing ...

2024-10-30_00-58 : iter      15000 <=> epoch 0.06739668283915194 | train loss 0.7882602811 | val loss 0.7877281308:
	--> training ...
	--> checkpointing ...

2024-10-30_01-48 : iter      20000 <=> epoch 0.08986224378553592 | train loss 0.7531667948 | val loss 0.7527490258:
	--> training ...
	--> checkpointing ...

2024-10-30_02-37 : iter      25000 <=> epoch 0.11232780473191989 | train loss 0.7142263651 | val loss 0.7152323127:
	--> training ...
	--> checkpointing ...

2024-10-30_03-26 : iter      30000 <=> epoch 0.13479336567830388 | train loss 0.6980949640 | val loss 0.6986622214:
	--> training ...
	--> checkpointing ...

2024-10-30_04-15 : iter      35000 <=> epoch 0.15725892662468785 | train loss 0.6886699200 | val loss 0.6909745932:
	--> training ...
	--> checkpointing ...

2024-10-30_05-04 : iter      40000 <=> epoch 0.17972448757107184 | train loss 0.6831588149 | val loss 0.6831127405:
	--> training ...
	--> checkpointing ...

2024-10-30_05-53 : iter      45000 <=> epoch 0.2021900485174558 | train loss 0.6789315939 | val loss 0.6803774238:
	--> training ...
	--> checkpointing ...

2024-10-30_06-42 : iter      50000 <=> epoch 0.22465560946383978 | train loss 0.6739143133 | val loss 0.6754407287:
	--> training ...
	--> checkpointing ...

2024-10-30_07-31 : iter      55000 <=> epoch 0.24712117041022377 | train loss 0.6716126204 | val loss 0.6732029915:
	--> training ...
	--> checkpointing ...

2024-10-30_08-20 : iter      60000 <=> epoch 0.26958673135660777 | train loss 0.6683137417 | val loss 0.6689804196:
	--> training ...
	--> checkpointing ...

2024-10-30_09-09 : iter      65000 <=> epoch 0.29205229230299173 | train loss 0.6661628485 | val loss 0.6676091552:
	--> training ...
	--> checkpointing ...

2024-10-30_09-58 : iter      70000 <=> epoch 0.3145178532493757 | train loss 0.6636740565 | val loss 0.6630280614:
	--> training ...
	--> checkpointing ...

2024-10-30_10-47 : iter      75000 <=> epoch 0.33698341419575967 | train loss 0.6616727114 | val loss 0.6610250473:
	--> training ...
	--> checkpointing ...

2024-10-30_11-36 : iter      80000 <=> epoch 0.3594489751421437 | train loss 0.6608021855 | val loss 0.6607171893:
	--> training ...
	--> checkpointing ...

2024-10-30_12-25 : iter      85000 <=> epoch 0.38191453608852766 | train loss 0.6597503424 | val loss 0.6592578292:
	--> training ...
	--> checkpointing ...

2024-10-30_13-14 : iter      90000 <=> epoch 0.4043800970349116 | train loss 0.6585406065 | val loss 0.6579797268:
	--> training ...
	--> checkpointing ...

2024-10-30_14-03 : iter      95000 <=> epoch 0.4268456579812956 | train loss 0.6569350362 | val loss 0.6570553184:
	--> training ...
	--> checkpointing ...

2024-10-30_14-53 : iter     100000 <=> epoch 0.44931121892767956 | train loss 0.6562675238 | val loss 0.6561795473:
	--> training ...
	--> checkpointing ...

2024-10-30_15-42 : iter     105000 <=> epoch 0.4717767798740636 | train loss 0.6548987627 | val loss 0.6559672952:
	--> training ...
	--> checkpointing ...

2024-10-30_16-31 : iter     110000 <=> epoch 0.49424234082044755 | train loss 0.6547049284 | val loss 0.6541293263:
	--> training ...
	--> checkpointing ...

2024-10-30_17-20 : iter     115000 <=> epoch 0.5167079017668316 | train loss 0.6540603638 | val loss 0.6540629864:
	--> training ...
	--> checkpointing ...

2024-10-30_18-09 : iter     120000 <=> epoch 0.5391734627132155 | train loss 0.6534276009 | val loss 0.6536244154:
	--> training ...
	--> checkpointing ...

2024-10-30_18-58 : iter     125000 <=> epoch 0.5616390236595995 | train loss 0.6522890329 | val loss 0.6523321867:
	--> training ...
	--> checkpointing ...

2024-10-30_19-47 : iter     130000 <=> epoch 0.5841045846059835 | train loss 0.6516602635 | val loss 0.6514997482:
	--> training ...
	--> checkpointing ...

2024-10-30_20-36 : iter     135000 <=> epoch 0.6065701455523674 | train loss 0.6510949731 | val loss 0.6510061622:
	--> training ...
	--> checkpointing ...

2024-10-30_21-25 : iter     140000 <=> epoch 0.6290357064987514 | train loss 0.6506228447 | val loss 0.6520115733:
	--> training ...
	--> checkpointing ...

2024-10-30_22-14 : iter     145000 <=> epoch 0.6515012674451354 | train loss 0.6505651474 | val loss 0.6502719522:
	--> training ...
	--> checkpointing ...

2024-10-30_23-03 : iter     150000 <=> epoch 0.6739668283915193 | train loss 0.6500248909 | val loss 0.6510413885:
	--> training ...
	--> checkpointing ...

2024-10-30_23-52 : iter     155000 <=> epoch 0.6964323893379033 | train loss 0.6492918730 | val loss 0.6496564746:
	--> training ...
	--> checkpointing ...

2024-10-31_00-41 : iter     160000 <=> epoch 0.7188979502842874 | train loss 0.6454157829 | val loss 0.6453443766:
	--> training ...
	--> checkpointing ...

2024-10-31_01-30 : iter     165000 <=> epoch 0.7413635112306713 | train loss 0.6442103386 | val loss 0.6460052133:
	--> training ...
	--> checkpointing ...

2024-10-31_02-19 : iter     170000 <=> epoch 0.7638290721770553 | train loss 0.6452746987 | val loss 0.6459717751:
	--> training ...
	--> checkpointing ...

2024-10-31_03-08 : iter     175000 <=> epoch 0.7862946331234393 | train loss 0.6446713805 | val loss 0.6454966068:
	--> training ...
	--> checkpointing ...

2024-10-31_03-57 : iter     180000 <=> epoch 0.8087601940698232 | train loss 0.6444442272 | val loss 0.6447755694:
	--> training ...
	--> checkpointing ...

2024-10-31_04-46 : iter     185000 <=> epoch 0.8312257550162072 | train loss 0.6437894702 | val loss 0.6432577968:
	--> training ...
	--> checkpointing ...

2024-10-31_05-35 : iter     190000 <=> epoch 0.8536913159625912 | train loss 0.6433886886 | val loss 0.6444370151:
	--> training ...
	--> checkpointing ...

2024-10-31_06-24 : iter     195000 <=> epoch 0.8761568769089751 | train loss 0.6441325545 | val loss 0.6442930698:
	--> training ...
	--> checkpointing ...

2024-10-31_07-13 : iter     200000 <=> epoch 0.8986224378553591 | train loss 0.6436725855 | val loss 0.6438351274:
	--> training ...
	--> checkpointing ...

2024-10-31_08-02 : iter     205000 <=> epoch 0.9210879988017432 | train loss 0.6443619132 | val loss 0.6429748535:
	--> training ...
	--> checkpointing ...

2024-10-31_08-51 : iter     210000 <=> epoch 0.9435535597481272 | train loss 0.6436163187 | val loss 0.6436807513:
	--> training ...
	--> checkpointing ...

2024-10-31_09-40 : iter     215000 <=> epoch 0.9660191206945111 | train loss 0.6435617805 | val loss 0.6441897750:
	--> training ...
	--> checkpointing ...

2024-10-31_10-29 : iter     220000 <=> epoch 0.9884846816408951 | train loss 0.6427088380 | val loss 0.6439981461:
	--> training ...
