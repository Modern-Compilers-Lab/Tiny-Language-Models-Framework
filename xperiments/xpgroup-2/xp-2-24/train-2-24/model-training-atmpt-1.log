|ITERS: 400858 / 400858 | COMP: 100.00% | RATE: 2.74 it./s | SPD: 0.3649 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 9.13 it./s | SPD: 0.1096 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:6.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 400858

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable
	--> loading vocab_size

Creating the model:

The model has 5M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-10_00-28 : iter     0 <=> epoch 0 | train loss 4.4266 | val loss 4.4268:
	--> training ...
	--> checkpointing ...

2024-11-10_01-03 : iter       5000 <=> epoch 0.021204472154669998 | train loss 1.0822976828 | val loss 1.0817530155:
	--> training ...
	--> checkpointing ...

2024-11-10_01-37 : iter      10000 <=> epoch 0.042408944309339995 | train loss 1.0591636896 | val loss 1.0615464449:
	--> training ...
	--> checkpointing ...

2024-11-10_02-10 : iter      15000 <=> epoch 0.06361341646400999 | train loss 1.0351589918 | val loss 1.0359327793:
	--> training ...
	--> checkpointing ...

2024-11-10_02-43 : iter      20000 <=> epoch 0.08481788861867999 | train loss 1.0248069763 | val loss 1.0250140429:
	--> training ...
	--> checkpointing ...

2024-11-10_03-15 : iter      25000 <=> epoch 0.10602236077334998 | train loss 1.0199706554 | val loss 1.0191371441:
	--> training ...
	--> checkpointing ...

2024-11-10_03-48 : iter      30000 <=> epoch 0.12722683292801998 | train loss 1.0154420137 | val loss 1.0143567324:
	--> training ...
	--> checkpointing ...

2024-11-10_04-20 : iter      35000 <=> epoch 0.14843130508268998 | train loss 1.0087360144 | val loss 1.0090066195:
	--> training ...
	--> checkpointing ...

2024-11-10_04-53 : iter      40000 <=> epoch 0.16963577723735998 | train loss 1.0047646761 | val loss 1.0049136877:
	--> training ...
	--> checkpointing ...

2024-11-10_05-26 : iter      45000 <=> epoch 0.19084024939202995 | train loss 1.0012072325 | val loss 1.0008884668:
	--> training ...
	--> checkpointing ...

2024-11-10_05-58 : iter      50000 <=> epoch 0.21204472154669995 | train loss 0.9998166561 | val loss 0.9997387528:
	--> training ...
	--> checkpointing ...

2024-11-10_06-31 : iter      55000 <=> epoch 0.23324919370136996 | train loss 0.9995749593 | val loss 0.9996306300:
	--> training ...
	--> checkpointing ...

2024-11-10_07-03 : iter      60000 <=> epoch 0.25445366585603996 | train loss 0.9968327284 | val loss 0.9964886308:
	--> training ...
	--> checkpointing ...

2024-11-10_07-36 : iter      65000 <=> epoch 0.27565813801070993 | train loss 0.9950555563 | val loss 0.9968246222:
	--> training ...
	--> checkpointing ...

2024-11-10_08-09 : iter      70000 <=> epoch 0.29686261016537996 | train loss 0.9960629940 | val loss 0.9966209531:
	--> training ...
	--> checkpointing ...

2024-11-10_08-41 : iter      75000 <=> epoch 0.31806708232004993 | train loss 0.9945229888 | val loss 0.9955848455:
	--> training ...
	--> checkpointing ...

2024-11-10_09-14 : iter      80000 <=> epoch 0.33927155447471996 | train loss 0.9947941899 | val loss 0.9947219491:
	--> training ...
	--> checkpointing ...

2024-11-10_09-46 : iter      85000 <=> epoch 0.36047602662938993 | train loss 0.9945200682 | val loss 0.9943635464:
	--> training ...
	--> checkpointing ...

2024-11-10_10-19 : iter      90000 <=> epoch 0.3816804987840599 | train loss 0.9926192760 | val loss 0.9927264452:
	--> training ...
	--> checkpointing ...

2024-11-10_10-51 : iter      95000 <=> epoch 0.40288497093872994 | train loss 0.9956173897 | val loss 0.9940739274:
	--> training ...
	--> checkpointing ...

2024-11-10_11-23 : iter     100000 <=> epoch 0.4240894430933999 | train loss 0.9920694232 | val loss 0.9924851060:
	--> training ...
	--> checkpointing ...

2024-11-10_11-55 : iter     105000 <=> epoch 0.44529391524806994 | train loss 0.9909802079 | val loss 0.9906782508:
	--> training ...
	--> checkpointing ...

2024-11-10_12-29 : iter     110000 <=> epoch 0.4664983874027399 | train loss 0.9906780720 | val loss 0.9907068610:
	--> training ...
	--> checkpointing ...

2024-11-10_13-01 : iter     115000 <=> epoch 0.48770285955740994 | train loss 0.9902072549 | val loss 0.9907299876:
	--> training ...
	--> checkpointing ...

2024-11-10_13-34 : iter     120000 <=> epoch 0.5089073317120799 | train loss 0.9905431867 | val loss 0.9897100925:
	--> training ...
	--> checkpointing ...

2024-11-10_14-07 : iter     125000 <=> epoch 0.5301118038667499 | train loss 0.9888511300 | val loss 0.9883583784:
	--> training ...
	--> checkpointing ...

2024-11-10_14-40 : iter     130000 <=> epoch 0.5513162760214199 | train loss 0.9892907143 | val loss 0.9898150563:
	--> training ...
	--> checkpointing ...

2024-11-10_15-13 : iter     135000 <=> epoch 0.5725207481760899 | train loss 0.9884756804 | val loss 0.9885109663:
	--> training ...
	--> checkpointing ...

2024-11-10_15-45 : iter     140000 <=> epoch 0.5937252203307599 | train loss 0.9901021719 | val loss 0.9884902239:
	--> training ...
	--> checkpointing ...

2024-11-10_16-18 : iter     145000 <=> epoch 0.61492969248543 | train loss 0.9891509414 | val loss 0.9892562032:
	--> training ...
	--> checkpointing ...

2024-11-10_16-51 : iter     150000 <=> epoch 0.6361341646400999 | train loss 0.9878370166 | val loss 0.9889533520:
	--> training ...
	--> checkpointing ...

2024-11-10_17-23 : iter     155000 <=> epoch 0.6573386367947699 | train loss 0.9873535037 | val loss 0.9876605868:
	--> training ...
	--> checkpointing ...

2024-11-10_17-56 : iter     160000 <=> epoch 0.6785431089494399 | train loss 0.9872921705 | val loss 0.9883028269:
	--> training ...
	--> checkpointing ...

2024-11-10_18-28 : iter     165000 <=> epoch 0.6997475811041098 | train loss 0.9872203469 | val loss 0.9879046679:
	--> training ...
	--> checkpointing ...

2024-11-10_19-01 : iter     170000 <=> epoch 0.7209520532587799 | train loss 0.9884636998 | val loss 0.9865868092:
	--> training ...
	--> checkpointing ...

2024-11-10_19-33 : iter     175000 <=> epoch 0.7421565254134499 | train loss 0.9874234200 | val loss 0.9871888757:
	--> training ...
	--> checkpointing ...

2024-11-10_20-06 : iter     180000 <=> epoch 0.7633609975681198 | train loss 0.9868556857 | val loss 0.9859370589:
	--> training ...
	--> checkpointing ...

2024-11-10_20-39 : iter     185000 <=> epoch 0.7845654697227898 | train loss 0.9869304299 | val loss 0.9881867766:
	--> training ...
	--> checkpointing ...

2024-11-10_21-12 : iter     190000 <=> epoch 0.8057699418774599 | train loss 0.9871451855 | val loss 0.9874528646:
	--> training ...
	--> checkpointing ...

2024-11-10_21-44 : iter     195000 <=> epoch 0.8269744140321299 | train loss 0.9881290793 | val loss 0.9865612388:
	--> training ...
	--> checkpointing ...

2024-11-10_22-16 : iter     200000 <=> epoch 0.8481788861867998 | train loss 0.9879049063 | val loss 0.9868276715:
	--> training ...
	--> checkpointing ...

2024-11-10_22-49 : iter     205000 <=> epoch 0.8693833583414698 | train loss 0.9865212440 | val loss 0.9862905741:
	--> training ...
	--> checkpointing ...

2024-11-10_23-22 : iter     210000 <=> epoch 0.8905878304961399 | train loss 0.9856340885 | val loss 0.9856879711:
	--> training ...
	--> checkpointing ...

2024-11-10_23-55 : iter     215000 <=> epoch 0.9117923026508098 | train loss 0.9873877764 | val loss 0.9869737625:
	--> training ...
	--> checkpointing ...

2024-11-11_00-27 : iter     220000 <=> epoch 0.9329967748054798 | train loss 0.9852360487 | val loss 0.9860931635:
	--> training ...
	--> checkpointing ...

2024-11-11_00-59 : iter     225000 <=> epoch 0.9542012469601499 | train loss 0.9855448604 | val loss 0.9865160584:
	--> training ...
	--> checkpointing ...

2024-11-11_01-31 : iter     230000 <=> epoch 0.9754057191148199 | train loss 0.9877809286 | val loss 0.9878194928:
	--> training ...
	--> checkpointing ...

2024-11-11_02-04 : iter     235000 <=> epoch 0.9966101912694898 | train loss 0.9881677032 | val loss 0.9869025946:
	--> training ...
	--> checkpointing ...

2024-11-11_02-36 : iter     240000 <=> epoch 1.0178146634241598 | train loss 0.9847421646 | val loss 0.9858179092:
	--> training ...
	--> checkpointing ...

2024-11-11_03-08 : iter     245000 <=> epoch 1.0390191355788299 | train loss 0.9856825471 | val loss 0.9856587052:
	--> training ...
	--> checkpointing ...

2024-11-11_03-41 : iter     250000 <=> epoch 1.0602236077334999 | train loss 0.9872123599 | val loss 0.9864178300:
	--> training ...
	--> checkpointing ...

2024-11-11_04-13 : iter     255000 <=> epoch 1.08142807988817 | train loss 0.9872316122 | val loss 0.9882578254:
	--> training ...
	--> checkpointing ...

2024-11-11_04-46 : iter     260000 <=> epoch 1.1026325520428397 | train loss 0.9858497381 | val loss 0.9853315353:
	--> training ...
	--> checkpointing ...

2024-11-11_05-18 : iter     265000 <=> epoch 1.1238370241975097 | train loss 0.9856756330 | val loss 0.9854314327:
	--> training ...
	--> checkpointing ...

2024-11-11_05-50 : iter     270000 <=> epoch 1.1450414963521798 | train loss 0.9865278602 | val loss 0.9846771955:
	--> training ...
	--> checkpointing ...

2024-11-11_06-23 : iter     275000 <=> epoch 1.1662459685068498 | train loss 0.9859713316 | val loss 0.9851273298:
	--> training ...
	--> checkpointing ...

2024-11-11_06-55 : iter     280000 <=> epoch 1.1874504406615198 | train loss 0.9865366220 | val loss 0.9858692288:
	--> training ...
	--> checkpointing ...

2024-11-11_07-28 : iter     285000 <=> epoch 1.2086549128161899 | train loss 0.9815586805 | val loss 0.9808428884:
	--> training ...
	--> checkpointing ...

2024-11-11_08-00 : iter     290000 <=> epoch 1.22985938497086 | train loss 0.9807337523 | val loss 0.9823480248:
	--> training ...
	--> checkpointing ...

2024-11-11_08-32 : iter     295000 <=> epoch 1.2510638571255297 | train loss 0.9814071059 | val loss 0.9821357131:
	--> training ...
	--> checkpointing ...

2024-11-11_09-05 : iter     300000 <=> epoch 1.2722683292801997 | train loss 0.9818949103 | val loss 0.9802287817:
	--> training ...
	--> checkpointing ...

2024-11-11_09-37 : iter     305000 <=> epoch 1.2934728014348698 | train loss 0.9804661870 | val loss 0.9801408648:
	--> training ...
	--> checkpointing ...

2024-11-11_10-10 : iter     310000 <=> epoch 1.3146772735895398 | train loss 0.9800722599 | val loss 0.9800935984:
	--> training ...
	--> checkpointing ...

2024-11-11_10-42 : iter     315000 <=> epoch 1.3358817457442098 | train loss 0.9813589454 | val loss 0.9808906317:
	--> training ...
	--> checkpointing ...

2024-11-11_11-15 : iter     320000 <=> epoch 1.3570862178988798 | train loss 0.9818500280 | val loss 0.9804120660:
	--> training ...
	--> checkpointing ...

2024-11-11_11-47 : iter     325000 <=> epoch 1.3782906900535496 | train loss 0.9810203314 | val loss 0.9808844924:
	--> training ...
	--> checkpointing ...

2024-11-11_12-19 : iter     330000 <=> epoch 1.3994951622082197 | train loss 0.9811651111 | val loss 0.9813236594:
	--> training ...
	--> checkpointing ...

2024-11-11_12-52 : iter     335000 <=> epoch 1.4206996343628897 | train loss 0.9810833335 | val loss 0.9807738662:
	--> training ...
	--> checkpointing ...

2024-11-11_13-24 : iter     340000 <=> epoch 1.4419041065175597 | train loss 0.9798713326 | val loss 0.9805387259:
	--> training ...
	--> checkpointing ...

2024-11-11_13-56 : iter     345000 <=> epoch 1.4631085786722298 | train loss 0.9790055752 | val loss 0.9806028605:
	--> training ...
	--> checkpointing ...

2024-11-11_14-29 : iter     350000 <=> epoch 1.4843130508268998 | train loss 0.9797044992 | val loss 0.9810062051:
	--> training ...
	--> checkpointing ...

2024-11-11_15-01 : iter     355000 <=> epoch 1.5055175229815698 | train loss 0.9805322886 | val loss 0.9807075858:
	--> training ...
	--> checkpointing ...

2024-11-11_15-33 : iter     360000 <=> epoch 1.5267219951362396 | train loss 0.9798924923 | val loss 0.9798338413:
	--> training ...
	--> checkpointing ...

2024-11-11_16-05 : iter     365000 <=> epoch 1.5479264672909097 | train loss 0.9804546833 | val loss 0.9814773202:
	--> training ...
	--> checkpointing ...

2024-11-11_16-37 : iter     370000 <=> epoch 1.5691309394455797 | train loss 0.9798247218 | val loss 0.9815427065:
	--> training ...
	--> checkpointing ...

2024-11-11_17-09 : iter     375000 <=> epoch 1.5903354116002497 | train loss 0.9811292887 | val loss 0.9805821776:
	--> training ...
	--> checkpointing ...

2024-11-11_17-42 : iter     380000 <=> epoch 1.6115398837549197 | train loss 0.9805157185 | val loss 0.9794099331:
	--> training ...
	--> checkpointing ...

2024-11-11_18-14 : iter     385000 <=> epoch 1.6327443559095898 | train loss 0.9795078039 | val loss 0.9800066948:
	--> training ...
	--> checkpointing ...

2024-11-11_18-46 : iter     390000 <=> epoch 1.6539488280642598 | train loss 0.9806644320 | val loss 0.9812222719:
	--> training ...
	--> checkpointing ...

2024-11-11_19-18 : iter     395000 <=> epoch 1.6751533002189296 | train loss 0.9807888269 | val loss 0.9805970192:
	--> training ...
	--> checkpointing ...

2024-11-11_19-50 : iter     400000 <=> epoch 1.6963577723735996 | train loss 0.9796272516 | val loss 0.9812511802:
	--> training ...
