|ITERS: 222562 / 222562 | COMP: 100.00% | RATE: 4.16 it./s | SPD: 0.2404 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 12.26 it./s | SPD: 0.0816 s/it.| ERT: (0, 0, 0, 0) |                                                                                                      

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:1.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 2)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 222562

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 30M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-28_03-39 : iter     0 <=> epoch 0 | train loss 4.1559 | val loss 4.1559:
	--> training ...
	--> checkpointing ...

2024-10-28_04-01 : iter       5000 <=> epoch 0.02246556094638398 | train loss 0.6998211145 | val loss 0.6993825436:
	--> training ...
	--> checkpointing ...

2024-10-28_04-22 : iter      10000 <=> epoch 0.04493112189276796 | train loss 0.6804969907 | val loss 0.6795865297:
	--> training ...
	--> checkpointing ...

2024-10-28_04-44 : iter      15000 <=> epoch 0.06739668283915194 | train loss 0.6723888516 | val loss 0.6718451381:
	--> training ...
	--> checkpointing ...

2024-10-28_05-05 : iter      20000 <=> epoch 0.08986224378553592 | train loss 0.6653391719 | val loss 0.6647627354:
	--> training ...
	--> checkpointing ...

2024-10-28_05-27 : iter      25000 <=> epoch 0.11232780473191989 | train loss 0.6632590890 | val loss 0.6622372270:
	--> training ...
	--> checkpointing ...

2024-10-28_05-48 : iter      30000 <=> epoch 0.13479336567830388 | train loss 0.6590691209 | val loss 0.6598817110:
	--> training ...
	--> checkpointing ...

2024-10-28_06-09 : iter      35000 <=> epoch 0.15725892662468785 | train loss 0.6583358645 | val loss 0.6590554118:
	--> training ...
	--> checkpointing ...

2024-10-28_06-31 : iter      40000 <=> epoch 0.17972448757107184 | train loss 0.6579490304 | val loss 0.6563870907:
	--> training ...
	--> checkpointing ...

2024-10-28_06-52 : iter      45000 <=> epoch 0.2021900485174558 | train loss 0.6566304564 | val loss 0.6572299004:
	--> training ...
	--> checkpointing ...

2024-10-28_07-14 : iter      50000 <=> epoch 0.22465560946383978 | train loss 0.6546667814 | val loss 0.6549553871:
	--> training ...
	--> checkpointing ...

2024-10-28_07-35 : iter      55000 <=> epoch 0.24712117041022377 | train loss 0.6545472145 | val loss 0.6547283530:
	--> training ...
	--> checkpointing ...

2024-10-28_07-57 : iter      60000 <=> epoch 0.26958673135660777 | train loss 0.6525747776 | val loss 0.6533166766:
	--> training ...
	--> checkpointing ...

2024-10-28_08-18 : iter      65000 <=> epoch 0.29205229230299173 | train loss 0.6519379616 | val loss 0.6537328959:
	--> training ...
	--> checkpointing ...

2024-10-28_08-40 : iter      70000 <=> epoch 0.3145178532493757 | train loss 0.6532982588 | val loss 0.6519969106:
	--> training ...
	--> checkpointing ...

2024-10-28_09-01 : iter      75000 <=> epoch 0.33698341419575967 | train loss 0.6506109834 | val loss 0.6520019770:
	--> training ...
	--> checkpointing ...

2024-10-28_09-22 : iter      80000 <=> epoch 0.3594489751421437 | train loss 0.6511127949 | val loss 0.6508840919:
	--> training ...
	--> checkpointing ...

2024-10-28_09-44 : iter      85000 <=> epoch 0.38191453608852766 | train loss 0.6516081095 | val loss 0.6516366005:
	--> training ...
	--> checkpointing ...

2024-10-28_10-05 : iter      90000 <=> epoch 0.4043800970349116 | train loss 0.6508086920 | val loss 0.6504905224:
	--> training ...
	--> checkpointing ...

2024-10-28_10-27 : iter      95000 <=> epoch 0.4268456579812956 | train loss 0.6491331458 | val loss 0.6514303684:
	--> training ...
	--> checkpointing ...

2024-10-28_10-48 : iter     100000 <=> epoch 0.44931121892767956 | train loss 0.6492824554 | val loss 0.6501967311:
	--> training ...
	--> checkpointing ...

2024-10-28_11-09 : iter     105000 <=> epoch 0.4717767798740636 | train loss 0.6486942172 | val loss 0.6502846479:
	--> training ...
	--> checkpointing ...

2024-10-28_11-31 : iter     110000 <=> epoch 0.49424234082044755 | train loss 0.6490157247 | val loss 0.6493139863:
	--> training ...
	--> checkpointing ...

2024-10-28_11-52 : iter     115000 <=> epoch 0.5167079017668316 | train loss 0.6490224600 | val loss 0.6497316957:
	--> training ...
	--> checkpointing ...

2024-10-28_12-14 : iter     120000 <=> epoch 0.5391734627132155 | train loss 0.6485696435 | val loss 0.6483787298:
	--> training ...
	--> checkpointing ...

2024-10-28_12-35 : iter     125000 <=> epoch 0.5616390236595995 | train loss 0.6484702229 | val loss 0.6481055021:
	--> training ...
	--> checkpointing ...

2024-10-28_12-57 : iter     130000 <=> epoch 0.5841045846059835 | train loss 0.6494979262 | val loss 0.6488080621:
	--> training ...
	--> checkpointing ...

2024-10-28_13-18 : iter     135000 <=> epoch 0.6065701455523674 | train loss 0.6490947604 | val loss 0.6484000683:
	--> training ...
	--> checkpointing ...

2024-10-28_13-39 : iter     140000 <=> epoch 0.6290357064987514 | train loss 0.6483890414 | val loss 0.6489185095:
	--> training ...
	--> checkpointing ...

2024-10-28_14-01 : iter     145000 <=> epoch 0.6515012674451354 | train loss 0.6477532387 | val loss 0.6476215124:
	--> training ...
	--> checkpointing ...

2024-10-28_14-22 : iter     150000 <=> epoch 0.6739668283915193 | train loss 0.6474087238 | val loss 0.6488000154:
	--> training ...
	--> checkpointing ...

2024-10-28_14-44 : iter     155000 <=> epoch 0.6964323893379033 | train loss 0.6469108462 | val loss 0.6466912627:
	--> training ...
	--> checkpointing ...

2024-10-28_15-05 : iter     160000 <=> epoch 0.7188979502842874 | train loss 0.6434090734 | val loss 0.6435931325:
	--> training ...
	--> checkpointing ...

2024-10-28_15-27 : iter     165000 <=> epoch 0.7413635112306713 | train loss 0.6434345841 | val loss 0.6445385218:
	--> training ...
	--> checkpointing ...

2024-10-28_15-48 : iter     170000 <=> epoch 0.7638290721770553 | train loss 0.6427398324 | val loss 0.6434532404:
	--> training ...
	--> checkpointing ...

2024-10-28_16-09 : iter     175000 <=> epoch 0.7862946331234393 | train loss 0.6430800557 | val loss 0.6425195336:
	--> training ...
	--> checkpointing ...

2024-10-28_16-31 : iter     180000 <=> epoch 0.8087601940698232 | train loss 0.6422967315 | val loss 0.6427995563:
	--> training ...
	--> checkpointing ...

2024-10-28_16-52 : iter     185000 <=> epoch 0.8312257550162072 | train loss 0.6423735619 | val loss 0.6432263255:
	--> training ...
	--> checkpointing ...

2024-10-28_17-14 : iter     190000 <=> epoch 0.8536913159625912 | train loss 0.6427468061 | val loss 0.6429334879:
	--> training ...
	--> checkpointing ...

2024-10-28_17-35 : iter     195000 <=> epoch 0.8761568769089751 | train loss 0.6412540078 | val loss 0.6429275274:
	--> training ...
	--> checkpointing ...

2024-10-28_17-57 : iter     200000 <=> epoch 0.8986224378553591 | train loss 0.6438486576 | val loss 0.6419634819:
	--> training ...
	--> checkpointing ...

2024-10-28_18-18 : iter     205000 <=> epoch 0.9210879988017432 | train loss 0.6429290771 | val loss 0.6417877674:
	--> training ...
	--> checkpointing ...

2024-10-28_18-39 : iter     210000 <=> epoch 0.9435535597481272 | train loss 0.6416009068 | val loss 0.6423866749:
	--> training ...
	--> checkpointing ...

2024-10-28_19-01 : iter     215000 <=> epoch 0.9660191206945111 | train loss 0.6433781981 | val loss 0.6429209113:
	--> training ...
	--> checkpointing ...

2024-10-28_19-22 : iter     220000 <=> epoch 0.9884846816408951 | train loss 0.6417273283 | val loss 0.6426811814:
	--> training ...
