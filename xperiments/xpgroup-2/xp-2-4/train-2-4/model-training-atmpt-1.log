|ITERS: 240818 / 240818 | COMP: 100.00% | RATE: 4.20 it./s | SPD: 0.2382 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 12.41 it./s | SPD: 0.0806 s/it.| ERT: (0, 0, 0, 0) |                                                                                                      

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:5.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 2)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 240818

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 30M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-23_12-33 : iter     0 <=> epoch 0 | train loss 4.1460 | val loss 4.1458:
	--> training ...
	--> checkpointing ...

2024-10-23_12-54 : iter       5000 <=> epoch 0.020762526678703094 | train loss 0.6920604706 | val loss 0.6925544143:
	--> training ...
	--> checkpointing ...

2024-10-23_13-15 : iter      10000 <=> epoch 0.04152505335740619 | train loss 0.6703716516 | val loss 0.6711937785:
	--> training ...
	--> checkpointing ...

2024-10-23_13-36 : iter      15000 <=> epoch 0.062287580036109284 | train loss 0.6575380564 | val loss 0.6574851871:
	--> training ...
	--> checkpointing ...

2024-10-23_13-57 : iter      20000 <=> epoch 0.08305010671481237 | train loss 0.6533005238 | val loss 0.6527677178:
	--> training ...
	--> checkpointing ...

2024-10-23_14-18 : iter      25000 <=> epoch 0.10381263339351547 | train loss 0.6477628350 | val loss 0.6481024623:
	--> training ...
	--> checkpointing ...

2024-10-23_14-39 : iter      30000 <=> epoch 0.12457516007221857 | train loss 0.6451824307 | val loss 0.6434707642:
	--> training ...
	--> checkpointing ...

2024-10-23_15-01 : iter      35000 <=> epoch 0.14533768675092165 | train loss 0.6400465369 | val loss 0.6410101652:
	--> training ...
	--> checkpointing ...

2024-10-23_15-22 : iter      40000 <=> epoch 0.16610021342962475 | train loss 0.6370013356 | val loss 0.6393617988:
	--> training ...
	--> checkpointing ...

2024-10-23_15-43 : iter      45000 <=> epoch 0.18686274010832785 | train loss 0.6364605427 | val loss 0.6356153488:
	--> training ...
	--> checkpointing ...

2024-10-23_16-04 : iter      50000 <=> epoch 0.20762526678703094 | train loss 0.6332504153 | val loss 0.6330412030:
	--> training ...
	--> checkpointing ...

2024-10-23_16-25 : iter      55000 <=> epoch 0.22838779346573404 | train loss 0.6317309141 | val loss 0.6321984529:
	--> training ...
	--> checkpointing ...

2024-10-23_16-46 : iter      60000 <=> epoch 0.24915032014443714 | train loss 0.6313681006 | val loss 0.6327388287:
	--> training ...
	--> checkpointing ...

2024-10-23_17-07 : iter      65000 <=> epoch 0.2699128468231402 | train loss 0.6298708916 | val loss 0.6307889223:
	--> training ...
	--> checkpointing ...

2024-10-23_17-29 : iter      70000 <=> epoch 0.2906753735018433 | train loss 0.6303943992 | val loss 0.6297036409:
	--> training ...
	--> checkpointing ...

2024-10-23_17-50 : iter      75000 <=> epoch 0.3114379001805464 | train loss 0.6299184561 | val loss 0.6289713979:
	--> training ...
	--> checkpointing ...

2024-10-23_18-11 : iter      80000 <=> epoch 0.3322004268592495 | train loss 0.6284409165 | val loss 0.6301456690:
	--> training ...
	--> checkpointing ...

2024-10-23_18-32 : iter      85000 <=> epoch 0.3529629535379526 | train loss 0.6284360886 | val loss 0.6276749372:
	--> training ...
	--> checkpointing ...

2024-10-23_18-53 : iter      90000 <=> epoch 0.3737254802166557 | train loss 0.6265219450 | val loss 0.6274778843:
	--> training ...
	--> checkpointing ...

2024-10-23_19-14 : iter      95000 <=> epoch 0.3944880068953588 | train loss 0.6255089045 | val loss 0.6269767284:
	--> training ...
	--> checkpointing ...

2024-10-23_19-35 : iter     100000 <=> epoch 0.4152505335740619 | train loss 0.6249564290 | val loss 0.6253730655:
	--> training ...
	--> checkpointing ...

2024-10-23_19-57 : iter     105000 <=> epoch 0.436013060252765 | train loss 0.6234888434 | val loss 0.6250442863:
	--> training ...
	--> checkpointing ...

2024-10-23_20-18 : iter     110000 <=> epoch 0.4567755869314681 | train loss 0.6242553592 | val loss 0.6243102551:
	--> training ...
	--> checkpointing ...

2024-10-23_20-39 : iter     115000 <=> epoch 0.4775381136101712 | train loss 0.6242369413 | val loss 0.6253572702:
	--> training ...
	--> checkpointing ...

2024-10-23_21-00 : iter     120000 <=> epoch 0.4983006402888743 | train loss 0.6232919693 | val loss 0.6245094538:
	--> training ...
	--> checkpointing ...

2024-10-23_21-21 : iter     125000 <=> epoch 0.5190631669675774 | train loss 0.6240562201 | val loss 0.6244764328:
	--> training ...
	--> checkpointing ...

2024-10-23_21-42 : iter     130000 <=> epoch 0.5398256936462804 | train loss 0.6215372682 | val loss 0.6241164207:
	--> training ...
	--> checkpointing ...

2024-10-23_22-03 : iter     135000 <=> epoch 0.5605882203249836 | train loss 0.6232637167 | val loss 0.6231704950:
	--> training ...
	--> checkpointing ...

2024-10-23_22-25 : iter     140000 <=> epoch 0.5813507470036866 | train loss 0.6218942404 | val loss 0.6229361296:
	--> training ...
	--> checkpointing ...

2024-10-23_22-46 : iter     145000 <=> epoch 0.6021132736823898 | train loss 0.6212882400 | val loss 0.6231527328:
	--> training ...
	--> checkpointing ...

2024-10-23_23-07 : iter     150000 <=> epoch 0.6228758003610928 | train loss 0.6214985847 | val loss 0.6239913106:
	--> training ...
	--> checkpointing ...

2024-10-23_23-28 : iter     155000 <=> epoch 0.643638327039796 | train loss 0.6228553057 | val loss 0.6208238602:
	--> training ...
	--> checkpointing ...

2024-10-23_23-49 : iter     160000 <=> epoch 0.664400853718499 | train loss 0.6222089529 | val loss 0.6213889718:
	--> training ...
	--> checkpointing ...

2024-10-24_00-10 : iter     165000 <=> epoch 0.6851633803972021 | train loss 0.6217066646 | val loss 0.6221051812:
	--> training ...
	--> checkpointing ...

2024-10-24_00-32 : iter     170000 <=> epoch 0.7059259070759052 | train loss 0.6168401837 | val loss 0.6184995770:
	--> training ...
	--> checkpointing ...

2024-10-24_00-53 : iter     175000 <=> epoch 0.7266884337546083 | train loss 0.6168627739 | val loss 0.6172224283:
	--> training ...
	--> checkpointing ...

2024-10-24_01-14 : iter     180000 <=> epoch 0.7474509604333114 | train loss 0.6164953113 | val loss 0.6166937947:
	--> training ...
	--> checkpointing ...

2024-10-24_01-35 : iter     185000 <=> epoch 0.7682134871120145 | train loss 0.6158713102 | val loss 0.6154785156:
	--> training ...
	--> checkpointing ...

2024-10-24_01-56 : iter     190000 <=> epoch 0.7889760137907176 | train loss 0.6154952645 | val loss 0.6167188883:
	--> training ...
	--> checkpointing ...

2024-10-24_02-17 : iter     195000 <=> epoch 0.8097385404694207 | train loss 0.6146606207 | val loss 0.6145206094:
	--> training ...
	--> checkpointing ...

2024-10-24_02-38 : iter     200000 <=> epoch 0.8305010671481238 | train loss 0.6145421267 | val loss 0.6157138944:
	--> training ...
	--> checkpointing ...

2024-10-24_03-00 : iter     205000 <=> epoch 0.8512635938268268 | train loss 0.6151458621 | val loss 0.6150428653:
	--> training ...
	--> checkpointing ...

2024-10-24_03-21 : iter     210000 <=> epoch 0.87202612050553 | train loss 0.6148816943 | val loss 0.6148551106:
	--> training ...
	--> checkpointing ...

2024-10-24_03-42 : iter     215000 <=> epoch 0.892788647184233 | train loss 0.6154739857 | val loss 0.6135210395:
	--> training ...
	--> checkpointing ...

2024-10-24_04-03 : iter     220000 <=> epoch 0.9135511738629362 | train loss 0.6140800714 | val loss 0.6138259172:
	--> training ...
	--> checkpointing ...

2024-10-24_04-24 : iter     225000 <=> epoch 0.9343137005416392 | train loss 0.6148873568 | val loss 0.6138611436:
	--> training ...
	--> checkpointing ...

2024-10-24_04-45 : iter     230000 <=> epoch 0.9550762272203424 | train loss 0.6151487827 | val loss 0.6140061617:
	--> training ...
	--> checkpointing ...

2024-10-24_05-06 : iter     235000 <=> epoch 0.9758387538990454 | train loss 0.6148475409 | val loss 0.6144685149:
	--> training ...
	--> checkpointing ...

2024-10-24_05-28 : iter     240000 <=> epoch 0.9966012805777485 | train loss 0.6139578819 | val loss 0.6158028841:
	--> training ...
