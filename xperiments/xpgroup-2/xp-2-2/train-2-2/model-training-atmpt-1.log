|ITERS: 240818 / 240818 | COMP: 100.00% | RATE: 3.05 it./s | SPD: 0.3282 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 9.38 it./s | SPD: 0.1067 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:1.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 2)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 240818

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 41M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-23_12-19 : iter     0 <=> epoch 0 | train loss 4.1594 | val loss 4.1593:
	--> training ...
	--> checkpointing ...

2024-10-23_12-50 : iter       5000 <=> epoch 0.020762526678703094 | train loss 0.6895871162 | val loss 0.6913504004:
	--> training ...
	--> checkpointing ...

2024-10-23_13-19 : iter      10000 <=> epoch 0.04152505335740619 | train loss 0.6660057306 | val loss 0.6659474373:
	--> training ...
	--> checkpointing ...

2024-10-23_13-48 : iter      15000 <=> epoch 0.062287580036109284 | train loss 0.6544653177 | val loss 0.6534036398:
	--> training ...
	--> checkpointing ...

2024-10-23_14-17 : iter      20000 <=> epoch 0.08305010671481237 | train loss 0.6452970505 | val loss 0.6457032561:
	--> training ...
	--> checkpointing ...

2024-10-23_14-47 : iter      25000 <=> epoch 0.10381263339351547 | train loss 0.6403806806 | val loss 0.6390334368:
	--> training ...
	--> checkpointing ...

2024-10-23_15-16 : iter      30000 <=> epoch 0.12457516007221857 | train loss 0.6356933117 | val loss 0.6363101602:
	--> training ...
	--> checkpointing ...

2024-10-23_15-45 : iter      35000 <=> epoch 0.14533768675092165 | train loss 0.6325584054 | val loss 0.6325123906:
	--> training ...
	--> checkpointing ...

2024-10-23_16-14 : iter      40000 <=> epoch 0.16610021342962475 | train loss 0.6316883564 | val loss 0.6317589283:
	--> training ...
	--> checkpointing ...

2024-10-23_16-43 : iter      45000 <=> epoch 0.18686274010832785 | train loss 0.6296707988 | val loss 0.6283941865:
	--> training ...
	--> checkpointing ...

2024-10-23_17-13 : iter      50000 <=> epoch 0.20762526678703094 | train loss 0.6273127198 | val loss 0.6288852692:
	--> training ...
	--> checkpointing ...

2024-10-23_17-42 : iter      55000 <=> epoch 0.22838779346573404 | train loss 0.6268542409 | val loss 0.6260983348:
	--> training ...
	--> checkpointing ...

2024-10-23_18-11 : iter      60000 <=> epoch 0.24915032014443714 | train loss 0.6260020733 | val loss 0.6267320514:
	--> training ...
	--> checkpointing ...

2024-10-23_18-40 : iter      65000 <=> epoch 0.2699128468231402 | train loss 0.6261286736 | val loss 0.6255615950:
	--> training ...
	--> checkpointing ...

2024-10-23_19-09 : iter      70000 <=> epoch 0.2906753735018433 | train loss 0.6234614253 | val loss 0.6230926514:
	--> training ...
	--> checkpointing ...

2024-10-23_19-38 : iter      75000 <=> epoch 0.3114379001805464 | train loss 0.6239543557 | val loss 0.6239270568:
	--> training ...
	--> checkpointing ...

2024-10-23_20-08 : iter      80000 <=> epoch 0.3322004268592495 | train loss 0.6228189468 | val loss 0.6215505004:
	--> training ...
	--> checkpointing ...

2024-10-23_20-37 : iter      85000 <=> epoch 0.3529629535379526 | train loss 0.6224217415 | val loss 0.6218309402:
	--> training ...
	--> checkpointing ...

2024-10-23_21-06 : iter      90000 <=> epoch 0.3737254802166557 | train loss 0.6221428514 | val loss 0.6233903766:
	--> training ...
	--> checkpointing ...

2024-10-23_21-35 : iter      95000 <=> epoch 0.3944880068953588 | train loss 0.6214038134 | val loss 0.6216919422:
	--> training ...
	--> checkpointing ...

2024-10-23_22-04 : iter     100000 <=> epoch 0.4152505335740619 | train loss 0.6203609109 | val loss 0.6198977828:
	--> training ...
	--> checkpointing ...

2024-10-23_22-33 : iter     105000 <=> epoch 0.436013060252765 | train loss 0.6217324138 | val loss 0.6203973889:
	--> training ...
	--> checkpointing ...

2024-10-23_23-03 : iter     110000 <=> epoch 0.4567755869314681 | train loss 0.6203067899 | val loss 0.6197866201:
	--> training ...
	--> checkpointing ...

2024-10-23_23-32 : iter     115000 <=> epoch 0.4775381136101712 | train loss 0.6205646992 | val loss 0.6201847196:
	--> training ...
	--> checkpointing ...

2024-10-24_00-01 : iter     120000 <=> epoch 0.4983006402888743 | train loss 0.6199970245 | val loss 0.6191648245:
	--> training ...
	--> checkpointing ...

2024-10-24_00-30 : iter     125000 <=> epoch 0.5190631669675774 | train loss 0.6182591915 | val loss 0.6210312247:
	--> training ...
	--> checkpointing ...

2024-10-24_00-59 : iter     130000 <=> epoch 0.5398256936462804 | train loss 0.6197554469 | val loss 0.6186878681:
	--> training ...
	--> checkpointing ...

2024-10-24_01-28 : iter     135000 <=> epoch 0.5605882203249836 | train loss 0.6198928356 | val loss 0.6194045544:
	--> training ...
	--> checkpointing ...

2024-10-24_01-58 : iter     140000 <=> epoch 0.5813507470036866 | train loss 0.6194387078 | val loss 0.6192244887:
	--> training ...
	--> checkpointing ...

2024-10-24_02-27 : iter     145000 <=> epoch 0.6021132736823898 | train loss 0.6174758673 | val loss 0.6197223067:
	--> training ...
	--> checkpointing ...

2024-10-24_02-56 : iter     150000 <=> epoch 0.6228758003610928 | train loss 0.6177586913 | val loss 0.6180453300:
	--> training ...
	--> checkpointing ...

2024-10-24_03-25 : iter     155000 <=> epoch 0.643638327039796 | train loss 0.6178230047 | val loss 0.6178498864:
	--> training ...
	--> checkpointing ...

2024-10-24_03-54 : iter     160000 <=> epoch 0.664400853718499 | train loss 0.6176863909 | val loss 0.6188458800:
	--> training ...
	--> checkpointing ...

2024-10-24_04-23 : iter     165000 <=> epoch 0.6851633803972021 | train loss 0.6172772050 | val loss 0.6171121001:
	--> training ...
	--> checkpointing ...

2024-10-24_04-53 : iter     170000 <=> epoch 0.7059259070759052 | train loss 0.6141749620 | val loss 0.6154860258:
	--> training ...
	--> checkpointing ...

2024-10-24_05-22 : iter     175000 <=> epoch 0.7266884337546083 | train loss 0.6125345826 | val loss 0.6124917865:
	--> training ...
	--> checkpointing ...

2024-10-24_05-51 : iter     180000 <=> epoch 0.7474509604333114 | train loss 0.6129724383 | val loss 0.6128541827:
	--> training ...
	--> checkpointing ...

2024-10-24_06-20 : iter     185000 <=> epoch 0.7682134871120145 | train loss 0.6129142642 | val loss 0.6123244762:
	--> training ...
	--> checkpointing ...

2024-10-24_06-48 : iter     190000 <=> epoch 0.7889760137907176 | train loss 0.6116173863 | val loss 0.6112050414:
	--> training ...
	--> checkpointing ...

2024-10-24_07-17 : iter     195000 <=> epoch 0.8097385404694207 | train loss 0.6105982065 | val loss 0.6113659739:
	--> training ...
	--> checkpointing ...

2024-10-24_07-46 : iter     200000 <=> epoch 0.8305010671481238 | train loss 0.6121644378 | val loss 0.6107805371:
	--> training ...
	--> checkpointing ...

2024-10-24_08-15 : iter     205000 <=> epoch 0.8512635938268268 | train loss 0.6110091805 | val loss 0.6125823855:
	--> training ...
	--> checkpointing ...

2024-10-24_08-44 : iter     210000 <=> epoch 0.87202612050553 | train loss 0.6112054586 | val loss 0.6106138825:
	--> training ...
	--> checkpointing ...

2024-10-24_09-13 : iter     215000 <=> epoch 0.892788647184233 | train loss 0.6110286117 | val loss 0.6112875938:
	--> training ...
	--> checkpointing ...

2024-10-24_09-42 : iter     220000 <=> epoch 0.9135511738629362 | train loss 0.6107013226 | val loss 0.6119235158:
	--> training ...
	--> checkpointing ...

2024-10-24_10-11 : iter     225000 <=> epoch 0.9343137005416392 | train loss 0.6112742424 | val loss 0.6114037633:
	--> training ...
	--> checkpointing ...

2024-10-24_10-40 : iter     230000 <=> epoch 0.9550762272203424 | train loss 0.6104286313 | val loss 0.6109098196:
	--> training ...
	--> checkpointing ...

2024-10-24_11-09 : iter     235000 <=> epoch 0.9758387538990454 | train loss 0.6102007627 | val loss 0.6115077138:
	--> training ...
	--> checkpointing ...

2024-10-24_11-38 : iter     240000 <=> epoch 0.9966012805777485 | train loss 0.6113672853 | val loss 0.6122170091:
	--> training ...
