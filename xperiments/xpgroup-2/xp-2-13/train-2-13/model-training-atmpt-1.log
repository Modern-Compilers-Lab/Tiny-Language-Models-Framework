|ITERS: 222562 / 222562 | COMP: 100.00% | RATE: 1.24 it./s | SPD: 0.8064 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 4.04 it./s | SPD: 0.2478 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:4.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 222562

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 62M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-29_20-56 : iter     0 <=> epoch 0 | train loss 4.1372 | val loss 4.1375:
	--> training ...
	--> checkpointing ...

2024-10-29_22-08 : iter       5000 <=> epoch 0.02246556094638398 | train loss 0.7096591592 | val loss 0.7098611593:
	--> training ...
	--> checkpointing ...

2024-10-29_23-19 : iter      10000 <=> epoch 0.04493112189276796 | train loss 0.6873041987 | val loss 0.6875835061:
	--> training ...
	--> checkpointing ...

2024-10-30_00-31 : iter      15000 <=> epoch 0.06739668283915194 | train loss 0.6760886908 | val loss 0.6768436432:
	--> training ...
	--> checkpointing ...

2024-10-30_01-43 : iter      20000 <=> epoch 0.08986224378553592 | train loss 0.6687821746 | val loss 0.6694070697:
	--> training ...
	--> checkpointing ...

2024-10-30_02-54 : iter      25000 <=> epoch 0.11232780473191989 | train loss 0.6641378403 | val loss 0.6655649543:
	--> training ...
	--> checkpointing ...

2024-10-30_04-06 : iter      30000 <=> epoch 0.13479336567830388 | train loss 0.6606472731 | val loss 0.6604610682:
	--> training ...
	--> checkpointing ...

2024-10-30_05-17 : iter      35000 <=> epoch 0.15725892662468785 | train loss 0.6559962034 | val loss 0.6574429870:
	--> training ...
	--> checkpointing ...

2024-10-30_06-28 : iter      40000 <=> epoch 0.17972448757107184 | train loss 0.6557403803 | val loss 0.6553766131:
	--> training ...
	--> checkpointing ...

2024-10-30_07-40 : iter      45000 <=> epoch 0.2021900485174558 | train loss 0.6527603865 | val loss 0.6533839107:
	--> training ...
	--> checkpointing ...

2024-10-30_08-51 : iter      50000 <=> epoch 0.22465560946383978 | train loss 0.6513087153 | val loss 0.6521319151:
	--> training ...
	--> checkpointing ...

2024-10-30_10-03 : iter      55000 <=> epoch 0.24712117041022377 | train loss 0.6510685682 | val loss 0.6511941552:
	--> training ...
	--> checkpointing ...

2024-10-30_11-14 : iter      60000 <=> epoch 0.26958673135660777 | train loss 0.6489155293 | val loss 0.6498900056:
	--> training ...
	--> checkpointing ...

2024-10-30_12-25 : iter      65000 <=> epoch 0.29205229230299173 | train loss 0.6487916708 | val loss 0.6492868066:
	--> training ...
	--> checkpointing ...

2024-10-30_13-37 : iter      70000 <=> epoch 0.3145178532493757 | train loss 0.6494488120 | val loss 0.6470561624:
	--> training ...
	--> checkpointing ...

2024-10-30_14-48 : iter      75000 <=> epoch 0.33698341419575967 | train loss 0.6477405429 | val loss 0.6480110884:
	--> training ...
	--> checkpointing ...

2024-10-30_16-00 : iter      80000 <=> epoch 0.3594489751421437 | train loss 0.6469397545 | val loss 0.6474815607:
	--> training ...
	--> checkpointing ...

2024-10-30_17-11 : iter      85000 <=> epoch 0.38191453608852766 | train loss 0.6475487351 | val loss 0.6462994218:
	--> training ...
	--> checkpointing ...

2024-10-30_18-22 : iter      90000 <=> epoch 0.4043800970349116 | train loss 0.6450003386 | val loss 0.6464397907:
	--> training ...
	--> checkpointing ...

2024-10-30_19-34 : iter      95000 <=> epoch 0.4268456579812956 | train loss 0.6468223929 | val loss 0.6458969116:
	--> training ...
	--> checkpointing ...

2024-10-30_20-45 : iter     100000 <=> epoch 0.44931121892767956 | train loss 0.6453453302 | val loss 0.6457664967:
	--> training ...
	--> checkpointing ...

2024-10-30_21-56 : iter     105000 <=> epoch 0.4717767798740636 | train loss 0.6446705461 | val loss 0.6450635195:
	--> training ...
	--> checkpointing ...

2024-10-30_23-08 : iter     110000 <=> epoch 0.49424234082044755 | train loss 0.6450385451 | val loss 0.6451430917:
	--> training ...
	--> checkpointing ...

2024-10-31_00-19 : iter     115000 <=> epoch 0.5167079017668316 | train loss 0.6446802616 | val loss 0.6452133060:
	--> training ...
	--> checkpointing ...

2024-10-31_01-31 : iter     120000 <=> epoch 0.5391734627132155 | train loss 0.6451411247 | val loss 0.6444706917:
	--> training ...
	--> checkpointing ...

2024-10-31_02-42 : iter     125000 <=> epoch 0.5616390236595995 | train loss 0.6426822543 | val loss 0.6439921856:
	--> training ...
	--> checkpointing ...

2024-10-31_03-53 : iter     130000 <=> epoch 0.5841045846059835 | train loss 0.6443560123 | val loss 0.6446772814:
	--> training ...
	--> checkpointing ...

2024-10-31_05-05 : iter     135000 <=> epoch 0.6065701455523674 | train loss 0.6420100927 | val loss 0.6425656080:
	--> training ...
	--> checkpointing ...

2024-10-31_06-16 : iter     140000 <=> epoch 0.6290357064987514 | train loss 0.6441180706 | val loss 0.6435732245:
	--> training ...
	--> checkpointing ...

2024-10-31_07-27 : iter     145000 <=> epoch 0.6515012674451354 | train loss 0.6430207491 | val loss 0.6442613602:
	--> training ...
	--> checkpointing ...

2024-10-31_08-39 : iter     150000 <=> epoch 0.6739668283915193 | train loss 0.6429406404 | val loss 0.6436897516:
	--> training ...
	--> checkpointing ...

2024-10-31_09-50 : iter     155000 <=> epoch 0.6964323893379033 | train loss 0.6428484917 | val loss 0.6435945034:
	--> training ...
	--> checkpointing ...

2024-10-31_11-02 : iter     160000 <=> epoch 0.7188979502842874 | train loss 0.6403242350 | val loss 0.6402080059:
	--> training ...
	--> checkpointing ...

2024-10-31_12-13 : iter     165000 <=> epoch 0.7413635112306713 | train loss 0.6394650340 | val loss 0.6387050748:
	--> training ...
	--> checkpointing ...

2024-10-31_13-24 : iter     170000 <=> epoch 0.7638290721770553 | train loss 0.6387394071 | val loss 0.6383008361:
	--> training ...
	--> checkpointing ...

2024-10-31_14-36 : iter     175000 <=> epoch 0.7862946331234393 | train loss 0.6381446719 | val loss 0.6409134865:
	--> training ...
	--> checkpointing ...

2024-10-31_15-47 : iter     180000 <=> epoch 0.8087601940698232 | train loss 0.6376476884 | val loss 0.6395146847:
	--> training ...
	--> checkpointing ...

2024-10-31_16-58 : iter     185000 <=> epoch 0.8312257550162072 | train loss 0.6377387643 | val loss 0.6385334730:
	--> training ...
	--> checkpointing ...

2024-10-31_18-10 : iter     190000 <=> epoch 0.8536913159625912 | train loss 0.6387039423 | val loss 0.6384375095:
	--> training ...
	--> checkpointing ...

2024-10-31_19-21 : iter     195000 <=> epoch 0.8761568769089751 | train loss 0.6385239363 | val loss 0.6378068924:
	--> training ...
	--> checkpointing ...

2024-10-31_20-33 : iter     200000 <=> epoch 0.8986224378553591 | train loss 0.6384519935 | val loss 0.6385931969:
	--> training ...
	--> checkpointing ...

2024-10-31_21-44 : iter     205000 <=> epoch 0.9210879988017432 | train loss 0.6390765309 | val loss 0.6383231878:
	--> training ...
	--> checkpointing ...

2024-10-31_22-55 : iter     210000 <=> epoch 0.9435535597481272 | train loss 0.6377226710 | val loss 0.6381749511:
	--> training ...
	--> checkpointing ...

2024-11-01_00-07 : iter     215000 <=> epoch 0.9660191206945111 | train loss 0.6374966502 | val loss 0.6382856965:
	--> training ...
	--> checkpointing ...

2024-11-01_01-18 : iter     220000 <=> epoch 0.9884846816408951 | train loss 0.6402237415 | val loss 0.6385657787:
	--> training ...
