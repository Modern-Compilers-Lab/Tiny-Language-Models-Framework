|ITERS: 445157 / 445157 | COMP: 100.00% | RATE: 1.80 it./s | SPD: 0.5568 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 5.67 it./s | SPD: 0.1764 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:0.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 4)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 445157

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 61M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-31_19-09 : iter     0 <=> epoch 0 | train loss 4.1515 | val loss 4.1516:
	--> training ...
	--> checkpointing ...

2024-10-31_19-58 : iter       5000 <=> epoch 0.011231967763699694 | train loss 0.7222072482 | val loss 0.7219173312:
	--> training ...
	--> checkpointing ...

2024-10-31_20-47 : iter      10000 <=> epoch 0.02246393552739939 | train loss 0.6928371191 | val loss 0.6929425597:
	--> training ...
	--> checkpointing ...

2024-10-31_21-37 : iter      15000 <=> epoch 0.03369590329109908 | train loss 0.6780766845 | val loss 0.6784623861:
	--> training ...
	--> checkpointing ...

2024-10-31_22-26 : iter      20000 <=> epoch 0.04492787105479878 | train loss 0.6710137129 | val loss 0.6705416441:
	--> training ...
	--> checkpointing ...

2024-10-31_23-15 : iter      25000 <=> epoch 0.056159838818498474 | train loss 0.6660917997 | val loss 0.6668026447:
	--> training ...
	--> checkpointing ...

2024-11-01_00-04 : iter      30000 <=> epoch 0.06739180658219816 | train loss 0.6627578735 | val loss 0.6612927318:
	--> training ...
	--> checkpointing ...

2024-11-01_00-53 : iter      35000 <=> epoch 0.07862377434589786 | train loss 0.6590887308 | val loss 0.6597847939:
	--> training ...
	--> checkpointing ...

2024-11-01_01-42 : iter      40000 <=> epoch 0.08985574210959756 | train loss 0.6567767859 | val loss 0.6593577862:
	--> training ...
	--> checkpointing ...

2024-11-01_02-31 : iter      45000 <=> epoch 0.10108770987329725 | train loss 0.6562186480 | val loss 0.6563633084:
	--> training ...
	--> checkpointing ...

2024-11-01_03-21 : iter      50000 <=> epoch 0.11231967763699695 | train loss 0.6548537612 | val loss 0.6542229056:
	--> training ...
	--> checkpointing ...

2024-11-01_04-10 : iter      55000 <=> epoch 0.12355164540069664 | train loss 0.6526090503 | val loss 0.6538289785:
	--> training ...
	--> checkpointing ...

2024-11-01_04-59 : iter      60000 <=> epoch 0.13478361316439633 | train loss 0.6516382694 | val loss 0.6523310542:
	--> training ...
	--> checkpointing ...

2024-11-01_05-48 : iter      65000 <=> epoch 0.14601558092809602 | train loss 0.6521490216 | val loss 0.6533080935:
	--> training ...
	--> checkpointing ...

2024-11-01_06-37 : iter      70000 <=> epoch 0.15724754869179572 | train loss 0.6515389681 | val loss 0.6514736414:
	--> training ...
	--> checkpointing ...

2024-11-01_07-26 : iter      75000 <=> epoch 0.16847951645549541 | train loss 0.6515219808 | val loss 0.6512357593:
	--> training ...
	--> checkpointing ...

2024-11-01_08-15 : iter      80000 <=> epoch 0.1797114842191951 | train loss 0.6514735818 | val loss 0.6511944532:
	--> training ...
	--> checkpointing ...

2024-11-01_09-05 : iter      85000 <=> epoch 0.1909434519828948 | train loss 0.6503355503 | val loss 0.6499071717:
	--> training ...
	--> checkpointing ...

2024-11-01_09-54 : iter      90000 <=> epoch 0.2021754197465945 | train loss 0.6497734189 | val loss 0.6497271657:
	--> training ...
	--> checkpointing ...

2024-11-01_10-43 : iter      95000 <=> epoch 0.2134073875102942 | train loss 0.6493133903 | val loss 0.6492877603:
	--> training ...
	--> checkpointing ...

2024-11-01_11-33 : iter     100000 <=> epoch 0.2246393552739939 | train loss 0.6480265260 | val loss 0.6493937373:
	--> training ...
	--> checkpointing ...

2024-11-01_12-22 : iter     105000 <=> epoch 0.2358713230376936 | train loss 0.6490722895 | val loss 0.6482956409:
	--> training ...
	--> checkpointing ...

2024-11-01_13-11 : iter     110000 <=> epoch 0.2471032908013933 | train loss 0.6474223137 | val loss 0.6489544511:
	--> training ...
	--> checkpointing ...

2024-11-01_14-01 : iter     115000 <=> epoch 0.258335258565093 | train loss 0.6478954554 | val loss 0.6475235224:
	--> training ...
	--> checkpointing ...

2024-11-01_14-50 : iter     120000 <=> epoch 0.26956722632879265 | train loss 0.6471573710 | val loss 0.6472582221:
	--> training ...
	--> checkpointing ...

2024-11-01_15-39 : iter     125000 <=> epoch 0.2807991940924924 | train loss 0.6474322081 | val loss 0.6464626789:
	--> training ...
	--> checkpointing ...

2024-11-01_16-29 : iter     130000 <=> epoch 0.29203116185619205 | train loss 0.6471728683 | val loss 0.6476036310:
	--> training ...
	--> checkpointing ...

2024-11-01_17-18 : iter     135000 <=> epoch 0.30326312961989177 | train loss 0.6462560892 | val loss 0.6469001174:
	--> training ...
	--> checkpointing ...

2024-11-01_18-07 : iter     140000 <=> epoch 0.31449509738359144 | train loss 0.6463893652 | val loss 0.6457822919:
	--> training ...
	--> checkpointing ...

2024-11-01_18-57 : iter     145000 <=> epoch 0.32572706514729116 | train loss 0.6460101604 | val loss 0.6463469863:
	--> training ...
	--> checkpointing ...

2024-11-01_19-46 : iter     150000 <=> epoch 0.33695903291099083 | train loss 0.6462196112 | val loss 0.6459119320:
	--> training ...
	--> checkpointing ...

2024-11-01_20-35 : iter     155000 <=> epoch 0.34819100067469055 | train loss 0.6459271312 | val loss 0.6453768015:
	--> training ...
	--> checkpointing ...

2024-11-01_21-24 : iter     160000 <=> epoch 0.3594229684383902 | train loss 0.6461628675 | val loss 0.6463541389:
	--> training ...
	--> checkpointing ...

2024-11-01_22-14 : iter     165000 <=> epoch 0.37065493620208995 | train loss 0.6459908485 | val loss 0.6461940408:
	--> training ...
	--> checkpointing ...

2024-11-01_23-03 : iter     170000 <=> epoch 0.3818869039657896 | train loss 0.6457152367 | val loss 0.6459259987:
	--> training ...
	--> checkpointing ...

2024-11-01_23-52 : iter     175000 <=> epoch 0.39311887172948934 | train loss 0.6448753476 | val loss 0.6453887820:
	--> training ...
	--> checkpointing ...

2024-11-02_00-41 : iter     180000 <=> epoch 0.404350839493189 | train loss 0.6453052759 | val loss 0.6461658478:
	--> training ...
	--> checkpointing ...

2024-11-02_01-31 : iter     185000 <=> epoch 0.41558280725688873 | train loss 0.6457963586 | val loss 0.6461336613:
	--> training ...
	--> checkpointing ...

2024-11-02_02-20 : iter     190000 <=> epoch 0.4268147750205884 | train loss 0.6460633874 | val loss 0.6453525424:
	--> training ...
	--> checkpointing ...

2024-11-02_03-09 : iter     195000 <=> epoch 0.4380467427842881 | train loss 0.6455124617 | val loss 0.6458857656:
	--> training ...
	--> checkpointing ...

2024-11-02_03-58 : iter     200000 <=> epoch 0.4492787105479878 | train loss 0.6436963081 | val loss 0.6440072656:
	--> training ...
	--> checkpointing ...

2024-11-02_04-48 : iter     205000 <=> epoch 0.46051067831168746 | train loss 0.6449258924 | val loss 0.6448431015:
	--> training ...
	--> checkpointing ...

2024-11-02_05-37 : iter     210000 <=> epoch 0.4717426460753872 | train loss 0.6454824209 | val loss 0.6429069042:
	--> training ...
	--> checkpointing ...

2024-11-02_06-26 : iter     215000 <=> epoch 0.48297461383908685 | train loss 0.6447809339 | val loss 0.6445905566:
	--> training ...
	--> checkpointing ...

2024-11-02_07-15 : iter     220000 <=> epoch 0.4942065816027866 | train loss 0.6440051794 | val loss 0.6450084448:
	--> training ...
	--> checkpointing ...

2024-11-02_08-04 : iter     225000 <=> epoch 0.5054385493664862 | train loss 0.6449794769 | val loss 0.6444308758:
	--> training ...
	--> checkpointing ...

2024-11-02_08-54 : iter     230000 <=> epoch 0.516670517130186 | train loss 0.6434892416 | val loss 0.6451205015:
	--> training ...
	--> checkpointing ...

2024-11-02_09-43 : iter     235000 <=> epoch 0.5279024848938857 | train loss 0.6433100700 | val loss 0.6435006261:
	--> training ...
	--> checkpointing ...

2024-11-02_10-32 : iter     240000 <=> epoch 0.5391344526575853 | train loss 0.6441597939 | val loss 0.6446037889:
	--> training ...
	--> checkpointing ...

2024-11-02_11-21 : iter     245000 <=> epoch 0.550366420421285 | train loss 0.6434998512 | val loss 0.6443188190:
	--> training ...
	--> checkpointing ...

2024-11-02_12-11 : iter     250000 <=> epoch 0.5615983881849848 | train loss 0.6439381242 | val loss 0.6447166204:
	--> training ...
	--> checkpointing ...

2024-11-02_13-00 : iter     255000 <=> epoch 0.5728303559486845 | train loss 0.6445956826 | val loss 0.6445765495:
	--> training ...
	--> checkpointing ...

2024-11-02_13-49 : iter     260000 <=> epoch 0.5840623237123841 | train loss 0.6435292959 | val loss 0.6456853747:
	--> training ...
	--> checkpointing ...

2024-11-02_14-38 : iter     265000 <=> epoch 0.5952942914760838 | train loss 0.6442447305 | val loss 0.6428173780:
	--> training ...
	--> checkpointing ...

2024-11-02_15-28 : iter     270000 <=> epoch 0.6065262592397835 | train loss 0.6429806352 | val loss 0.6431500316:
	--> training ...
	--> checkpointing ...

2024-11-02_16-17 : iter     275000 <=> epoch 0.6177582270034833 | train loss 0.6444725394 | val loss 0.6440501809:
	--> training ...
	--> checkpointing ...

2024-11-02_17-06 : iter     280000 <=> epoch 0.6289901947671829 | train loss 0.6434944272 | val loss 0.6430964470:
	--> training ...
	--> checkpointing ...

2024-11-02_17-55 : iter     285000 <=> epoch 0.6402221625308826 | train loss 0.6428856850 | val loss 0.6444455385:
	--> training ...
	--> checkpointing ...

2024-11-02_18-45 : iter     290000 <=> epoch 0.6514541302945823 | train loss 0.6433521509 | val loss 0.6429587007:
	--> training ...
	--> checkpointing ...

2024-11-02_19-34 : iter     295000 <=> epoch 0.6626860980582819 | train loss 0.6420253515 | val loss 0.6444171071:
	--> training ...
	--> checkpointing ...

2024-11-02_20-23 : iter     300000 <=> epoch 0.6739180658219817 | train loss 0.6434171200 | val loss 0.6439858675:
	--> training ...
	--> checkpointing ...

2024-11-02_21-12 : iter     305000 <=> epoch 0.6851500335856814 | train loss 0.6434630752 | val loss 0.6439726949:
	--> training ...
	--> checkpointing ...

2024-11-02_22-02 : iter     310000 <=> epoch 0.6963820013493811 | train loss 0.6436163783 | val loss 0.6432881951:
	--> training ...
	--> checkpointing ...

2024-11-02_22-51 : iter     315000 <=> epoch 0.7076139691130807 | train loss 0.6401050091 | val loss 0.6404649019:
	--> training ...
	--> checkpointing ...

2024-11-02_23-40 : iter     320000 <=> epoch 0.7188459368767804 | train loss 0.6393947005 | val loss 0.6402849555:
	--> training ...
	--> checkpointing ...

2024-11-03_00-29 : iter     325000 <=> epoch 0.7300779046404802 | train loss 0.6384702325 | val loss 0.6384364963:
	--> training ...
	--> checkpointing ...

2024-11-03_01-19 : iter     330000 <=> epoch 0.7413098724041799 | train loss 0.6394796968 | val loss 0.6389549375:
	--> training ...
	--> checkpointing ...

2024-11-03_02-08 : iter     335000 <=> epoch 0.7525418401678795 | train loss 0.6398004889 | val loss 0.6387890577:
	--> training ...
	--> checkpointing ...

2024-11-03_02-57 : iter     340000 <=> epoch 0.7637738079315792 | train loss 0.6384165883 | val loss 0.6376116872:
	--> training ...
	--> checkpointing ...

2024-11-03_03-46 : iter     345000 <=> epoch 0.775005775695279 | train loss 0.6381533146 | val loss 0.6382511854:
	--> training ...
	--> checkpointing ...

2024-11-03_04-36 : iter     350000 <=> epoch 0.7862377434589787 | train loss 0.6378031969 | val loss 0.6391500831:
	--> training ...
	--> checkpointing ...

2024-11-03_05-25 : iter     355000 <=> epoch 0.7974697112226783 | train loss 0.6393996477 | val loss 0.6385644674:
	--> training ...
	--> checkpointing ...

2024-11-03_06-14 : iter     360000 <=> epoch 0.808701678986378 | train loss 0.6385009885 | val loss 0.6381595135:
	--> training ...
	--> checkpointing ...

2024-11-03_07-03 : iter     365000 <=> epoch 0.8199336467500777 | train loss 0.6396585107 | val loss 0.6372992396:
	--> training ...
	--> checkpointing ...

2024-11-03_07-53 : iter     370000 <=> epoch 0.8311656145137775 | train loss 0.6397753954 | val loss 0.6387027502:
	--> training ...
	--> checkpointing ...

2024-11-03_08-42 : iter     375000 <=> epoch 0.8423975822774771 | train loss 0.6388221383 | val loss 0.6389237642:
	--> training ...
	--> checkpointing ...

2024-11-03_09-31 : iter     380000 <=> epoch 0.8536295500411768 | train loss 0.6387917399 | val loss 0.6394318342:
	--> training ...
	--> checkpointing ...

2024-11-03_10-20 : iter     385000 <=> epoch 0.8648615178048765 | train loss 0.6389758587 | val loss 0.6391326785:
	--> training ...
	--> checkpointing ...

2024-11-03_11-10 : iter     390000 <=> epoch 0.8760934855685762 | train loss 0.6381532550 | val loss 0.6392377615:
	--> training ...
	--> checkpointing ...

2024-11-03_11-59 : iter     395000 <=> epoch 0.8873254533322759 | train loss 0.6391605139 | val loss 0.6369127631:
	--> training ...
	--> checkpointing ...

2024-11-03_12-48 : iter     400000 <=> epoch 0.8985574210959756 | train loss 0.6384239793 | val loss 0.6372846365:
	--> training ...
	--> checkpointing ...

2024-11-03_13-37 : iter     405000 <=> epoch 0.9097893888596753 | train loss 0.6393349171 | val loss 0.6389288902:
	--> training ...
	--> checkpointing ...

2024-11-03_14-27 : iter     410000 <=> epoch 0.9210213566233749 | train loss 0.6385042667 | val loss 0.6381149888:
	--> training ...
	--> checkpointing ...

2024-11-03_15-16 : iter     415000 <=> epoch 0.9322533243870746 | train loss 0.6389829516 | val loss 0.6392981410:
	--> training ...
	--> checkpointing ...

2024-11-03_16-05 : iter     420000 <=> epoch 0.9434852921507744 | train loss 0.6387299895 | val loss 0.6387967467:
	--> training ...
	--> checkpointing ...

2024-11-03_16-54 : iter     425000 <=> epoch 0.9547172599144741 | train loss 0.6374932528 | val loss 0.6383233666:
	--> training ...
	--> checkpointing ...

2024-11-03_17-44 : iter     430000 <=> epoch 0.9659492276781737 | train loss 0.6380763650 | val loss 0.6377472281:
	--> training ...
	--> checkpointing ...

2024-11-03_18-33 : iter     435000 <=> epoch 0.9771811954418734 | train loss 0.6386526227 | val loss 0.6390023828:
	--> training ...
	--> checkpointing ...

2024-11-03_19-22 : iter     440000 <=> epoch 0.9884131632055732 | train loss 0.6385351419 | val loss 0.6385908723:
	--> training ...
	--> checkpointing ...

2024-11-03_20-12 : iter     445000 <=> epoch 0.9996451309692729 | train loss 0.6382005811 | val loss 0.6378163695:
	--> training ...
