|ITERS: 222562 / 222562 | COMP: 100.00% | RATE: 2.01 it./s | SPD: 0.4987 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 6.40 it./s | SPD: 0.1561 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:3.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 222562

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 50M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-31_03-13 : iter     0 <=> epoch 0 | train loss 4.0904 | val loss 4.0900:
	--> training ...
	--> checkpointing ...

2024-10-31_03-57 : iter       5000 <=> epoch 0.02246556094638398 | train loss 0.7150184512 | val loss 0.7146641016:
	--> training ...
	--> checkpointing ...

2024-10-31_04-41 : iter      10000 <=> epoch 0.04493112189276796 | train loss 0.6880742311 | val loss 0.6877817512:
	--> training ...
	--> checkpointing ...

2024-10-31_05-25 : iter      15000 <=> epoch 0.06739668283915194 | train loss 0.6773814559 | val loss 0.6754949689:
	--> training ...
	--> checkpointing ...

2024-10-31_06-10 : iter      20000 <=> epoch 0.08986224378553592 | train loss 0.6682349443 | val loss 0.6692147255:
	--> training ...
	--> checkpointing ...

2024-10-31_06-54 : iter      25000 <=> epoch 0.11232780473191989 | train loss 0.6624576449 | val loss 0.6640180945:
	--> training ...
	--> checkpointing ...

2024-10-31_07-38 : iter      30000 <=> epoch 0.13479336567830388 | train loss 0.6594218612 | val loss 0.6608847380:
	--> training ...
	--> checkpointing ...

2024-10-31_08-22 : iter      35000 <=> epoch 0.15725892662468785 | train loss 0.6603708863 | val loss 0.6593495011:
	--> training ...
	--> checkpointing ...

2024-10-31_09-06 : iter      40000 <=> epoch 0.17972448757107184 | train loss 0.6556794643 | val loss 0.6574351788:
	--> training ...
	--> checkpointing ...

2024-10-31_09-50 : iter      45000 <=> epoch 0.2021900485174558 | train loss 0.6563181877 | val loss 0.6549606919:
	--> training ...
	--> checkpointing ...

2024-10-31_10-34 : iter      50000 <=> epoch 0.22465560946383978 | train loss 0.6538062096 | val loss 0.6534654498:
	--> training ...
	--> checkpointing ...

2024-10-31_11-19 : iter      55000 <=> epoch 0.24712117041022377 | train loss 0.6521613598 | val loss 0.6526262760:
	--> training ...
	--> checkpointing ...

2024-10-31_12-03 : iter      60000 <=> epoch 0.26958673135660777 | train loss 0.6519159079 | val loss 0.6510461569:
	--> training ...
	--> checkpointing ...

2024-10-31_12-47 : iter      65000 <=> epoch 0.29205229230299173 | train loss 0.6508381367 | val loss 0.6512008905:
	--> training ...
	--> checkpointing ...

2024-10-31_13-31 : iter      70000 <=> epoch 0.3145178532493757 | train loss 0.6507289410 | val loss 0.6509763002:
	--> training ...
	--> checkpointing ...

2024-10-31_14-15 : iter      75000 <=> epoch 0.33698341419575967 | train loss 0.6505755186 | val loss 0.6501995325:
	--> training ...
	--> checkpointing ...

2024-10-31_14-59 : iter      80000 <=> epoch 0.3594489751421437 | train loss 0.6507512927 | val loss 0.6500007510:
	--> training ...
	--> checkpointing ...

2024-10-31_15-44 : iter      85000 <=> epoch 0.38191453608852766 | train loss 0.6492744088 | val loss 0.6481347680:
	--> training ...
	--> checkpointing ...

2024-10-31_16-28 : iter      90000 <=> epoch 0.4043800970349116 | train loss 0.6484811902 | val loss 0.6487487555:
	--> training ...
	--> checkpointing ...

2024-10-31_17-12 : iter      95000 <=> epoch 0.4268456579812956 | train loss 0.6475796103 | val loss 0.6479879022:
	--> training ...
	--> checkpointing ...

2024-10-31_17-56 : iter     100000 <=> epoch 0.44931121892767956 | train loss 0.6480209827 | val loss 0.6473304629:
	--> training ...
	--> checkpointing ...

2024-10-31_18-40 : iter     105000 <=> epoch 0.4717767798740636 | train loss 0.6476795077 | val loss 0.6478354335:
	--> training ...
	--> checkpointing ...

2024-10-31_19-24 : iter     110000 <=> epoch 0.49424234082044755 | train loss 0.6473211646 | val loss 0.6479080319:
	--> training ...
	--> checkpointing ...

2024-10-31_20-09 : iter     115000 <=> epoch 0.5167079017668316 | train loss 0.6473647952 | val loss 0.6477792263:
	--> training ...
	--> checkpointing ...

2024-10-31_20-53 : iter     120000 <=> epoch 0.5391734627132155 | train loss 0.6473291516 | val loss 0.6477083564:
	--> training ...
	--> checkpointing ...

2024-10-31_21-37 : iter     125000 <=> epoch 0.5616390236595995 | train loss 0.6462386847 | val loss 0.6472292542:
	--> training ...
	--> checkpointing ...

2024-10-31_22-21 : iter     130000 <=> epoch 0.5841045846059835 | train loss 0.6462721825 | val loss 0.6460591555:
	--> training ...
	--> checkpointing ...

2024-10-31_23-05 : iter     135000 <=> epoch 0.6065701455523674 | train loss 0.6476036310 | val loss 0.6460952163:
	--> training ...
	--> checkpointing ...

2024-10-31_23-49 : iter     140000 <=> epoch 0.6290357064987514 | train loss 0.6465557814 | val loss 0.6467037797:
	--> training ...
	--> checkpointing ...

2024-11-01_00-34 : iter     145000 <=> epoch 0.6515012674451354 | train loss 0.6437039375 | val loss 0.6463671327:
	--> training ...
	--> checkpointing ...

2024-11-01_01-18 : iter     150000 <=> epoch 0.6739668283915193 | train loss 0.6447436810 | val loss 0.6457177997:
	--> training ...
	--> checkpointing ...

2024-11-01_02-02 : iter     155000 <=> epoch 0.6964323893379033 | train loss 0.6449585557 | val loss 0.6461759806:
	--> training ...
	--> checkpointing ...

2024-11-01_02-46 : iter     160000 <=> epoch 0.7188979502842874 | train loss 0.6426162720 | val loss 0.6426579356:
	--> training ...
	--> checkpointing ...

2024-11-01_03-30 : iter     165000 <=> epoch 0.7413635112306713 | train loss 0.6413491368 | val loss 0.6416109204:
	--> training ...
	--> checkpointing ...

2024-11-01_04-14 : iter     170000 <=> epoch 0.7638290721770553 | train loss 0.6406458616 | val loss 0.6399954557:
	--> training ...
	--> checkpointing ...

2024-11-01_04-59 : iter     175000 <=> epoch 0.7862946331234393 | train loss 0.6408425570 | val loss 0.6405214667:
	--> training ...
	--> checkpointing ...

2024-11-01_05-43 : iter     180000 <=> epoch 0.8087601940698232 | train loss 0.6406698227 | val loss 0.6410232186:
	--> training ...
	--> checkpointing ...

2024-11-01_06-27 : iter     185000 <=> epoch 0.8312257550162072 | train loss 0.6405519843 | val loss 0.6412257552:
	--> training ...
	--> checkpointing ...

2024-11-01_07-11 : iter     190000 <=> epoch 0.8536913159625912 | train loss 0.6402895451 | val loss 0.6411316991:
	--> training ...
	--> checkpointing ...

2024-11-01_07-55 : iter     195000 <=> epoch 0.8761568769089751 | train loss 0.6410368085 | val loss 0.6408300400:
	--> training ...
	--> checkpointing ...

2024-11-01_08-39 : iter     200000 <=> epoch 0.8986224378553591 | train loss 0.6405231953 | val loss 0.6405878663:
	--> training ...
	--> checkpointing ...

2024-11-01_09-24 : iter     205000 <=> epoch 0.9210879988017432 | train loss 0.6409664154 | val loss 0.6411055923:
	--> training ...
	--> checkpointing ...

2024-11-01_10-08 : iter     210000 <=> epoch 0.9435535597481272 | train loss 0.6402565241 | val loss 0.6409777403:
	--> training ...
	--> checkpointing ...

2024-11-01_10-52 : iter     215000 <=> epoch 0.9660191206945111 | train loss 0.6403084397 | val loss 0.6415184736:
	--> training ...
	--> checkpointing ...

2024-11-01_11-36 : iter     220000 <=> epoch 0.9884846816408951 | train loss 0.6395285130 | val loss 0.6412916780:
	--> training ...
