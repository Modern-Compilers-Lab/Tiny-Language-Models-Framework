|ITERS: 222562 / 222562 | COMP: 100.00% | RATE: 2.74 it./s | SPD: 0.3656 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 8.39 it./s | SPD: 0.1192 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:2.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 222562

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 30M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-28_04-13 : iter     0 <=> epoch 0 | train loss 4.2912 | val loss 4.2915:
	--> training ...
	--> checkpointing ...

2024-10-28_04-46 : iter       5000 <=> epoch 0.02246556094638398 | train loss 0.7073872089 | val loss 0.7062066793:
	--> training ...
	--> checkpointing ...

2024-10-28_05-18 : iter      10000 <=> epoch 0.04493112189276796 | train loss 0.6868443489 | val loss 0.6872242689:
	--> training ...
	--> checkpointing ...

2024-10-28_05-51 : iter      15000 <=> epoch 0.06739668283915194 | train loss 0.6755270958 | val loss 0.6766625047:
	--> training ...
	--> checkpointing ...

2024-10-28_06-23 : iter      20000 <=> epoch 0.08986224378553592 | train loss 0.6697252989 | val loss 0.6694853306:
	--> training ...
	--> checkpointing ...

2024-10-28_06-56 : iter      25000 <=> epoch 0.11232780473191989 | train loss 0.6643100381 | val loss 0.6644656658:
	--> training ...
	--> checkpointing ...

2024-10-28_07-28 : iter      30000 <=> epoch 0.13479336567830388 | train loss 0.6622830033 | val loss 0.6624233127:
	--> training ...
	--> checkpointing ...

2024-10-28_08-01 : iter      35000 <=> epoch 0.15725892662468785 | train loss 0.6591051817 | val loss 0.6582595706:
	--> training ...
	--> checkpointing ...

2024-10-28_08-33 : iter      40000 <=> epoch 0.17972448757107184 | train loss 0.6581579447 | val loss 0.6562780738:
	--> training ...
	--> checkpointing ...

2024-10-28_09-06 : iter      45000 <=> epoch 0.2021900485174558 | train loss 0.6567993760 | val loss 0.6570242643:
	--> training ...
	--> checkpointing ...

2024-10-28_09-38 : iter      50000 <=> epoch 0.22465560946383978 | train loss 0.6549842358 | val loss 0.6544096470:
	--> training ...
	--> checkpointing ...

2024-10-28_10-11 : iter      55000 <=> epoch 0.24712117041022377 | train loss 0.6537882090 | val loss 0.6532827020:
	--> training ...
	--> checkpointing ...

2024-10-28_10-43 : iter      60000 <=> epoch 0.26958673135660777 | train loss 0.6525015831 | val loss 0.6529411674:
	--> training ...
	--> checkpointing ...

2024-10-28_11-16 : iter      65000 <=> epoch 0.29205229230299173 | train loss 0.6517785788 | val loss 0.6519488692:
	--> training ...
	--> checkpointing ...

2024-10-28_11-48 : iter      70000 <=> epoch 0.3145178532493757 | train loss 0.6504195929 | val loss 0.6499679089:
	--> training ...
	--> checkpointing ...

2024-10-28_12-21 : iter      75000 <=> epoch 0.33698341419575967 | train loss 0.6510621905 | val loss 0.6503351927:
	--> training ...
	--> checkpointing ...

2024-10-28_12-53 : iter      80000 <=> epoch 0.3594489751421437 | train loss 0.6496831775 | val loss 0.6513285041:
	--> training ...
	--> checkpointing ...

2024-10-28_13-26 : iter      85000 <=> epoch 0.38191453608852766 | train loss 0.6487330198 | val loss 0.6497745514:
	--> training ...
	--> checkpointing ...

2024-10-28_13-58 : iter      90000 <=> epoch 0.4043800970349116 | train loss 0.6485234499 | val loss 0.6499944329:
	--> training ...
	--> checkpointing ...

2024-10-28_14-31 : iter      95000 <=> epoch 0.4268456579812956 | train loss 0.6494894624 | val loss 0.6472572088:
	--> training ...
	--> checkpointing ...

2024-10-28_15-03 : iter     100000 <=> epoch 0.44931121892767956 | train loss 0.6471090317 | val loss 0.6484521627:
	--> training ...
	--> checkpointing ...

2024-10-28_15-36 : iter     105000 <=> epoch 0.4717767798740636 | train loss 0.6468406320 | val loss 0.6481902003:
	--> training ...
	--> checkpointing ...

2024-10-28_16-08 : iter     110000 <=> epoch 0.49424234082044755 | train loss 0.6475625038 | val loss 0.6473444700:
	--> training ...
	--> checkpointing ...

2024-10-28_16-41 : iter     115000 <=> epoch 0.5167079017668316 | train loss 0.6467563510 | val loss 0.6486306190:
	--> training ...
	--> checkpointing ...

2024-10-28_17-13 : iter     120000 <=> epoch 0.5391734627132155 | train loss 0.6459487081 | val loss 0.6477390528:
	--> training ...
	--> checkpointing ...

2024-10-28_17-46 : iter     125000 <=> epoch 0.5616390236595995 | train loss 0.6463931799 | val loss 0.6473131180:
	--> training ...
	--> checkpointing ...

2024-10-28_18-18 : iter     130000 <=> epoch 0.5841045846059835 | train loss 0.6460635662 | val loss 0.6455091238:
	--> training ...
	--> checkpointing ...

2024-10-28_18-51 : iter     135000 <=> epoch 0.6065701455523674 | train loss 0.6943014264 | val loss 0.6939063072:
	--> training ...
	--> checkpointing ...

2024-10-28_19-23 : iter     140000 <=> epoch 0.6290357064987514 | train loss 0.6574931741 | val loss 0.6578999758:
	--> training ...
	--> checkpointing ...

2024-10-28_19-56 : iter     145000 <=> epoch 0.6515012674451354 | train loss 0.6441846490 | val loss 0.6455417275:
	--> training ...
	--> checkpointing ...

2024-10-28_20-28 : iter     150000 <=> epoch 0.6739668283915193 | train loss 0.6445490718 | val loss 0.6457791924:
	--> training ...
	--> checkpointing ...

2024-10-28_21-01 : iter     155000 <=> epoch 0.6964323893379033 | train loss 0.6441587806 | val loss 0.6457127929:
	--> training ...
	--> checkpointing ...

2024-10-28_21-33 : iter     160000 <=> epoch 0.7188979502842874 | train loss 0.6415370703 | val loss 0.6418743134:
	--> training ...
	--> checkpointing ...

2024-10-28_22-06 : iter     165000 <=> epoch 0.7413635112306713 | train loss 0.6406360865 | val loss 0.6418463588:
	--> training ...
	--> checkpointing ...

2024-10-28_22-38 : iter     170000 <=> epoch 0.7638290721770553 | train loss 0.6411229372 | val loss 0.6406679153:
	--> training ...
	--> checkpointing ...

2024-10-28_23-11 : iter     175000 <=> epoch 0.7862946331234393 | train loss 0.6397424340 | val loss 0.6407998800:
	--> training ...
	--> checkpointing ...

2024-10-28_23-43 : iter     180000 <=> epoch 0.8087601940698232 | train loss 0.6401593089 | val loss 0.6408045292:
	--> training ...
	--> checkpointing ...

2024-10-29_00-16 : iter     185000 <=> epoch 0.8312257550162072 | train loss 0.6396440268 | val loss 0.6406575441:
	--> training ...
	--> checkpointing ...

2024-10-29_00-48 : iter     190000 <=> epoch 0.8536913159625912 | train loss 0.6386945248 | val loss 0.6411795020:
	--> training ...
	--> checkpointing ...

2024-10-29_01-21 : iter     195000 <=> epoch 0.8761568769089751 | train loss 0.6412537694 | val loss 0.6400852203:
	--> training ...
	--> checkpointing ...

2024-10-29_01-53 : iter     200000 <=> epoch 0.8986224378553591 | train loss 0.6406626105 | val loss 0.6414290667:
	--> training ...
	--> checkpointing ...

2024-10-29_02-25 : iter     205000 <=> epoch 0.9210879988017432 | train loss 0.6406028867 | val loss 0.6413682699:
	--> training ...
	--> checkpointing ...

2024-10-29_02-58 : iter     210000 <=> epoch 0.9435535597481272 | train loss 0.6396871209 | val loss 0.6404753327:
	--> training ...
	--> checkpointing ...

2024-10-29_03-30 : iter     215000 <=> epoch 0.9660191206945111 | train loss 0.6400468349 | val loss 0.6413820386:
	--> training ...
	--> checkpointing ...

2024-10-29_04-03 : iter     220000 <=> epoch 0.9884846816408951 | train loss 0.6400375366 | val loss 0.6407283545:
	--> training ...
