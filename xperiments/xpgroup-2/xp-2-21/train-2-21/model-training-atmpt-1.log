|ITERS: 222562 / 222562 | COMP: 100.00% | RATE: 0.84 it./s | SPD: 1.1887 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 2.76 it./s | SPD: 0.3618 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:3.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 222562

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 58M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-01_17-34 : iter     0 <=> epoch 0 | train loss 4.2122 | val loss 4.2123:
	--> training ...
	--> checkpointing ...

2024-11-01_19-19 : iter       5000 <=> epoch 0.02246556094638398 | train loss 0.7197327614 | val loss 0.7196501493:
	--> training ...
	--> checkpointing ...

2024-11-01_21-05 : iter      10000 <=> epoch 0.04493112189276796 | train loss 0.6907974482 | val loss 0.6915069222:
	--> training ...
	--> checkpointing ...

2024-11-01_22-51 : iter      15000 <=> epoch 0.06739668283915194 | train loss 0.6760335565 | val loss 0.6766939163:
	--> training ...
	--> checkpointing ...

2024-11-02_00-36 : iter      20000 <=> epoch 0.08986224378553592 | train loss 0.6664137244 | val loss 0.6676746011:
	--> training ...
	--> checkpointing ...

2024-11-02_02-21 : iter      25000 <=> epoch 0.11232780473191989 | train loss 0.6616492271 | val loss 0.6634473205:
	--> training ...
	--> checkpointing ...

2024-11-02_04-06 : iter      30000 <=> epoch 0.13479336567830388 | train loss 0.6584330797 | val loss 0.6590245366:
	--> training ...
	--> checkpointing ...

2024-11-02_05-52 : iter      35000 <=> epoch 0.15725892662468785 | train loss 0.6562966108 | val loss 0.6563799977:
	--> training ...
	--> checkpointing ...

2024-11-02_07-37 : iter      40000 <=> epoch 0.17972448757107184 | train loss 0.6538673043 | val loss 0.6542169452:
	--> training ...
	--> checkpointing ...

2024-11-02_09-22 : iter      45000 <=> epoch 0.2021900485174558 | train loss 0.6537541151 | val loss 0.6531406045:
	--> training ...
	--> checkpointing ...

2024-11-02_11-07 : iter      50000 <=> epoch 0.22465560946383978 | train loss 0.6516465545 | val loss 0.6512337923:
	--> training ...
	--> checkpointing ...

2024-11-02_12-52 : iter      55000 <=> epoch 0.24712117041022377 | train loss 0.6511872411 | val loss 0.6513906121:
	--> training ...
	--> checkpointing ...

2024-11-02_14-37 : iter      60000 <=> epoch 0.26958673135660777 | train loss 0.6496415734 | val loss 0.6494788527:
	--> training ...
	--> checkpointing ...

2024-11-02_16-22 : iter      65000 <=> epoch 0.29205229230299173 | train loss 0.6484531760 | val loss 0.6477251649:
	--> training ...
	--> checkpointing ...

2024-11-02_18-08 : iter      70000 <=> epoch 0.3145178532493757 | train loss 0.6465938687 | val loss 0.6499996185:
	--> training ...
	--> checkpointing ...

2024-11-02_19-53 : iter      75000 <=> epoch 0.33698341419575967 | train loss 0.6468777061 | val loss 0.6474061012:
	--> training ...
	--> checkpointing ...

2024-11-02_21-38 : iter      80000 <=> epoch 0.3594489751421437 | train loss 0.6471872926 | val loss 0.6480082870:
	--> training ...
	--> checkpointing ...

2024-11-02_23-24 : iter      85000 <=> epoch 0.38191453608852766 | train loss 0.6463283896 | val loss 0.6473189592:
	--> training ...
	--> checkpointing ...

2024-11-03_01-09 : iter      90000 <=> epoch 0.4043800970349116 | train loss 0.6455370188 | val loss 0.6479189992:
	--> training ...
	--> checkpointing ...

2024-11-03_02-54 : iter      95000 <=> epoch 0.4268456579812956 | train loss 0.6468863487 | val loss 0.6469985247:
	--> training ...
	--> checkpointing ...

2024-11-03_04-39 : iter     100000 <=> epoch 0.44931121892767956 | train loss 0.6460670829 | val loss 0.6472322345:
	--> training ...
	--> checkpointing ...

2024-11-03_06-24 : iter     105000 <=> epoch 0.4717767798740636 | train loss 0.6451767087 | val loss 0.6453878284:
	--> training ...
	--> checkpointing ...

2024-11-03_08-09 : iter     110000 <=> epoch 0.49424234082044755 | train loss 0.6449321508 | val loss 0.6443018913:
	--> training ...
	--> checkpointing ...

2024-11-03_09-54 : iter     115000 <=> epoch 0.5167079017668316 | train loss 0.6448623538 | val loss 0.6445218921:
	--> training ...
	--> checkpointing ...

2024-11-03_11-39 : iter     120000 <=> epoch 0.5391734627132155 | train loss 0.6445893049 | val loss 0.6442708969:
	--> training ...
	--> checkpointing ...

2024-11-03_13-25 : iter     125000 <=> epoch 0.5616390236595995 | train loss 0.6447797418 | val loss 0.6437340379:
	--> training ...
	--> checkpointing ...

2024-11-03_15-10 : iter     130000 <=> epoch 0.5841045846059835 | train loss 0.6449751258 | val loss 0.6457285881:
	--> training ...
	--> checkpointing ...

2024-11-03_16-55 : iter     135000 <=> epoch 0.6065701455523674 | train loss 0.6436281204 | val loss 0.6435645819:
	--> training ...
	--> checkpointing ...

2024-11-03_18-40 : iter     140000 <=> epoch 0.6290357064987514 | train loss 0.6434415579 | val loss 0.6434863806:
	--> training ...
	--> checkpointing ...

2024-11-03_20-25 : iter     145000 <=> epoch 0.6515012674451354 | train loss 0.6426478028 | val loss 0.6429911256:
	--> training ...
	--> checkpointing ...

2024-11-03_22-10 : iter     150000 <=> epoch 0.6739668283915193 | train loss 0.6433748007 | val loss 0.6428564787:
	--> training ...
	--> checkpointing ...

2024-11-03_23-55 : iter     155000 <=> epoch 0.6964323893379033 | train loss 0.6431882381 | val loss 0.6433472633:
	--> training ...
	--> checkpointing ...

2024-11-04_01-41 : iter     160000 <=> epoch 0.7188979502842874 | train loss 0.6396601200 | val loss 0.6389824152:
	--> training ...
	--> checkpointing ...

2024-11-04_03-26 : iter     165000 <=> epoch 0.7413635112306713 | train loss 0.6400394440 | val loss 0.6392099261:
	--> training ...
	--> checkpointing ...

2024-11-04_05-11 : iter     170000 <=> epoch 0.7638290721770553 | train loss 0.6387814283 | val loss 0.6396078467:
	--> training ...
	--> checkpointing ...

2024-11-04_06-57 : iter     175000 <=> epoch 0.7862946331234393 | train loss 0.6393527389 | val loss 0.6392573118:
	--> training ...
	--> checkpointing ...

2024-11-04_08-42 : iter     180000 <=> epoch 0.8087601940698232 | train loss 0.6378039122 | val loss 0.6387939453:
	--> training ...
	--> checkpointing ...

2024-11-04_10-28 : iter     185000 <=> epoch 0.8312257550162072 | train loss 0.6391391754 | val loss 0.6391134858:
	--> training ...
	--> checkpointing ...

2024-11-04_12-13 : iter     190000 <=> epoch 0.8536913159625912 | train loss 0.6373372674 | val loss 0.6380858421:
	--> training ...
	--> checkpointing ...

2024-11-04_13-58 : iter     195000 <=> epoch 0.8761568769089751 | train loss 0.6382963061 | val loss 0.6385434866:
	--> training ...
	--> checkpointing ...

2024-11-04_15-44 : iter     200000 <=> epoch 0.8986224378553591 | train loss 0.6385436654 | val loss 0.6391861439:
	--> training ...
	--> checkpointing ...

2024-11-04_17-29 : iter     205000 <=> epoch 0.9210879988017432 | train loss 0.6388906240 | val loss 0.6394274831:
	--> training ...
	--> checkpointing ...

2024-11-04_19-15 : iter     210000 <=> epoch 0.9435535597481272 | train loss 0.6382420063 | val loss 0.6395971775:
	--> training ...
	--> checkpointing ...

2024-11-04_21-01 : iter     215000 <=> epoch 0.9660191206945111 | train loss 0.6391781569 | val loss 0.6388993263:
	--> training ...
	--> checkpointing ...

2024-11-04_22-46 : iter     220000 <=> epoch 0.9884846816408951 | train loss 0.6383491158 | val loss 0.6376014948:
	--> training ...
