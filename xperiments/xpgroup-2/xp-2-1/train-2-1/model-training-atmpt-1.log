|ITERS: 240818 / 240818 | COMP: 100.00% | RATE: 4.21 it./s | SPD: 0.2377 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 12.36 it./s | SPD: 0.0809 s/it.| ERT: (0, 0, 0, 0) |                                                                                                      

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:0.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 2)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 240818

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 30M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-23_12-19 : iter     0 <=> epoch 0 | train loss 4.1517 | val loss 4.1516:
	--> training ...
	--> checkpointing ...

2024-10-23_12-40 : iter       5000 <=> epoch 0.020762526678703094 | train loss 0.6872518063 | val loss 0.6879378557:
	--> training ...
	--> checkpointing ...

2024-10-23_13-01 : iter      10000 <=> epoch 0.04152505335740619 | train loss 0.6682161689 | val loss 0.6688681245:
	--> training ...
	--> checkpointing ...

2024-10-23_13-23 : iter      15000 <=> epoch 0.062287580036109284 | train loss 0.6574096680 | val loss 0.6573941112:
	--> training ...
	--> checkpointing ...

2024-10-23_13-44 : iter      20000 <=> epoch 0.08305010671481237 | train loss 0.6515650749 | val loss 0.6508203745:
	--> training ...
	--> checkpointing ...

2024-10-23_14-05 : iter      25000 <=> epoch 0.10381263339351547 | train loss 0.6466788054 | val loss 0.6470016241:
	--> training ...
	--> checkpointing ...

2024-10-23_14-26 : iter      30000 <=> epoch 0.12457516007221857 | train loss 0.6428803205 | val loss 0.6412575245:
	--> training ...
	--> checkpointing ...

2024-10-23_14-47 : iter      35000 <=> epoch 0.14533768675092165 | train loss 0.6382535696 | val loss 0.6392479539:
	--> training ...
	--> checkpointing ...

2024-10-23_15-09 : iter      40000 <=> epoch 0.16610021342962475 | train loss 0.6354150176 | val loss 0.6378676891:
	--> training ...
	--> checkpointing ...

2024-10-23_15-30 : iter      45000 <=> epoch 0.18686274010832785 | train loss 0.6357177496 | val loss 0.6348392963:
	--> training ...
	--> checkpointing ...

2024-10-23_15-51 : iter      50000 <=> epoch 0.20762526678703094 | train loss 0.6327827573 | val loss 0.6324969530:
	--> training ...
	--> checkpointing ...

2024-10-23_16-12 : iter      55000 <=> epoch 0.22838779346573404 | train loss 0.6313491464 | val loss 0.6318411827:
	--> training ...
	--> checkpointing ...

2024-10-23_16-33 : iter      60000 <=> epoch 0.24915032014443714 | train loss 0.6311195493 | val loss 0.6323562860:
	--> training ...
	--> checkpointing ...

2024-10-23_16-55 : iter      65000 <=> epoch 0.2699128468231402 | train loss 0.6297031641 | val loss 0.6306173801:
	--> training ...
	--> checkpointing ...

2024-10-23_17-16 : iter      70000 <=> epoch 0.2906753735018433 | train loss 0.6300933957 | val loss 0.6294509768:
	--> training ...
	--> checkpointing ...

2024-10-23_17-37 : iter      75000 <=> epoch 0.3114379001805464 | train loss 0.6301995516 | val loss 0.6293019056:
	--> training ...
	--> checkpointing ...

2024-10-23_17-58 : iter      80000 <=> epoch 0.3322004268592495 | train loss 0.6278439760 | val loss 0.6296406388:
	--> training ...
	--> checkpointing ...

2024-10-23_18-19 : iter      85000 <=> epoch 0.3529629535379526 | train loss 0.6284298301 | val loss 0.6276056767:
	--> training ...
	--> checkpointing ...

2024-10-23_18-41 : iter      90000 <=> epoch 0.3737254802166557 | train loss 0.6259847283 | val loss 0.6268936992:
	--> training ...
	--> checkpointing ...

2024-10-23_19-02 : iter      95000 <=> epoch 0.3944880068953588 | train loss 0.6258184910 | val loss 0.6272499561:
	--> training ...
	--> checkpointing ...

2024-10-23_19-23 : iter     100000 <=> epoch 0.4152505335740619 | train loss 0.6248390079 | val loss 0.6251963377:
	--> training ...
	--> checkpointing ...

2024-10-23_19-44 : iter     105000 <=> epoch 0.436013060252765 | train loss 0.6236286163 | val loss 0.6252016425:
	--> training ...
	--> checkpointing ...

2024-10-23_20-06 : iter     110000 <=> epoch 0.4567755869314681 | train loss 0.6244170666 | val loss 0.6245275140:
	--> training ...
	--> checkpointing ...

2024-10-23_20-27 : iter     115000 <=> epoch 0.4775381136101712 | train loss 0.6244024038 | val loss 0.6253284216:
	--> training ...
	--> checkpointing ...

2024-10-23_20-48 : iter     120000 <=> epoch 0.4983006402888743 | train loss 0.6233729124 | val loss 0.6245416999:
	--> training ...
	--> checkpointing ...

2024-10-23_21-09 : iter     125000 <=> epoch 0.5190631669675774 | train loss 0.6242642999 | val loss 0.6248791218:
	--> training ...
	--> checkpointing ...

2024-10-23_21-30 : iter     130000 <=> epoch 0.5398256936462804 | train loss 0.6218391061 | val loss 0.6243935227:
	--> training ...
	--> checkpointing ...

2024-10-23_21-52 : iter     135000 <=> epoch 0.5605882203249836 | train loss 0.6234058738 | val loss 0.6233811378:
	--> training ...
	--> checkpointing ...

2024-10-23_22-13 : iter     140000 <=> epoch 0.5813507470036866 | train loss 0.6219857931 | val loss 0.6230049729:
	--> training ...
	--> checkpointing ...

2024-10-23_22-34 : iter     145000 <=> epoch 0.6021132736823898 | train loss 0.6211155057 | val loss 0.6229764819:
	--> training ...
	--> checkpointing ...

2024-10-23_22-55 : iter     150000 <=> epoch 0.6228758003610928 | train loss 0.6215952635 | val loss 0.6240932941:
	--> training ...
	--> checkpointing ...

2024-10-23_23-16 : iter     155000 <=> epoch 0.643638327039796 | train loss 0.6231104136 | val loss 0.6210726500:
	--> training ...
	--> checkpointing ...

2024-10-23_23-38 : iter     160000 <=> epoch 0.664400853718499 | train loss 0.6224390268 | val loss 0.6216589808:
	--> training ...
	--> checkpointing ...

2024-10-23_23-59 : iter     165000 <=> epoch 0.6851633803972021 | train loss 0.6217997670 | val loss 0.6220800877:
	--> training ...
	--> checkpointing ...

2024-10-24_00-20 : iter     170000 <=> epoch 0.7059259070759052 | train loss 0.6170504689 | val loss 0.6187152267:
	--> training ...
	--> checkpointing ...

2024-10-24_00-41 : iter     175000 <=> epoch 0.7266884337546083 | train loss 0.6171236038 | val loss 0.6174552441:
	--> training ...
	--> checkpointing ...

2024-10-24_01-03 : iter     180000 <=> epoch 0.7474509604333114 | train loss 0.6166896224 | val loss 0.6168559790:
	--> training ...
	--> checkpointing ...

2024-10-24_01-24 : iter     185000 <=> epoch 0.7682134871120145 | train loss 0.6160954237 | val loss 0.6156588793:
	--> training ...
	--> checkpointing ...

2024-10-24_01-45 : iter     190000 <=> epoch 0.7889760137907176 | train loss 0.6157258153 | val loss 0.6169756055:
	--> training ...
	--> checkpointing ...

2024-10-24_02-06 : iter     195000 <=> epoch 0.8097385404694207 | train loss 0.6149295568 | val loss 0.6148020625:
	--> training ...
	--> checkpointing ...

2024-10-24_02-27 : iter     200000 <=> epoch 0.8305010671481238 | train loss 0.6147841811 | val loss 0.6159315109:
	--> training ...
	--> checkpointing ...

2024-10-24_02-49 : iter     205000 <=> epoch 0.8512635938268268 | train loss 0.6154037118 | val loss 0.6153371930:
	--> training ...
	--> checkpointing ...

2024-10-24_03-10 : iter     210000 <=> epoch 0.87202612050553 | train loss 0.6152071953 | val loss 0.6150947213:
	--> training ...
	--> checkpointing ...

2024-10-24_03-31 : iter     215000 <=> epoch 0.892788647184233 | train loss 0.6156899929 | val loss 0.6137714386:
	--> training ...
	--> checkpointing ...

2024-10-24_03-52 : iter     220000 <=> epoch 0.9135511738629362 | train loss 0.6143802404 | val loss 0.6141172647:
	--> training ...
	--> checkpointing ...

2024-10-24_04-13 : iter     225000 <=> epoch 0.9343137005416392 | train loss 0.6151841879 | val loss 0.6141186953:
	--> training ...
	--> checkpointing ...

2024-10-24_04-35 : iter     230000 <=> epoch 0.9550762272203424 | train loss 0.6154488921 | val loss 0.6143244505:
	--> training ...
	--> checkpointing ...

2024-10-24_04-56 : iter     235000 <=> epoch 0.9758387538990454 | train loss 0.6150713563 | val loss 0.6146996021:
	--> training ...
	--> checkpointing ...

2024-10-24_05-17 : iter     240000 <=> epoch 0.9966012805777485 | train loss 0.6142099500 | val loss 0.6160718203:
	--> training ...
