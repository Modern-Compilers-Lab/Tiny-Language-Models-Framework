|ITERS: 240818 / 240818 | COMP: 100.00% | RATE: 4.09 it./s | SPD: 0.2442 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 12.36 it./s | SPD: 0.0809 s/it.| ERT: (0, 0, 0, 0) |                                                                                                      

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:2.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 240818

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 30M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-23_12-30 : iter     0 <=> epoch 0 | train loss 4.1530 | val loss 4.1529:
	--> training ...
	--> checkpointing ...

2024-10-23_12-51 : iter       5000 <=> epoch 0.020762526678703094 | train loss 0.6861482859 | val loss 0.6867104173:
	--> training ...
	--> checkpointing ...

2024-10-23_13-13 : iter      10000 <=> epoch 0.04152505335740619 | train loss 0.6709415913 | val loss 0.6716551781:
	--> training ...
	--> checkpointing ...

2024-10-23_13-34 : iter      15000 <=> epoch 0.062287580036109284 | train loss 0.6552340984 | val loss 0.6553252935:
	--> training ...
	--> checkpointing ...

2024-10-23_13-56 : iter      20000 <=> epoch 0.08305010671481237 | train loss 0.6499702930 | val loss 0.6492160559:
	--> training ...
	--> checkpointing ...

2024-10-23_14-17 : iter      25000 <=> epoch 0.10381263339351547 | train loss 0.6439257860 | val loss 0.6442656517:
	--> training ...
	--> checkpointing ...

2024-10-23_14-39 : iter      30000 <=> epoch 0.12457516007221857 | train loss 0.6413162947 | val loss 0.6396462321:
	--> training ...
	--> checkpointing ...

2024-10-23_15-01 : iter      35000 <=> epoch 0.14533768675092165 | train loss 0.6364516616 | val loss 0.6374078393:
	--> training ...
	--> checkpointing ...

2024-10-23_15-22 : iter      40000 <=> epoch 0.16610021342962475 | train loss 0.6337542534 | val loss 0.6363372207:
	--> training ...
	--> checkpointing ...

2024-10-23_15-44 : iter      45000 <=> epoch 0.18686274010832785 | train loss 0.6343695521 | val loss 0.6336334348:
	--> training ...
	--> checkpointing ...

2024-10-23_16-05 : iter      50000 <=> epoch 0.20762526678703094 | train loss 0.6311667562 | val loss 0.6309209466:
	--> training ...
	--> checkpointing ...

2024-10-23_16-27 : iter      55000 <=> epoch 0.22838779346573404 | train loss 0.6296901107 | val loss 0.6302745342:
	--> training ...
	--> checkpointing ...

2024-10-23_16-48 : iter      60000 <=> epoch 0.24915032014443714 | train loss 0.6293058991 | val loss 0.6305391192:
	--> training ...
	--> checkpointing ...

2024-10-23_17-10 : iter      65000 <=> epoch 0.2699128468231402 | train loss 0.6283326745 | val loss 0.6291871667:
	--> training ...
	--> checkpointing ...

2024-10-23_17-31 : iter      70000 <=> epoch 0.2906753735018433 | train loss 0.6286635995 | val loss 0.6280340552:
	--> training ...
	--> checkpointing ...

2024-10-23_17-53 : iter      75000 <=> epoch 0.3114379001805464 | train loss 0.6279441714 | val loss 0.6271811724:
	--> training ...
	--> checkpointing ...

2024-10-23_18-15 : iter      80000 <=> epoch 0.3322004268592495 | train loss 0.6263419390 | val loss 0.6280244589:
	--> training ...
	--> checkpointing ...

2024-10-23_18-36 : iter      85000 <=> epoch 0.3529629535379526 | train loss 0.6264687181 | val loss 0.6256297827:
	--> training ...
	--> checkpointing ...

2024-10-23_18-58 : iter      90000 <=> epoch 0.3737254802166557 | train loss 0.6246866584 | val loss 0.6255211234:
	--> training ...
	--> checkpointing ...

2024-10-23_19-19 : iter      95000 <=> epoch 0.3944880068953588 | train loss 0.6242554188 | val loss 0.6256874800:
	--> training ...
	--> checkpointing ...

2024-10-23_19-41 : iter     100000 <=> epoch 0.4152505335740619 | train loss 0.6237743497 | val loss 0.6241635680:
	--> training ...
	--> checkpointing ...

2024-10-23_20-02 : iter     105000 <=> epoch 0.436013060252765 | train loss 0.6223405004 | val loss 0.6238746047:
	--> training ...
	--> checkpointing ...

2024-10-23_20-24 : iter     110000 <=> epoch 0.4567755869314681 | train loss 0.6230819225 | val loss 0.6230981350:
	--> training ...
	--> checkpointing ...

2024-10-23_20-45 : iter     115000 <=> epoch 0.4775381136101712 | train loss 0.6229348183 | val loss 0.6239855886:
	--> training ...
	--> checkpointing ...

2024-10-23_21-07 : iter     120000 <=> epoch 0.4983006402888743 | train loss 0.6220245361 | val loss 0.6231077909:
	--> training ...
	--> checkpointing ...

2024-10-23_21-28 : iter     125000 <=> epoch 0.5190631669675774 | train loss 0.6227895617 | val loss 0.6233849525:
	--> training ...
	--> checkpointing ...

2024-10-23_21-50 : iter     130000 <=> epoch 0.5398256936462804 | train loss 0.6204504967 | val loss 0.6230567694:
	--> training ...
	--> checkpointing ...

2024-10-23_22-11 : iter     135000 <=> epoch 0.5605882203249836 | train loss 0.6231400967 | val loss 0.6230440140:
	--> training ...
	--> checkpointing ...

2024-10-23_22-33 : iter     140000 <=> epoch 0.5813507470036866 | train loss 0.6208682060 | val loss 0.6219573021:
	--> training ...
	--> checkpointing ...

2024-10-23_22-55 : iter     145000 <=> epoch 0.6021132736823898 | train loss 0.6199586391 | val loss 0.6217898130:
	--> training ...
	--> checkpointing ...

2024-10-23_23-16 : iter     150000 <=> epoch 0.6228758003610928 | train loss 0.6203904748 | val loss 0.6228800416:
	--> training ...
	--> checkpointing ...

2024-10-23_23-38 : iter     155000 <=> epoch 0.643638327039796 | train loss 0.6221714616 | val loss 0.6201047301:
	--> training ...
	--> checkpointing ...

2024-10-23_23-59 : iter     160000 <=> epoch 0.664400853718499 | train loss 0.6210722923 | val loss 0.6202790737:
	--> training ...
	--> checkpointing ...

2024-10-24_00-21 : iter     165000 <=> epoch 0.6851633803972021 | train loss 0.6210684776 | val loss 0.6213875413:
	--> training ...
	--> checkpointing ...

2024-10-24_00-42 : iter     170000 <=> epoch 0.7059259070759052 | train loss 0.6159701347 | val loss 0.6176922321:
	--> training ...
	--> checkpointing ...

2024-10-24_01-04 : iter     175000 <=> epoch 0.7266884337546083 | train loss 0.6159484386 | val loss 0.6163486838:
	--> training ...
	--> checkpointing ...

2024-10-24_01-25 : iter     180000 <=> epoch 0.7474509604333114 | train loss 0.6155770421 | val loss 0.6157877445:
	--> training ...
	--> checkpointing ...

2024-10-24_01-47 : iter     185000 <=> epoch 0.7682134871120145 | train loss 0.6150304675 | val loss 0.6145685911:
	--> training ...
	--> checkpointing ...

2024-10-24_02-08 : iter     190000 <=> epoch 0.7889760137907176 | train loss 0.6146216989 | val loss 0.6159036756:
	--> training ...
	--> checkpointing ...

2024-10-24_02-30 : iter     195000 <=> epoch 0.8097385404694207 | train loss 0.6138620377 | val loss 0.6136469841:
	--> training ...
	--> checkpointing ...

2024-10-24_02-51 : iter     200000 <=> epoch 0.8305010671481238 | train loss 0.6136815548 | val loss 0.6147955060:
	--> training ...
	--> checkpointing ...

2024-10-24_03-13 : iter     205000 <=> epoch 0.8512635938268268 | train loss 0.6143406034 | val loss 0.6142030954:
	--> training ...
	--> checkpointing ...

2024-10-24_03-35 : iter     210000 <=> epoch 0.87202612050553 | train loss 0.6140226722 | val loss 0.6139423847:
	--> training ...
	--> checkpointing ...

2024-10-24_03-56 : iter     215000 <=> epoch 0.892788647184233 | train loss 0.6145836115 | val loss 0.6126447916:
	--> training ...
	--> checkpointing ...

2024-10-24_04-18 : iter     220000 <=> epoch 0.9135511738629362 | train loss 0.6132060289 | val loss 0.6129845977:
	--> training ...
	--> checkpointing ...

2024-10-24_04-39 : iter     225000 <=> epoch 0.9343137005416392 | train loss 0.6140521169 | val loss 0.6130062938:
	--> training ...
	--> checkpointing ...

2024-10-24_05-01 : iter     230000 <=> epoch 0.9550762272203424 | train loss 0.6142481565 | val loss 0.6131403446:
	--> training ...
	--> checkpointing ...

2024-10-24_05-22 : iter     235000 <=> epoch 0.9758387538990454 | train loss 0.6139336228 | val loss 0.6136077642:
	--> training ...
	--> checkpointing ...

2024-10-24_05-44 : iter     240000 <=> epoch 0.9966012805777485 | train loss 0.6130446196 | val loss 0.6148999929:
	--> training ...
