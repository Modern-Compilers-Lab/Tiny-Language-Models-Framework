|ITERS: 693343 / 1135199 | COMP: 61.08% | RATE: 1.23 it./s | SPD: 0.8100 s/it.| ERT: (4, 3, 24, 58)                                                                                                     
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 4.03 it./s | SPD: 0.2482 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:4.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 3)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 1135199

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 62M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-03_03-04 : iter     0 <=> epoch 0 | train loss 4.1375 | val loss 4.1375:
	--> training ...
	--> checkpointing ...

2024-11-03_04-15 : iter       5000 <=> epoch 0.007487667946396042 | train loss 0.7034004927 | val loss 0.7038060427:
	--> training ...
	--> checkpointing ...

2024-11-03_05-27 : iter      10000 <=> epoch 0.014975335892792084 | train loss 0.6822161674 | val loss 0.6852126122:
	--> training ...
	--> checkpointing ...

2024-11-03_06-38 : iter      15000 <=> epoch 0.022463003839188127 | train loss 0.6722896695 | val loss 0.6722629666:
	--> training ...
	--> checkpointing ...

2024-11-03_07-50 : iter      20000 <=> epoch 0.02995067178558417 | train loss 0.6651781201 | val loss 0.6660634279:
	--> training ...
	--> checkpointing ...

2024-11-03_09-01 : iter      25000 <=> epoch 0.037438339731980214 | train loss 0.6610870361 | val loss 0.6599078178:
	--> training ...
	--> checkpointing ...

2024-11-03_10-13 : iter      30000 <=> epoch 0.044926007678376255 | train loss 0.6586636901 | val loss 0.6578909755:
	--> training ...
	--> checkpointing ...

2024-11-03_11-25 : iter      35000 <=> epoch 0.052413675624772296 | train loss 0.6558496356 | val loss 0.6550108790:
	--> training ...
	--> checkpointing ...

2024-11-03_12-36 : iter      40000 <=> epoch 0.05990134357116834 | train loss 0.6542118192 | val loss 0.6543319225:
	--> training ...
	--> checkpointing ...

2024-11-03_13-48 : iter      45000 <=> epoch 0.06738901151756438 | train loss 0.6530724168 | val loss 0.6513646245:
	--> training ...
	--> checkpointing ...

2024-11-03_14-59 : iter      50000 <=> epoch 0.07487667946396043 | train loss 0.6517044306 | val loss 0.6515011191:
	--> training ...
	--> checkpointing ...

2024-11-03_16-11 : iter      55000 <=> epoch 0.08236434741035646 | train loss 0.6501279473 | val loss 0.6498899460:
	--> training ...
	--> checkpointing ...

2024-11-03_17-22 : iter      60000 <=> epoch 0.08985201535675251 | train loss 0.6504048109 | val loss 0.6499944329:
	--> training ...
	--> checkpointing ...

2024-11-03_18-34 : iter      65000 <=> epoch 0.09733968330314854 | train loss 0.6485403776 | val loss 0.6496176124:
	--> training ...
	--> checkpointing ...

2024-11-03_19-46 : iter      70000 <=> epoch 0.10482735124954459 | train loss 0.6478431821 | val loss 0.6491116285:
	--> training ...
	--> checkpointing ...

2024-11-03_20-57 : iter      75000 <=> epoch 0.11231501919594063 | train loss 0.6471998096 | val loss 0.6479275227:
	--> training ...
	--> checkpointing ...

2024-11-03_22-09 : iter      80000 <=> epoch 0.11980268714233668 | train loss 0.6491122842 | val loss 0.6465826035:
	--> training ...
	--> checkpointing ...

2024-11-03_23-20 : iter      85000 <=> epoch 0.1272903550887327 | train loss 0.6475120783 | val loss 0.6472065449:
	--> training ...
	--> checkpointing ...

2024-11-04_00-32 : iter      90000 <=> epoch 0.13477802303512876 | train loss 0.6463115215 | val loss 0.6467163563:
	--> training ...
	--> checkpointing ...

2024-11-04_01-44 : iter      95000 <=> epoch 0.1422656909815248 | train loss 0.6470613480 | val loss 0.6455845237:
	--> training ...
	--> checkpointing ...

2024-11-04_02-55 : iter     100000 <=> epoch 0.14975335892792085 | train loss 0.6469854712 | val loss 0.6459375024:
	--> training ...
	--> checkpointing ...

2024-11-04_04-07 : iter     105000 <=> epoch 0.15724102687431687 | train loss 0.6480921507 | val loss 0.6469559669:
	--> training ...
	--> checkpointing ...

2024-11-04_05-18 : iter     110000 <=> epoch 0.16472869482071292 | train loss 0.6445329785 | val loss 0.6463986635:
	--> training ...
	--> checkpointing ...

2024-11-04_06-30 : iter     115000 <=> epoch 0.17221636276710897 | train loss 0.6456717849 | val loss 0.6457281709:
	--> training ...
	--> checkpointing ...

2024-11-04_07-41 : iter     120000 <=> epoch 0.17970403071350502 | train loss 0.6449010968 | val loss 0.6440911293:
	--> training ...
	--> checkpointing ...

2024-11-04_08-53 : iter     125000 <=> epoch 0.18719169865990107 | train loss 0.6437303424 | val loss 0.6443370581:
	--> training ...
	--> checkpointing ...

2024-11-04_10-04 : iter     130000 <=> epoch 0.1946793666062971 | train loss 0.6455885172 | val loss 0.6448607445:
	--> training ...
	--> checkpointing ...

2024-11-04_11-16 : iter     135000 <=> epoch 0.20216703455269314 | train loss 0.6448336840 | val loss 0.6436134577:
	--> training ...
	--> checkpointing ...

2024-11-04_12-27 : iter     140000 <=> epoch 0.20965470249908918 | train loss 0.6443023086 | val loss 0.6450417638:
	--> training ...
	--> checkpointing ...

2024-11-04_13-39 : iter     145000 <=> epoch 0.21714237044548523 | train loss 0.6439638138 | val loss 0.6450037956:
	--> training ...
	--> checkpointing ...

2024-11-04_14-50 : iter     150000 <=> epoch 0.22463003839188125 | train loss 0.6432536244 | val loss 0.6438116431:
	--> training ...
	--> checkpointing ...

2024-11-04_16-02 : iter     155000 <=> epoch 0.2321177063382773 | train loss 0.6439746618 | val loss 0.6434193850:
	--> training ...
	--> checkpointing ...

2024-11-04_17-13 : iter     160000 <=> epoch 0.23960537428467335 | train loss 0.6444759965 | val loss 0.6440842152:
	--> training ...
	--> checkpointing ...

2024-11-04_18-25 : iter     165000 <=> epoch 0.2470930422310694 | train loss 0.6429519057 | val loss 0.6427070498:
	--> training ...
	--> checkpointing ...

2024-11-04_19-36 : iter     170000 <=> epoch 0.2545807101774654 | train loss 0.6422226429 | val loss 0.6434834003:
	--> training ...
	--> checkpointing ...

2024-11-04_20-48 : iter     175000 <=> epoch 0.26206837812386147 | train loss 0.6425660849 | val loss 0.6441162229:
	--> training ...
	--> checkpointing ...

2024-11-04_22-00 : iter     180000 <=> epoch 0.2695560460702575 | train loss 0.6444298029 | val loss 0.6423651576:
	--> training ...
	--> checkpointing ...

2024-11-04_23-11 : iter     185000 <=> epoch 0.27704371401665356 | train loss 0.6422730088 | val loss 0.6432700753:
	--> training ...
	--> checkpointing ...

2024-11-05_00-23 : iter     190000 <=> epoch 0.2845313819630496 | train loss 0.6440640092 | val loss 0.6429968476:
	--> training ...
	--> checkpointing ...

2024-11-05_01-34 : iter     195000 <=> epoch 0.29201904990944566 | train loss 0.6432527900 | val loss 0.6437606812:
	--> training ...
	--> checkpointing ...

2024-11-05_02-46 : iter     200000 <=> epoch 0.2995067178558417 | train loss 0.6420997381 | val loss 0.6432338953:
	--> training ...
	--> checkpointing ...

2024-11-05_03-58 : iter     205000 <=> epoch 0.30699438580223776 | train loss 0.6425798535 | val loss 0.6425763369:
	--> training ...
	--> checkpointing ...

2024-11-05_05-09 : iter     210000 <=> epoch 0.31448205374863375 | train loss 0.6430432200 | val loss 0.6428921223:
	--> training ...
	--> checkpointing ...

2024-11-05_06-21 : iter     215000 <=> epoch 0.3219697216950298 | train loss 0.6425222158 | val loss 0.6420865059:
	--> training ...
	--> checkpointing ...

2024-11-05_07-33 : iter     220000 <=> epoch 0.32945738964142585 | train loss 0.6417264938 | val loss 0.6425699592:
	--> training ...
	--> checkpointing ...

2024-11-05_08-44 : iter     225000 <=> epoch 0.3369450575878219 | train loss 0.6415871978 | val loss 0.6433917880:
	--> training ...
	--> checkpointing ...

2024-11-05_09-56 : iter     230000 <=> epoch 0.34443272553421794 | train loss 0.6420636773 | val loss 0.6419770122:
	--> training ...
	--> checkpointing ...

2024-11-05_11-08 : iter     235000 <=> epoch 0.351920393480614 | train loss 0.6418272853 | val loss 0.6418690085:
	--> training ...
	--> checkpointing ...

2024-11-05_12-19 : iter     240000 <=> epoch 0.35940806142701004 | train loss 0.6438431740 | val loss 0.6418461800:
	--> training ...
	--> checkpointing ...

2024-11-05_13-31 : iter     245000 <=> epoch 0.3668957293734061 | train loss 0.6426623464 | val loss 0.6407437921:
	--> training ...
	--> checkpointing ...

2024-11-05_14-43 : iter     250000 <=> epoch 0.37438339731980214 | train loss 0.6420699954 | val loss 0.6420668364:
	--> training ...
	--> checkpointing ...

2024-11-05_15-54 : iter     255000 <=> epoch 0.38187106526619813 | train loss 0.6411867738 | val loss 0.6420575976:
	--> training ...
	--> checkpointing ...

2024-11-05_17-06 : iter     260000 <=> epoch 0.3893587332125942 | train loss 0.6401789188 | val loss 0.6402226686:
	--> training ...
	--> checkpointing ...

2024-11-05_18-18 : iter     265000 <=> epoch 0.3968464011589902 | train loss 0.6418028474 | val loss 0.6427131295:
	--> training ...
	--> checkpointing ...

2024-11-05_19-29 : iter     270000 <=> epoch 0.4043340691053863 | train loss 0.6409990788 | val loss 0.6413621902:
	--> training ...
	--> checkpointing ...

2024-11-05_20-41 : iter     275000 <=> epoch 0.4118217370517823 | train loss 0.6420954466 | val loss 0.6428406835:
	--> training ...
	--> checkpointing ...

2024-11-05_21-53 : iter     280000 <=> epoch 0.41930940499817837 | train loss 0.6410183311 | val loss 0.6403062940:
	--> training ...
	--> checkpointing ...

2024-11-05_23-04 : iter     285000 <=> epoch 0.4267970729445744 | train loss 0.6414255500 | val loss 0.6416057348:
	--> training ...
	--> checkpointing ...

2024-11-06_00-16 : iter     290000 <=> epoch 0.43428474089097047 | train loss 0.6416543126 | val loss 0.6414443254:
	--> training ...
	--> checkpointing ...

2024-11-06_01-28 : iter     295000 <=> epoch 0.4417724088373665 | train loss 0.6399073601 | val loss 0.6406422853:
	--> training ...
	--> checkpointing ...

2024-11-06_02-39 : iter     300000 <=> epoch 0.4492600767837625 | train loss 0.6412991285 | val loss 0.6412609816:
	--> training ...
	--> checkpointing ...

2024-11-06_03-51 : iter     305000 <=> epoch 0.45674774473015856 | train loss 0.6409357786 | val loss 0.6407154799:
	--> training ...
	--> checkpointing ...

2024-11-06_05-03 : iter     310000 <=> epoch 0.4642354126765546 | train loss 0.6418149471 | val loss 0.6421248913:
	--> training ...
	--> checkpointing ...

2024-11-06_06-14 : iter     315000 <=> epoch 0.47172308062295065 | train loss 0.6405845881 | val loss 0.6401898861:
	--> training ...
	--> checkpointing ...

2024-11-06_07-26 : iter     320000 <=> epoch 0.4792107485693467 | train loss 0.6410046220 | val loss 0.6410362720:
	--> training ...
	--> checkpointing ...

2024-11-06_08-38 : iter     325000 <=> epoch 0.48669841651574275 | train loss 0.6408281326 | val loss 0.6400048137:
	--> training ...
	--> checkpointing ...

2024-11-06_09-49 : iter     330000 <=> epoch 0.4941860844621388 | train loss 0.6405129433 | val loss 0.6394688487:
	--> training ...
	--> checkpointing ...

2024-11-06_11-01 : iter     335000 <=> epoch 0.5016737524085348 | train loss 0.6411004663 | val loss 0.6411529779:
	--> training ...
	--> checkpointing ...

2024-11-06_12-13 : iter     340000 <=> epoch 0.5091614203549308 | train loss 0.6406596899 | val loss 0.6408804059:
	--> training ...
	--> checkpointing ...

2024-11-06_13-24 : iter     345000 <=> epoch 0.5166490883013269 | train loss 0.6404730678 | val loss 0.6403192282:
	--> training ...
	--> checkpointing ...

2024-11-06_14-36 : iter     350000 <=> epoch 0.5241367562477229 | train loss 0.6405389905 | val loss 0.6408929229:
	--> training ...
	--> checkpointing ...

2024-11-06_15-47 : iter     355000 <=> epoch 0.531624424194119 | train loss 0.6418412924 | val loss 0.6419250369:
	--> training ...
	--> checkpointing ...

2024-11-06_16-59 : iter     360000 <=> epoch 0.539112092140515 | train loss 0.6409580708 | val loss 0.6411644816:
	--> training ...
	--> checkpointing ...

2024-11-06_18-10 : iter     365000 <=> epoch 0.5465997600869111 | train loss 0.6407326460 | val loss 0.6407817006:
	--> training ...
	--> checkpointing ...

2024-11-06_19-22 : iter     370000 <=> epoch 0.5540874280333071 | train loss 0.6403437257 | val loss 0.6399881840:
	--> training ...
	--> checkpointing ...

2024-11-06_20-33 : iter     375000 <=> epoch 0.5615750959797031 | train loss 0.6412881613 | val loss 0.6418662667:
	--> training ...
	--> checkpointing ...

2024-11-06_21-45 : iter     380000 <=> epoch 0.5690627639260992 | train loss 0.6415098310 | val loss 0.6411286592:
	--> training ...
	--> checkpointing ...

2024-11-06_22-57 : iter     385000 <=> epoch 0.5765504318724952 | train loss 0.6391577125 | val loss 0.6403333545:
	--> training ...
	--> checkpointing ...

2024-11-07_00-08 : iter     390000 <=> epoch 0.5840380998188913 | train loss 0.6413216591 | val loss 0.6396629214:
	--> training ...
	--> checkpointing ...

2024-11-07_01-20 : iter     395000 <=> epoch 0.5915257677652873 | train loss 0.6395697594 | val loss 0.6407893300:
	--> training ...
	--> checkpointing ...

2024-11-07_02-31 : iter     400000 <=> epoch 0.5990134357116834 | train loss 0.6406274438 | val loss 0.6399837136:
	--> training ...
	--> checkpointing ...

2024-11-07_03-43 : iter     405000 <=> epoch 0.6065011036580794 | train loss 0.6398828030 | val loss 0.6408556104:
	--> training ...
	--> checkpointing ...

2024-11-07_04-54 : iter     410000 <=> epoch 0.6139887716044755 | train loss 0.6400972009 | val loss 0.6405792832:
	--> training ...
	--> checkpointing ...

2024-11-07_06-06 : iter     415000 <=> epoch 0.6214764395508715 | train loss 0.6407389641 | val loss 0.6407129169:
	--> training ...
	--> checkpointing ...

2024-11-07_07-18 : iter     420000 <=> epoch 0.6289641074972675 | train loss 0.6401148438 | val loss 0.6410503983:
	--> training ...
	--> checkpointing ...

2024-11-07_08-29 : iter     425000 <=> epoch 0.6364517754436636 | train loss 0.6404380202 | val loss 0.6406107545:
	--> training ...
	--> checkpointing ...

2024-11-07_09-41 : iter     430000 <=> epoch 0.6439394433900596 | train loss 0.6404702663 | val loss 0.6404245496:
	--> training ...
	--> checkpointing ...

2024-11-07_10-52 : iter     435000 <=> epoch 0.6514271113364557 | train loss 0.6389378309 | val loss 0.6401727200:
	--> training ...
	--> checkpointing ...

2024-11-07_12-04 : iter     440000 <=> epoch 0.6589147792828517 | train loss 0.6399708390 | val loss 0.6413216591:
	--> training ...
	--> checkpointing ...

2024-11-07_13-15 : iter     445000 <=> epoch 0.6664024472292478 | train loss 0.6403738856 | val loss 0.6403048635:
	--> training ...
	--> checkpointing ...

2024-11-07_14-27 : iter     450000 <=> epoch 0.6738901151756438 | train loss 0.6399942040 | val loss 0.6394289136:
	--> training ...
	--> checkpointing ...

2024-11-07_15-39 : iter     455000 <=> epoch 0.6813777831220399 | train loss 0.6422325373 | val loss 0.6397375464:
	--> training ...
	--> checkpointing ...

2024-11-07_16-50 : iter     460000 <=> epoch 0.6888654510684359 | train loss 0.6427596211 | val loss 0.6428169012:
	--> training ...
	--> checkpointing ...

2024-11-07_18-02 : iter     465000 <=> epoch 0.6963531190148319 | train loss 0.6425520778 | val loss 0.6424245834:
	--> training ...
	--> checkpointing ...

2024-11-07_19-13 : iter     470000 <=> epoch 0.703840786961228 | train loss 0.6395673156 | val loss 0.6390253305:
	--> training ...
	--> checkpointing ...

2024-11-07_20-25 : iter     475000 <=> epoch 0.711328454907624 | train loss 0.6401701570 | val loss 0.6402806044:
	--> training ...
	--> checkpointing ...

2024-11-07_21-37 : iter     480000 <=> epoch 0.7188161228540201 | train loss 0.6401178241 | val loss 0.6393982172:
	--> training ...
	--> checkpointing ...

2024-11-07_22-48 : iter     485000 <=> epoch 0.7263037908004161 | train loss 0.6403458714 | val loss 0.6404725313:
	--> training ...
	--> checkpointing ...

2024-11-08_00-00 : iter     490000 <=> epoch 0.7337914587468122 | train loss 0.6396833062 | val loss 0.6408411264:
	--> training ...
	--> checkpointing ...

2024-11-08_01-11 : iter     495000 <=> epoch 0.7412791266932082 | train loss 0.6401720643 | val loss 0.6400920153:
	--> training ...
	--> checkpointing ...

2024-11-08_02-23 : iter     500000 <=> epoch 0.7487667946396043 | train loss 0.6400251389 | val loss 0.6394151449:
	--> training ...
	--> checkpointing ...

2024-11-08_03-34 : iter     505000 <=> epoch 0.7562544625860003 | train loss 0.6396234035 | val loss 0.6396235824:
	--> training ...
	--> checkpointing ...

2024-11-08_04-46 : iter     510000 <=> epoch 0.7637421305323963 | train loss 0.6392661929 | val loss 0.6403902769:
	--> training ...
	--> checkpointing ...

2024-11-08_05-58 : iter     515000 <=> epoch 0.7712297984787924 | train loss 0.6384956241 | val loss 0.6392807961:
	--> training ...
	--> checkpointing ...

2024-11-08_07-09 : iter     520000 <=> epoch 0.7787174664251884 | train loss 0.6402404308 | val loss 0.6390984058:
	--> training ...
	--> checkpointing ...

2024-11-08_08-21 : iter     525000 <=> epoch 0.7862051343715845 | train loss 0.6407852769 | val loss 0.6403926611:
	--> training ...
	--> checkpointing ...

2024-11-08_09-32 : iter     530000 <=> epoch 0.7936928023179805 | train loss 0.6399801373 | val loss 0.6391823292:
	--> training ...
	--> checkpointing ...

2024-11-08_10-44 : iter     535000 <=> epoch 0.8011804702643766 | train loss 0.6395232081 | val loss 0.6408069134:
	--> training ...
	--> checkpointing ...

2024-11-08_11-55 : iter     540000 <=> epoch 0.8086681382107725 | train loss 0.6397812366 | val loss 0.6405136585:
	--> training ...
	--> checkpointing ...

2024-11-08_13-07 : iter     545000 <=> epoch 0.8161558061571687 | train loss 0.6382684708 | val loss 0.6392486095:
	--> training ...
	--> checkpointing ...

2024-11-08_14-19 : iter     550000 <=> epoch 0.8236434741035646 | train loss 0.6400065422 | val loss 0.6388698220:
	--> training ...
	--> checkpointing ...

2024-11-08_15-30 : iter     555000 <=> epoch 0.8311311420499606 | train loss 0.6385014653 | val loss 0.6397650838:
	--> training ...
	--> checkpointing ...

2024-11-08_16-42 : iter     560000 <=> epoch 0.8386188099963567 | train loss 0.6395846605 | val loss 0.6395666003:
	--> training ...
	--> checkpointing ...

2024-11-08_17-53 : iter     565000 <=> epoch 0.8461064779427527 | train loss 1.3370317221 | val loss 1.3356564045:
	--> training ...
	--> checkpointing ...

2024-11-08_19-05 : iter     570000 <=> epoch 0.8535941458891488 | train loss 0.7990851402 | val loss 0.7997894287:
	--> training ...
	--> checkpointing ...

2024-11-08_20-16 : iter     575000 <=> epoch 0.8610818138355448 | train loss 0.7221882343 | val loss 0.7228848338:
	--> training ...
	--> checkpointing ...

2024-11-08_21-28 : iter     580000 <=> epoch 0.8685694817819409 | train loss 0.7003509402 | val loss 0.6998470426:
	--> training ...
	--> checkpointing ...

2024-11-08_22-40 : iter     585000 <=> epoch 0.8760571497283369 | train loss 0.6880639791 | val loss 0.6869019866:
	--> training ...
	--> checkpointing ...

2024-11-08_23-51 : iter     590000 <=> epoch 0.883544817674733 | train loss 0.6817498207 | val loss 0.6803393364:
	--> training ...
	--> checkpointing ...

2024-11-09_01-03 : iter     595000 <=> epoch 0.891032485621129 | train loss 0.6762837172 | val loss 0.6751419902:
	--> training ...
	--> checkpointing ...

2024-11-09_02-14 : iter     600000 <=> epoch 0.898520153567525 | train loss 0.6685321927 | val loss 0.6703041792:
	--> training ...
	--> checkpointing ...

2024-11-09_03-26 : iter     605000 <=> epoch 0.9060078215139211 | train loss 0.6650313735 | val loss 0.6661388874:
	--> training ...
	--> checkpointing ...

2024-11-09_04-38 : iter     610000 <=> epoch 0.9134954894603171 | train loss 0.6628351212 | val loss 0.6634075046:
	--> training ...
	--> checkpointing ...

2024-11-09_05-49 : iter     615000 <=> epoch 0.9209831574067132 | train loss 0.6620255113 | val loss 0.6615130901:
	--> training ...
	--> checkpointing ...

2024-11-09_07-01 : iter     620000 <=> epoch 0.9284708253531092 | train loss 0.6594109535 | val loss 0.6601457596:
	--> training ...
	--> checkpointing ...

2024-11-09_08-12 : iter     625000 <=> epoch 0.9359584932995053 | train loss 0.6584248543 | val loss 0.6579717994:
	--> training ...
	--> checkpointing ...

2024-11-09_09-24 : iter     630000 <=> epoch 0.9434461612459013 | train loss 0.6572988033 | val loss 0.6579545736:
	--> training ...
	--> checkpointing ...

2024-11-09_10-36 : iter     635000 <=> epoch 0.9509338291922974 | train loss 0.6556801796 | val loss 0.6553889513:
	--> training ...
	--> checkpointing ...

2024-11-09_11-47 : iter     640000 <=> epoch 0.9584214971386934 | train loss 0.6654154062 | val loss 0.6648985147:
	--> training ...
	--> checkpointing ...

2024-11-09_12-59 : iter     645000 <=> epoch 0.9659091650850894 | train loss 0.6556711197 | val loss 0.6569750905:
	--> training ...
	--> checkpointing ...

2024-11-09_14-11 : iter     650000 <=> epoch 0.9733968330314855 | train loss 0.6562580466 | val loss 0.6554278731:
	--> training ...
	--> checkpointing ...

2024-11-09_15-22 : iter     655000 <=> epoch 0.9808845009778815 | train loss 0.6538502574 | val loss 0.6537806988:
	--> training ...
	--> checkpointing ...

2024-11-09_16-34 : iter     660000 <=> epoch 0.9883721689242776 | train loss 0.6534388661 | val loss 0.6534813046:
	--> training ...
	--> checkpointing ...

2024-11-09_17-45 : iter     665000 <=> epoch 0.9958598368706736 | train loss 0.6532359719 | val loss 0.6536855698:
	--> training ...
	--> checkpointing ...

2024-11-09_18-57 : iter     670000 <=> epoch 1.0033475048170697 | train loss 0.6533914208 | val loss 0.6541423202:
	--> training ...
	--> checkpointing ...

2024-11-09_20-09 : iter     675000 <=> epoch 1.0108351727634657 | train loss 0.6525354981 | val loss 0.6520832777:
	--> training ...
	--> checkpointing ...

2024-11-09_21-20 : iter     680000 <=> epoch 1.0183228407098617 | train loss 0.6520920396 | val loss 0.6520048976:
	--> training ...
	--> checkpointing ...

2024-11-09_22-32 : iter     685000 <=> epoch 1.025810508656258 | train loss 0.6526867151 | val loss 0.6523387432:
	--> training ...
	--> checkpointing ...

2024-11-09_23-43 : iter     690000 <=> epoch 1.0332981766026539 | train loss 0.6517148018 | val loss 0.6513585448:
	--> training ...
