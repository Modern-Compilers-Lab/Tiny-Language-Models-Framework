|ITERS: 400858 / 400858 | COMP: 100.00% | RATE: 2.41 it./s | SPD: 0.4148 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 7.54 it./s | SPD: 0.1326 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 1)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:4.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 400858

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable
	--> loading vocab_size

Creating the model:

The model has 11M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-11-10_00-33 : iter     0 <=> epoch 0 | train loss 4.3365 | val loss 4.3367:
	--> training ...
	--> checkpointing ...

2024-11-10_01-10 : iter       5000 <=> epoch 0.021204472154669998 | train loss 1.0765628815 | val loss 1.0757495165:
	--> training ...
	--> checkpointing ...

2024-11-10_01-46 : iter      10000 <=> epoch 0.042408944309339995 | train loss 1.0514448881 | val loss 1.0515501499:
	--> training ...
	--> checkpointing ...

2024-11-10_02-24 : iter      15000 <=> epoch 0.06361341646400999 | train loss 1.0281444788 | val loss 1.0292769670:
	--> training ...
	--> checkpointing ...

2024-11-10_03-01 : iter      20000 <=> epoch 0.08481788861867999 | train loss 1.0195822716 | val loss 1.0189081430:
	--> training ...
	--> checkpointing ...

2024-11-10_03-37 : iter      25000 <=> epoch 0.10602236077334998 | train loss 1.0125879049 | val loss 1.0128796101:
	--> training ...
	--> checkpointing ...

2024-11-10_04-14 : iter      30000 <=> epoch 0.12722683292801998 | train loss 1.0082205534 | val loss 1.0080485344:
	--> training ...
	--> checkpointing ...

2024-11-10_04-51 : iter      35000 <=> epoch 0.14843130508268998 | train loss 1.0015301704 | val loss 1.0024361610:
	--> training ...
	--> checkpointing ...

2024-11-10_05-28 : iter      40000 <=> epoch 0.16963577723735998 | train loss 0.9990199804 | val loss 0.9987039566:
	--> training ...
	--> checkpointing ...

2024-11-10_06-04 : iter      45000 <=> epoch 0.19084024939202995 | train loss 0.9965979457 | val loss 0.9977400899:
	--> training ...
	--> checkpointing ...

2024-11-10_06-41 : iter      50000 <=> epoch 0.21204472154669995 | train loss 0.9948308468 | val loss 0.9927805066:
	--> training ...
	--> checkpointing ...

2024-11-10_07-18 : iter      55000 <=> epoch 0.23324919370136996 | train loss 0.9919189215 | val loss 0.9922394753:
	--> training ...
	--> checkpointing ...

2024-11-10_07-55 : iter      60000 <=> epoch 0.25445366585603996 | train loss 0.9907549024 | val loss 0.9915203452:
	--> training ...
	--> checkpointing ...

2024-11-10_08-32 : iter      65000 <=> epoch 0.27565813801070993 | train loss 0.9897574782 | val loss 0.9898706079:
	--> training ...
	--> checkpointing ...

2024-11-10_09-09 : iter      70000 <=> epoch 0.29686261016537996 | train loss 0.9901604056 | val loss 0.9892476201:
	--> training ...
	--> checkpointing ...

2024-11-10_09-46 : iter      75000 <=> epoch 0.31806708232004993 | train loss 0.9894272089 | val loss 0.9896789193:
	--> training ...
	--> checkpointing ...

2024-11-10_10-22 : iter      80000 <=> epoch 0.33927155447471996 | train loss 0.9887009263 | val loss 0.9875208139:
	--> training ...
	--> checkpointing ...

2024-11-10_10-59 : iter      85000 <=> epoch 0.36047602662938993 | train loss 0.9870943427 | val loss 0.9876069427:
	--> training ...
	--> checkpointing ...

2024-11-10_11-36 : iter      90000 <=> epoch 0.3816804987840599 | train loss 0.9876131415 | val loss 0.9875315428:
	--> training ...
	--> checkpointing ...

2024-11-10_12-13 : iter      95000 <=> epoch 0.40288497093872994 | train loss 0.9874032736 | val loss 0.9869565368:
	--> training ...
	--> checkpointing ...

2024-11-10_12-49 : iter     100000 <=> epoch 0.4240894430933999 | train loss 0.9874358177 | val loss 0.9866982698:
	--> training ...
	--> checkpointing ...

2024-11-10_13-26 : iter     105000 <=> epoch 0.44529391524806994 | train loss 0.9871062040 | val loss 0.9860486984:
	--> training ...
	--> checkpointing ...

2024-11-10_14-03 : iter     110000 <=> epoch 0.4664983874027399 | train loss 0.9857923388 | val loss 0.9861536622:
	--> training ...
	--> checkpointing ...

2024-11-10_14-40 : iter     115000 <=> epoch 0.48770285955740994 | train loss 0.9862091541 | val loss 0.9858125448:
	--> training ...
	--> checkpointing ...

2024-11-10_15-16 : iter     120000 <=> epoch 0.5089073317120799 | train loss 0.9862222672 | val loss 0.9851586819:
	--> training ...
	--> checkpointing ...

2024-11-10_15-53 : iter     125000 <=> epoch 0.5301118038667499 | train loss 0.9846464992 | val loss 0.9856207371:
	--> training ...
	--> checkpointing ...

2024-11-10_16-30 : iter     130000 <=> epoch 0.5513162760214199 | train loss 0.9868481159 | val loss 0.9860559106:
	--> training ...
	--> checkpointing ...

2024-11-10_17-07 : iter     135000 <=> epoch 0.5725207481760899 | train loss 0.9852598906 | val loss 0.9843766689:
	--> training ...
	--> checkpointing ...

2024-11-10_17-43 : iter     140000 <=> epoch 0.5937252203307599 | train loss 0.9846326709 | val loss 0.9850291014:
	--> training ...
	--> checkpointing ...

2024-11-10_18-20 : iter     145000 <=> epoch 0.61492969248543 | train loss 0.9848116040 | val loss 0.9848781228:
	--> training ...
	--> checkpointing ...

2024-11-10_18-57 : iter     150000 <=> epoch 0.6361341646400999 | train loss 0.9844795465 | val loss 0.9851762056:
	--> training ...
	--> checkpointing ...

2024-11-10_19-33 : iter     155000 <=> epoch 0.6573386367947699 | train loss 0.9843241572 | val loss 0.9855120778:
	--> training ...
	--> checkpointing ...

2024-11-10_20-10 : iter     160000 <=> epoch 0.6785431089494399 | train loss 0.9844177961 | val loss 0.9846671224:
	--> training ...
	--> checkpointing ...

2024-11-10_20-46 : iter     165000 <=> epoch 0.6997475811041098 | train loss 0.9836674333 | val loss 0.9846268892:
	--> training ...
	--> checkpointing ...

2024-11-10_21-23 : iter     170000 <=> epoch 0.7209520532587799 | train loss 0.9843649864 | val loss 0.9837722778:
	--> training ...
	--> checkpointing ...

2024-11-10_22-00 : iter     175000 <=> epoch 0.7421565254134499 | train loss 0.9865726829 | val loss 0.9877260923:
	--> training ...
	--> checkpointing ...

2024-11-10_22-37 : iter     180000 <=> epoch 0.7633609975681198 | train loss 0.9840393066 | val loss 0.9829301238:
	--> training ...
	--> checkpointing ...

2024-11-10_23-14 : iter     185000 <=> epoch 0.7845654697227898 | train loss 0.9832341075 | val loss 0.9841284156:
	--> training ...
	--> checkpointing ...

2024-11-10_23-50 : iter     190000 <=> epoch 0.8057699418774599 | train loss 0.9837172627 | val loss 0.9848705530:
	--> training ...
	--> checkpointing ...

2024-11-11_00-27 : iter     195000 <=> epoch 0.8269744140321299 | train loss 0.9844716787 | val loss 0.9842719436:
	--> training ...
	--> checkpointing ...

2024-11-11_01-04 : iter     200000 <=> epoch 0.8481788861867998 | train loss 0.9851750731 | val loss 0.9834014773:
	--> training ...
	--> checkpointing ...

2024-11-11_01-41 : iter     205000 <=> epoch 0.8693833583414698 | train loss 0.9839641452 | val loss 0.9838561416:
	--> training ...
	--> checkpointing ...

2024-11-11_02-17 : iter     210000 <=> epoch 0.8905878304961399 | train loss 0.9835194349 | val loss 0.9841342568:
	--> training ...
	--> checkpointing ...

2024-11-11_02-54 : iter     215000 <=> epoch 0.9117923026508098 | train loss 0.9831754565 | val loss 0.9831624627:
	--> training ...
	--> checkpointing ...

2024-11-11_03-31 : iter     220000 <=> epoch 0.9329967748054798 | train loss 0.9846567512 | val loss 0.9850905538:
	--> training ...
	--> checkpointing ...

2024-11-11_04-08 : iter     225000 <=> epoch 0.9542012469601499 | train loss 0.9848738909 | val loss 0.9846954346:
	--> training ...
	--> checkpointing ...

2024-11-11_04-45 : iter     230000 <=> epoch 0.9754057191148199 | train loss 0.9836026430 | val loss 0.9825432897:
	--> training ...
	--> checkpointing ...

2024-11-11_05-22 : iter     235000 <=> epoch 0.9966101912694898 | train loss 0.9841605425 | val loss 0.9826089144:
	--> training ...
	--> checkpointing ...

2024-11-11_05-59 : iter     240000 <=> epoch 1.0178146634241598 | train loss 0.9841362238 | val loss 0.9836724997:
	--> training ...
	--> checkpointing ...

2024-11-11_06-36 : iter     245000 <=> epoch 1.0390191355788299 | train loss 0.9841037393 | val loss 0.9819730520:
	--> training ...
	--> checkpointing ...

2024-11-11_07-12 : iter     250000 <=> epoch 1.0602236077334999 | train loss 0.9826269150 | val loss 0.9833972454:
	--> training ...
	--> checkpointing ...

2024-11-11_07-49 : iter     255000 <=> epoch 1.08142807988817 | train loss 0.9854277372 | val loss 0.9829430580:
	--> training ...
	--> checkpointing ...

2024-11-11_08-25 : iter     260000 <=> epoch 1.1026325520428397 | train loss 0.9832155704 | val loss 0.9835436940:
	--> training ...
	--> checkpointing ...

2024-11-11_09-02 : iter     265000 <=> epoch 1.1238370241975097 | train loss 0.9827672839 | val loss 0.9830486774:
	--> training ...
	--> checkpointing ...

2024-11-11_09-39 : iter     270000 <=> epoch 1.1450414963521798 | train loss 0.9837544560 | val loss 0.9829431176:
	--> training ...
	--> checkpointing ...

2024-11-11_10-15 : iter     275000 <=> epoch 1.1662459685068498 | train loss 0.9817947745 | val loss 0.9821308255:
	--> training ...
	--> checkpointing ...

2024-11-11_10-52 : iter     280000 <=> epoch 1.1874504406615198 | train loss 0.9832912087 | val loss 0.9841751456:
	--> training ...
	--> checkpointing ...

2024-11-11_11-29 : iter     285000 <=> epoch 1.2086549128161899 | train loss 0.9797788858 | val loss 0.9803139567:
	--> training ...
	--> checkpointing ...

2024-11-11_12-06 : iter     290000 <=> epoch 1.22985938497086 | train loss 0.9807311893 | val loss 0.9786433578:
	--> training ...
	--> checkpointing ...

2024-11-11_12-42 : iter     295000 <=> epoch 1.2510638571255297 | train loss 0.9791648984 | val loss 0.9797249436:
	--> training ...
	--> checkpointing ...

2024-11-11_13-19 : iter     300000 <=> epoch 1.2722683292801997 | train loss 0.9790113568 | val loss 0.9784653187:
	--> training ...
	--> checkpointing ...

2024-11-11_13-56 : iter     305000 <=> epoch 1.2934728014348698 | train loss 0.9795643687 | val loss 0.9800215364:
	--> training ...
	--> checkpointing ...

2024-11-11_14-33 : iter     310000 <=> epoch 1.3146772735895398 | train loss 0.9794531465 | val loss 0.9794387817:
	--> training ...
	--> checkpointing ...

2024-11-11_15-10 : iter     315000 <=> epoch 1.3358817457442098 | train loss 0.9781128168 | val loss 0.9788331389:
	--> training ...
	--> checkpointing ...

2024-11-11_15-46 : iter     320000 <=> epoch 1.3570862178988798 | train loss 0.9787311554 | val loss 0.9794024825:
	--> training ...
	--> checkpointing ...

2024-11-11_16-23 : iter     325000 <=> epoch 1.3782906900535496 | train loss 0.9781409502 | val loss 0.9783906341:
	--> training ...
	--> checkpointing ...

2024-11-11_17-00 : iter     330000 <=> epoch 1.3994951622082197 | train loss 0.9795531034 | val loss 0.9780612588:
	--> training ...
	--> checkpointing ...

2024-11-11_17-37 : iter     335000 <=> epoch 1.4206996343628897 | train loss 0.9780134559 | val loss 0.9772088528:
	--> training ...
	--> checkpointing ...

2024-11-11_18-14 : iter     340000 <=> epoch 1.4419041065175597 | train loss 0.9782345295 | val loss 0.9787149429:
	--> training ...
	--> checkpointing ...

2024-11-11_18-50 : iter     345000 <=> epoch 1.4631085786722298 | train loss 0.9782292247 | val loss 0.9782478809:
	--> training ...
	--> checkpointing ...

2024-11-11_19-27 : iter     350000 <=> epoch 1.4843130508268998 | train loss 0.9782081842 | val loss 0.9786041975:
	--> training ...
	--> checkpointing ...

2024-11-11_20-03 : iter     355000 <=> epoch 1.5055175229815698 | train loss 0.9796397090 | val loss 0.9783328772:
	--> training ...
	--> checkpointing ...

2024-11-11_20-40 : iter     360000 <=> epoch 1.5267219951362396 | train loss 0.9783056378 | val loss 0.9785797596:
	--> training ...
	--> checkpointing ...

2024-11-11_21-17 : iter     365000 <=> epoch 1.5479264672909097 | train loss 0.9797151685 | val loss 0.9786306620:
	--> training ...
	--> checkpointing ...

2024-11-11_21-53 : iter     370000 <=> epoch 1.5691309394455797 | train loss 0.9790468812 | val loss 0.9787868857:
	--> training ...
	--> checkpointing ...

2024-11-11_22-31 : iter     375000 <=> epoch 1.5903354116002497 | train loss 0.9786350727 | val loss 0.9784078002:
	--> training ...
	--> checkpointing ...

2024-11-11_23-07 : iter     380000 <=> epoch 1.6115398837549197 | train loss 0.9806309342 | val loss 0.9774418473:
	--> training ...
	--> checkpointing ...

2024-11-11_23-44 : iter     385000 <=> epoch 1.6327443559095898 | train loss 0.9778051972 | val loss 0.9778525233:
	--> training ...
	--> checkpointing ...

2024-11-12_00-21 : iter     390000 <=> epoch 1.6539488280642598 | train loss 0.9786843657 | val loss 0.9803304672:
	--> training ...
	--> checkpointing ...

2024-11-12_00-57 : iter     395000 <=> epoch 1.6751533002189296 | train loss 0.9783061147 | val loss 0.9787023664:
	--> training ...
	--> checkpointing ...

2024-11-12_01-34 : iter     400000 <=> epoch 1.6963577723735996 | train loss 0.9780021906 | val loss 0.9796348810:
	--> training ...
