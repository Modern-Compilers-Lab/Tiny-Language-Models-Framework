|ITERS: 222562 / 222562 | COMP: 100.00% | RATE: 1.68 it./s | SPD: 0.5950 s/it.| ERT: (0, 0, 0, 0)                                                                                                       
val>|ITERS: 500 / 500 | COMP: 100.00% | RATE: 5.27 it./s | SPD: 0.1897 s/it.| ERT: (0, 0, 0, 0) |                                                                                                       

Importing ...:
	--> took (0, 0, 0, 2)

Starting the netpune logging:
	--> neptune init
	--> saving the runid

Set the random seed for reproducibility:

Set the device to GPU if available, otherwise CPU:
	--> device set to cuda:5.

Setting arch-hyperparams for the GPT model:

Loading the training and evaluation data:
	--> train.bin
	--> took (0, 0, 0, 1)
	--> val.bin
	--> took (0, 0, 0, 0)

Setting train-hyperparams and util variables:
	--> max_iters = 222562

Defining the model and utilities:

The model:
	--> def get random batch of data
	--> def estimate loss
	--> def human readable

Loading the meta object:
	--> loading
	--> setting vocab size

Creating the model:

The model has 61M trainable parameters:

Preparing for the training loop:
	--> initialiazing the optimizer
	--> initializing the learing rate scheduler
	--> computing the initial loss
	--> saving the last loss for early stopping
	--> neptune logging the initial loss

Training loop:

==========================================================================================:

2024-10-30_03-23 : iter     0 <=> epoch 0 | train loss 4.0686 | val loss 4.0688:
	--> training ...
	--> checkpointing ...

2024-10-30_04-15 : iter       5000 <=> epoch 0.02246556094638398 | train loss 0.8430860639 | val loss 0.8427388668:
	--> training ...
	--> checkpointing ...

2024-10-30_05-08 : iter      10000 <=> epoch 0.04493112189276796 | train loss 0.8006111383 | val loss 0.8001182675:
	--> training ...
	--> checkpointing ...

2024-10-30_06-01 : iter      15000 <=> epoch 0.06739668283915194 | train loss 0.8004489541 | val loss 0.8001478910:
	--> training ...
	--> checkpointing ...

2024-10-30_06-53 : iter      20000 <=> epoch 0.08986224378553592 | train loss 0.7826288939 | val loss 0.7846070528:
	--> training ...
	--> checkpointing ...

2024-10-30_07-46 : iter      25000 <=> epoch 0.11232780473191989 | train loss 0.7961634994 | val loss 0.7981491685:
	--> training ...
	--> checkpointing ...

2024-10-30_08-38 : iter      30000 <=> epoch 0.13479336567830388 | train loss 0.7827036977 | val loss 0.7823301554:
	--> training ...
	--> checkpointing ...

2024-10-30_09-31 : iter      35000 <=> epoch 0.15725892662468785 | train loss 0.7787625194 | val loss 0.7808163166:
	--> training ...
	--> checkpointing ...

2024-10-30_10-24 : iter      40000 <=> epoch 0.17972448757107184 | train loss 0.7642292976 | val loss 0.7630339265:
	--> training ...
	--> checkpointing ...

2024-10-30_11-16 : iter      45000 <=> epoch 0.2021900485174558 | train loss 0.7681382895 | val loss 0.7683937550:
	--> training ...
	--> checkpointing ...

2024-10-30_12-09 : iter      50000 <=> epoch 0.22465560946383978 | train loss 0.7688416839 | val loss 0.7689083219:
	--> training ...
	--> checkpointing ...

2024-10-30_13-01 : iter      55000 <=> epoch 0.24712117041022377 | train loss 0.7573149800 | val loss 0.7556132078:
	--> training ...
	--> checkpointing ...

2024-10-30_13-54 : iter      60000 <=> epoch 0.26958673135660777 | train loss 0.7571563721 | val loss 0.7565730810:
	--> training ...
	--> checkpointing ...

2024-10-30_14-47 : iter      65000 <=> epoch 0.29205229230299173 | train loss 0.7442032099 | val loss 0.7448000312:
	--> training ...
	--> checkpointing ...

2024-10-30_15-39 : iter      70000 <=> epoch 0.3145178532493757 | train loss 0.7569397688 | val loss 0.7569521666:
	--> training ...
	--> checkpointing ...

2024-10-30_16-32 : iter      75000 <=> epoch 0.33698341419575967 | train loss 0.7694422007 | val loss 0.7690279484:
	--> training ...
	--> checkpointing ...

2024-10-30_17-24 : iter      80000 <=> epoch 0.3594489751421437 | train loss 0.7445043921 | val loss 0.7442155480:
	--> training ...
	--> checkpointing ...

2024-10-30_18-17 : iter      85000 <=> epoch 0.38191453608852766 | train loss 0.7508524656 | val loss 0.7501407862:
	--> training ...
	--> checkpointing ...

2024-10-30_19-10 : iter      90000 <=> epoch 0.4043800970349116 | train loss 0.7403519750 | val loss 0.7424324751:
	--> training ...
	--> checkpointing ...

2024-10-30_20-02 : iter      95000 <=> epoch 0.4268456579812956 | train loss 0.7507120371 | val loss 0.7514144182:
	--> training ...
	--> checkpointing ...

2024-10-30_20-55 : iter     100000 <=> epoch 0.44931121892767956 | train loss 0.7366206050 | val loss 0.7372843027:
	--> training ...
	--> checkpointing ...

2024-10-30_21-47 : iter     105000 <=> epoch 0.4717767798740636 | train loss 0.7307054996 | val loss 0.7320619226:
	--> training ...
	--> checkpointing ...

2024-10-30_22-40 : iter     110000 <=> epoch 0.49424234082044755 | train loss 0.7315781116 | val loss 0.7307609916:
	--> training ...
	--> checkpointing ...

2024-10-30_23-33 : iter     115000 <=> epoch 0.5167079017668316 | train loss 0.7289935350 | val loss 0.7296835780:
	--> training ...
	--> checkpointing ...

2024-10-31_00-25 : iter     120000 <=> epoch 0.5391734627132155 | train loss 0.7326109409 | val loss 0.7315283418:
	--> training ...
	--> checkpointing ...

2024-10-31_01-18 : iter     125000 <=> epoch 0.5616390236595995 | train loss 0.7281640172 | val loss 0.7286757827:
	--> training ...
	--> checkpointing ...

2024-10-31_02-11 : iter     130000 <=> epoch 0.5841045846059835 | train loss 0.7265769839 | val loss 0.7263075709:
	--> training ...
	--> checkpointing ...

2024-10-31_03-03 : iter     135000 <=> epoch 0.6065701455523674 | train loss 0.7208054066 | val loss 0.7213204503:
	--> training ...
	--> checkpointing ...

2024-10-31_03-56 : iter     140000 <=> epoch 0.6290357064987514 | train loss 0.7208676338 | val loss 0.7196614742:
	--> training ...
	--> checkpointing ...

2024-10-31_04-48 : iter     145000 <=> epoch 0.6515012674451354 | train loss 0.7174183130 | val loss 0.7165935040:
	--> training ...
	--> checkpointing ...

2024-10-31_05-41 : iter     150000 <=> epoch 0.6739668283915193 | train loss 0.7143928409 | val loss 0.7156770825:
	--> training ...
	--> checkpointing ...

2024-10-31_06-34 : iter     155000 <=> epoch 0.6964323893379033 | train loss 0.7151530385 | val loss 0.7148478627:
	--> training ...
	--> checkpointing ...

2024-10-31_07-27 : iter     160000 <=> epoch 0.7188979502842874 | train loss 0.7042851448 | val loss 0.7056573629:
	--> training ...
	--> checkpointing ...

2024-10-31_08-19 : iter     165000 <=> epoch 0.7413635112306713 | train loss 0.7030386925 | val loss 0.7026265860:
	--> training ...
	--> checkpointing ...

2024-10-31_09-12 : iter     170000 <=> epoch 0.7638290721770553 | train loss 0.7009260654 | val loss 0.7029398680:
	--> training ...
	--> checkpointing ...

2024-10-31_10-05 : iter     175000 <=> epoch 0.7862946331234393 | train loss 0.7000198364 | val loss 0.7011045814:
	--> training ...
	--> checkpointing ...

2024-10-31_10-58 : iter     180000 <=> epoch 0.8087601940698232 | train loss 0.6999449730 | val loss 0.6992065310:
	--> training ...
	--> checkpointing ...

2024-10-31_11-50 : iter     185000 <=> epoch 0.8312257550162072 | train loss 0.6983168125 | val loss 0.6990259290:
	--> training ...
	--> checkpointing ...

2024-10-31_12-43 : iter     190000 <=> epoch 0.8536913159625912 | train loss 0.6996347308 | val loss 0.6998940110:
	--> training ...
	--> checkpointing ...

2024-10-31_13-36 : iter     195000 <=> epoch 0.8761568769089751 | train loss 0.6981253624 | val loss 0.6994837523:
	--> training ...
	--> checkpointing ...

2024-10-31_14-29 : iter     200000 <=> epoch 0.8986224378553591 | train loss 0.7003301978 | val loss 0.6986373663:
	--> training ...
	--> checkpointing ...

2024-10-31_15-22 : iter     205000 <=> epoch 0.9210879988017432 | train loss 0.6997252703 | val loss 0.6985216141:
	--> training ...
	--> checkpointing ...

2024-10-31_16-15 : iter     210000 <=> epoch 0.9435535597481272 | train loss 0.6987200379 | val loss 0.6974105835:
	--> training ...
	--> checkpointing ...

2024-10-31_17-07 : iter     215000 <=> epoch 0.9660191206945111 | train loss 0.6991633177 | val loss 0.6987751722:
	--> training ...
	--> checkpointing ...

2024-10-31_18-00 : iter     220000 <=> epoch 0.9884846816408951 | train loss 0.6985977888 | val loss 0.6987009645:
	--> training ...
